TinyGPT â€“ From Scratch Training (15M Params)

This repo trains a ~15â€“17M parameter GPT-style language model from scratch on OpenWebText.

Goal:

Understand full LM pipeline

Tokenizer â†’ Dataset â†’ Model â†’ Training â†’ Resume â†’ Eval

Run locally (MPS) and scale to 3090 cloud

Train up to ~200M tokens under $20

ğŸ”§ System Requirements

Cloud:

RTX 3090 (24GB VRAM)

CUDA available

50GB+ disk

Local:

Mac (MPS) or CPU

For debugging only

ğŸ“ Pipeline Overview

We rebuild everything on cloud in this order:

Download raw text (~1GB)

Clean text

Train SentencePiece tokenizer (8k unigram)

Tokenize + pack to .bin

Train model (pilot â†’ full)

ğŸš€ Cloud Training Instructions
0ï¸âƒ£ Setup
git clone <repo>
cd tinygpt

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt

Verify CUDA:

import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))

Must show RTX 3090.

1ï¸âƒ£ Download 1GB OpenWebText

Edit:

scripts/download_owt.py

TARGET_CHAR_COUNT = 1_000_000_000

Run:

python scripts/download_owt.py

Output:

data/raw/raw.txt

Expected size: ~1GB

2ï¸âƒ£ Clean Text
python scripts/clean_text.py

Output:

data/cleaned/clean.txt

Expected:
~700Mâ€“850M characters.

3ï¸âƒ£ Train Tokenizer (Unigram 8k)
python scripts/train_tokenizer.py

Output:

data/spm/spm.model
data/spm/spm.vocab

Time: ~20â€“40 minutes.

4ï¸âƒ£ Tokenize + Pack
python scripts/tokenize_dataset.py

Output:

data/tokenized/train.bin
data/tokenized/val.bin

Check printed token count.

Target:
~200M tokens.

If <150M:
Increase raw text size and repeat.

ğŸ§  Model Specs

~17M parameters

d_model = 384

layers = 6

heads = 6

vocab = 8192

context = 512

weight tying enabled

ğŸ§ª Pilot Training (50M Tokens)

Edit train.py config:

CONTEXT_SIZE = 512
BATCH_SIZE = 32
GRAD_ACCUM_STEPS = 4
USE_AMP = True
MAX_STEPS = 800
LR = 5e-4
SAVE_EVERY = 100

Run:

PYTHONPATH=. python src/train/train.py

Monitor GPU:

watch -n 1 nvidia-smi

Expect:

20kâ€“40k tokens/sec

<8GB VRAM usage

Loss decreasing smoothly

ğŸ§  Full Training (200M Tokens)

After pilot is stable:

MAX_STEPS = 3200

Then:

PYTHONPATH=. python src/train/train.py

Estimated runtime:
~2â€“4 hours on 3090.

ğŸ” Resume Training

Training auto-resumes if:

checkpoint.pt

exists.

If spot instance dies:

PYTHONPATH=. python src/train/train.py

Resume happens automatically.

ğŸ“Š Metrics Logged

Train loss

Val loss

Tokens/sec

Step count

WandB logging enabled

Perplexity:

ppl = exp(loss)
ğŸ’° Cost Estimate

Pilot (50M tokens):
~30â€“45 minutes

Full (200M tokens):
~2â€“4 hours

On Vast.ai 3090 spot:
<$10 likely
<$20 worst case

ğŸ§  Learning Outcomes

You will understand:

Scaling laws (Chinchilla intuition)

Tokenizer impact

Batch size vs LR

Resume safety

Throughput estimation

Cloud cost control

Small model capacity limits

ğŸ”¬ Next Steps After Base LM

Compute real perplexity

Add text generation script

Instruction fine-tuning

Tiny reward model

Log-likelihood evaluation harness

This project is small in parameters, but large in understanding.

You are not building a toy.
You are building the mental model of how large models actually train