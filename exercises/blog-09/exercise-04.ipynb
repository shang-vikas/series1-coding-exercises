{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178ac10a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-04.ipynb)\n",
    "\n",
    "# Fine-tuning GPT-2 on Poetry Dataset\n",
    "\n",
    "## Goal\n",
    "Fine-tune GPT-2 on ~10M+ poetry tokens in Colab with:\n",
    "- Strong stylistic consistency\n",
    "- No overfitting\n",
    "- Stable training\n",
    "- Clean reproducibility\n",
    "\n",
    "## Assumptions\n",
    "- Colab with GPU (T4 / A100)\n",
    "- Gutenberg Poetry Corpus (`gutenberg-poetry.ndjson.gz`) downloaded from [aparrish/gutenberg-poetry-corpus](https://github.com/aparrish/gutenberg-poetry-corpus)\n",
    "- The compressed file is available in your Colab filesystem (e.g. `/content/gutenberg-poetry.ndjson.gz`)\n",
    "\n",
    "## Approach\n",
    "1. Load + clean data\n",
    "2. Build tokenizer (GPT-2 base)\n",
    "3. Proper train/val split\n",
    "4. Chunk into blocks (256 context)\n",
    "5. Fine-tune GPT-2 with proper schedule\n",
    "6. Monitor overfitting\n",
    "7. Generate samples\n",
    "\n",
    "This is how I would structure it in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8b124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1Ô∏è‚É£ Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -q transformers datasets accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933afc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2Ô∏è‚É£ Load Gutenberg Poetry Corpus\n",
    "\n",
    "We‚Äôll download the compressed NDJSON file directly from the Gutenberg Poetry Corpus and cache it in the Colab filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760c6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where we‚Äôll store the Gutenberg Poetry Corpus\n",
    "corpus_path = \"/content/gutenberg-poetry.ndjson.gz\"\n",
    "\n",
    "# Official corpus URL from Allison Parrish‚Äôs Gutenberg Poetry Corpus\n",
    "# See: https://github.com/aparrish/gutenberg-poetry-corpus\n",
    "corpus_url = \"https://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz\"\n",
    "\n",
    "# Download once if not already present\n",
    "if not os.path.exists(corpus_path):\n",
    "    print(\"Downloading Gutenberg Poetry Corpus from:\", corpus_url)\n",
    "    resp = requests.get(corpus_url, stream=True)\n",
    "    resp.raise_for_status()\n",
    "    with open(corpus_path, \"wb\") as f:\n",
    "        for chunk in resp.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Found existing corpus file at\", corpus_path)\n",
    "\n",
    "# Load lines from the compressed NDJSON corpus\n",
    "lines = []\n",
    "with gzip.open(corpus_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        obj = json.loads(line)\n",
    "        s = obj.get(\"s\", \"\").strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        lines.append(s)\n",
    "\n",
    "print(\"Total lines loaded:\", len(lines))\n",
    "\n",
    "# We use `texts` to keep the rest of the pipeline unchanged\n",
    "texts = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc44f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3Ô∏è‚É£ Clean Data (Minimal but Important)\n",
    "\n",
    "We preserve formatting. Poetry depends on line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(t):\n",
    "    t = t.strip()\n",
    "    t = t.replace(\"\\r\\n\", \"\\n\")\n",
    "    t = t.replace(\"\\r\", \"\\n\")\n",
    "    return t\n",
    "\n",
    "texts = [clean_text(t) for t in texts if len(t) > 50]\n",
    "\n",
    "print(\"Number of poems:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4Ô∏è‚É£ Combine into Single Corpus\n",
    "\n",
    "We preserve stanza breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\\n\\n\".join(texts)\n",
    "\n",
    "print(\"Total characters:\", len(corpus))\n",
    "\n",
    "# Check token scale later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5Ô∏è‚É£ Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cbd370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Add stanza token for stronger structure modeling\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<STANZA>\"]})\n",
    "\n",
    "# Replace double newlines\n",
    "corpus = corpus.replace(\"\\n\\n\", \" <STANZA> \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6Ô∏è‚É£ Tokenize Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(corpus, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "print(\"Total tokens:\", tokens.shape[0])\n",
    "\n",
    "# You want 10M+ tokens here.\n",
    "# If less, consider:\n",
    "# - concatenating multiple Kaggle poetry datasets\n",
    "# - adding Gutenberg poetry corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7Ô∏è‚É£ Train / Validation Split (Critical)\n",
    "\n",
    "Never train on 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e21d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(0.95 * len(tokens))\n",
    "train_tokens = tokens[:split_idx]\n",
    "val_tokens = tokens[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5614ac",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Chunk Into Fixed-Length Blocks\n",
    "\n",
    "We use block size 256 for poetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab914ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "\n",
    "def chunk_tokens(token_tensor):\n",
    "    examples = []\n",
    "    for i in range(0, len(token_tensor) - block_size, block_size):\n",
    "        examples.append(token_tensor[i:i+block_size])\n",
    "    return examples\n",
    "\n",
    "train_examples = chunk_tokens(train_tokens)\n",
    "val_examples = chunk_tokens(val_tokens)\n",
    "\n",
    "print(\"Train chunks:\", len(train_examples))\n",
    "print(\"Val chunks:\", len(val_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14469b",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Convert to HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\"input_ids\": train_examples})\n",
    "val_dataset = Dataset.from_dict({\"input_ids\": val_examples})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650dd3b0",
   "metadata": {},
   "source": [
    "## üîü Load GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45d334",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f7127b",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Training Arguments (Expert Settings)\n",
    "\n",
    "These are tuned for stability + minimal overfitting.\n",
    "\n",
    "**Why these values:**\n",
    "- `5e-5` ‚Üí safe for fine-tuning\n",
    "- `cosine decay` ‚Üí smoother convergence\n",
    "- `warmup` prevents early divergence\n",
    "- `weight_decay` combats overfitting\n",
    "- `gradient accumulation` for stable large batch\n",
    "- `load_best_model_at_end` ensures best val checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"poetry-gpt2\",\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    \n",
    "    gradient_accumulation_steps=8,  # effective batch 64\n",
    "    \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    fp16=True,\n",
    "    \n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    \n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a6729d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5308cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2380f7d5",
   "metadata": {},
   "source": [
    "## üìâ Monitoring Overfitting\n",
    "\n",
    "Watch:\n",
    "- **training loss**\n",
    "- **validation loss**\n",
    "\n",
    "If validation loss:\n",
    "- **decreases then increases** ‚Üí stop early\n",
    "- **flat** ‚Üí LR too low\n",
    "- **exploding** ‚Üí LR too high\n",
    "\n",
    "For 10M tokens, 3‚Äì5 epochs is usually enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dea410",
   "metadata": {},
   "source": [
    "## üé≠ Generate Poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b504f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=200):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.92,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate(\"Moonlight spills across the silent river\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c68ae",
   "metadata": {},
   "source": [
    "## üéØ Best Practices Summary\n",
    "\n",
    "### To prevent overfitting:\n",
    "- ‚úÖ Proper 95/5 split\n",
    "- ‚úÖ Cosine decay\n",
    "- ‚úÖ Weight decay 0.01\n",
    "- ‚úÖ No more than 5 epochs\n",
    "- ‚úÖ Load best checkpoint\n",
    "- ‚úÖ Large effective batch\n",
    "- ‚úÖ Monitor validation loss\n",
    "- ‚úÖ Do not crank LR above 1e-4\n",
    "\n",
    "### To improve poetic quality:\n",
    "- ‚úÖ Preserve line breaks\n",
    "- ‚úÖ Add `<STANZA>` token\n",
    "- ‚úÖ Use temperature 0.8‚Äì1.0\n",
    "- ‚úÖ Adjust top-p for creativity\n",
    "\n",
    "### If You Want Even Better Stability\n",
    "\n",
    "Upgrade:\n",
    "- Use `bitsandbytes` 8-bit Adam\n",
    "- Use gradient clipping (1.0)\n",
    "- Use Flash Attention (if GPU supports)\n",
    "\n",
    "**Now you have:**\n",
    "- Industrial-grade fine-tuning setup\n",
    "- Proper data pipeline\n",
    "- Anti-overfitting strategy\n",
    "- Stable optimization\n",
    "- Controlled generation\n",
    "\n",
    "This is how I would deploy a stylistically consistent GPT-2 fine-tune in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00136cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
