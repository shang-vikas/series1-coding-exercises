{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "178ac10a",
      "metadata": {
        "id": "178ac10a"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-04.ipynb)\n",
        "\n",
        "# Fine-tuning GPT-2 on Poetry Dataset\n",
        "\n",
        "## Goal\n",
        "Fine-tune GPT-2 on ~10M+ poetry tokens in Colab with:\n",
        "- Strong stylistic consistency\n",
        "- No overfitting\n",
        "- Stable training\n",
        "- Clean reproducibility\n",
        "\n",
        "## Assumptions\n",
        "- Colab with GPU (T4 / A100)\n",
        "- Gutenberg Poetry Corpus (`gutenberg-poetry.ndjson.gz`) downloaded from [aparrish/gutenberg-poetry-corpus](https://github.com/aparrish/gutenberg-poetry-corpus)\n",
        "- The compressed file is available in your Colab filesystem (e.g. `/content/gutenberg-poetry.ndjson.gz`)\n",
        "\n",
        "## Approach\n",
        "1. Load + clean data\n",
        "2. Build tokenizer (GPT-2 base)\n",
        "3. Proper train/val split\n",
        "4. Chunk into blocks (256 context)\n",
        "5. Fine-tune GPT-2 with proper schedule\n",
        "6. Monitor overfitting\n",
        "7. Generate samples\n",
        "\n",
        "This is how I would structure it in production."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q transformers accelerate bitsandbytes datasets"
      ],
      "metadata": {
        "id": "jzEmpty3Z2QC",
        "outputId": "e75e3b18-a63d-42e8-d3ba-ec436ee0ddfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jzEmpty3Z2QC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "1a8b124a",
      "metadata": {
        "id": "1a8b124a"
      },
      "outputs": [],
      "source": [
        "## 1Ô∏è‚É£ Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ca0b2c1",
      "metadata": {
        "id": "8ca0b2c1",
        "outputId": "8547dd42-6b7d-4a08-d9b1-366bd8774b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    GPT2TokenizerFast,\n",
        "    GPT2LMHeadModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Load Gutenberg Poetry Corpus\n",
        "\n",
        "We‚Äôll download the compressed NDJSON file directly from the Gutenberg Poetry Corpus and cache it in the Colab filesystem."
      ],
      "metadata": {
        "id": "933afc49"
      },
      "id": "933afc49"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "760c6619",
      "metadata": {
        "id": "760c6619",
        "outputId": "41effc41-b222-4413-80c9-1628aa47b2c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing corpus file at /content/gutenberg-poetry.ndjson.gz\n",
            "Total lines loaded: 3085117\n"
          ]
        }
      ],
      "source": [
        "# Path where we‚Äôll store the Gutenberg Poetry Corpus\n",
        "corpus_path = \"/content/gutenberg-poetry.ndjson.gz\"\n",
        "\n",
        "# Official corpus URL from Allison Parrish‚Äôs Gutenberg Poetry Corpus\n",
        "# See: https://github.com/aparrish/gutenberg-poetry-corpus\n",
        "corpus_url = \"https://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz\"\n",
        "\n",
        "# Download once if not already present\n",
        "if not os.path.exists(corpus_path):\n",
        "    print(\"Downloading Gutenberg Poetry Corpus from:\", corpus_url)\n",
        "    resp = requests.get(corpus_url, stream=True)\n",
        "    resp.raise_for_status()\n",
        "    with open(corpus_path, \"wb\") as f:\n",
        "        for chunk in resp.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    print(\"Download complete.\")\n",
        "else:\n",
        "    print(\"Found existing corpus file at\", corpus_path)\n",
        "\n",
        "# Load lines from the compressed NDJSON corpus\n",
        "lines = []\n",
        "with gzip.open(corpus_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        obj = json.loads(line)\n",
        "        s = obj.get(\"s\", \"\").strip()\n",
        "        if not s:\n",
        "            continue\n",
        "        lines.append(s)\n",
        "\n",
        "print(\"Total lines loaded:\", len(lines))\n",
        "\n",
        "# We use `texts` to keep the rest of the pipeline unchanged\n",
        "texts = lines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Clean Data (Minimal but Important)\n",
        "\n",
        "We preserve formatting. Poetry depends on line breaks."
      ],
      "metadata": {
        "id": "9KoARUU5WsFa"
      },
      "id": "9KoARUU5WsFa"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9774fefa",
      "metadata": {
        "id": "9774fefa",
        "outputId": "12aec0d2-af55-430a-aecb-759198424c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of poems: 324589\n"
          ]
        }
      ],
      "source": [
        "def clean_text(t):\n",
        "    t = t.strip()\n",
        "    t = t.replace(\"\\r\\n\", \"\\n\")\n",
        "    t = t.replace(\"\\r\", \"\\n\")\n",
        "    return t\n",
        "\n",
        "texts = [clean_text(t) for t in texts if len(t) > 50]\n",
        "\n",
        "print(\"Number of poems:\", len(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Combine into Single Corpus\n",
        "\n",
        "We preserve stanza breaks."
      ],
      "metadata": {
        "id": "9a4e745f"
      },
      "id": "9a4e745f"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cb0d8a10",
      "metadata": {
        "id": "cb0d8a10",
        "outputId": "59a15d99-99e2-4497-f582-d772f0cff08f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 19417189\n"
          ]
        }
      ],
      "source": [
        "corpus = \"\\n\\n\".join(texts)\n",
        "\n",
        "print(\"Total characters:\", len(corpus))\n",
        "\n",
        "# Check token scale later"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Initialize Tokenizer"
      ],
      "metadata": {
        "id": "72a9ea12"
      },
      "id": "72a9ea12"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "57cbd370",
      "metadata": {
        "id": "57cbd370",
        "outputId": "b6c8562b-dd52-41ea-f264-0fbdac82e9ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Add stanza token for stronger structure modeling\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<STANZA>\"]})\n",
        "\n",
        "# Replace double newlines\n",
        "corpus = corpus.replace(\"\\n\\n\", \" <STANZA> \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Tokenize Corpus"
      ],
      "metadata": {
        "id": "01e0241e"
      },
      "id": "01e0241e"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def get_or_build_tokens(corpus, tokenizer, save_path=\"poetry_tokens.pt\"):\n",
        "    \"\"\"\n",
        "    Load tokens from disk if available.\n",
        "    Otherwise tokenize corpus and save.\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(save_path):\n",
        "        print(\"Loading existing token file...\")\n",
        "        tokens = torch.load(save_path)\n",
        "        print(\"Loaded tokens:\", tokens.shape[0])\n",
        "    else:\n",
        "        print(\"Tokenizing corpus...\")\n",
        "        tokens = tokenizer(corpus, return_tensors=\"pt\")[\"input_ids\"][0]\n",
        "        print(\"Total tokens:\", tokens.shape[0])\n",
        "\n",
        "        torch.save(tokens, save_path)\n",
        "        print(f\"Saved tokens to {save_path}\")\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "wHcZmulvXcN6"
      },
      "id": "wHcZmulvXcN6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "58f2205e",
      "metadata": {
        "id": "58f2205e",
        "outputId": "106d685a-bebd-4d23-c604-e7b7e7bbc4c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing token file...\n",
            "Loaded tokens: 6150837\n",
            "Total tokens: 6150837\n"
          ]
        }
      ],
      "source": [
        "tokens = get_or_build_tokens(corpus, tokenizer)\n",
        "print(\"Total tokens:\", tokens.shape[0])\n",
        "\n",
        "# You want 10M+ tokens here.\n",
        "# If less, consider:\n",
        "# - concatenating multiple Kaggle poetry datasets\n",
        "# - adding Gutenberg poetry corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5e1a2d91",
      "metadata": {
        "id": "5e1a2d91"
      },
      "outputs": [],
      "source": [
        "## 7Ô∏è‚É£ Train / Validation Split (Critical)\n",
        "\n",
        "#Never train on 100%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "13e21d7e",
      "metadata": {
        "id": "13e21d7e"
      },
      "outputs": [],
      "source": [
        "split_idx = int(0.95 * len(tokens))\n",
        "train_tokens = tokens[:split_idx]\n",
        "val_tokens = tokens[split_idx:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5614ac",
      "metadata": {
        "id": "5e5614ac"
      },
      "source": [
        "## 8Ô∏è‚É£ Chunk Into Fixed-Length Blocks\n",
        "\n",
        "We use block size 256 for poetry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3ab914ea",
      "metadata": {
        "id": "3ab914ea",
        "outputId": "5e65df7e-261d-4410-95fb-441276f4d5d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train chunks: 22825\n",
            "Val chunks: 1201\n"
          ]
        }
      ],
      "source": [
        "block_size = 256\n",
        "\n",
        "def chunk_tokens(token_tensor):\n",
        "    examples = []\n",
        "    for i in range(0, len(token_tensor) - block_size, block_size):\n",
        "        examples.append(token_tensor[i:i+block_size])\n",
        "    return examples\n",
        "\n",
        "train_examples = chunk_tokens(train_tokens)\n",
        "val_examples = chunk_tokens(val_tokens)\n",
        "\n",
        "print(\"Train chunks:\", len(train_examples))\n",
        "print(\"Val chunks:\", len(val_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c14469b",
      "metadata": {
        "id": "9c14469b"
      },
      "source": [
        "## 9Ô∏è‚É£ Convert to HuggingFace Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "effb4b2b",
      "metadata": {
        "id": "effb4b2b"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_dict({\"input_ids\": train_examples})\n",
        "val_dataset = Dataset.from_dict({\"input_ids\": val_examples})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "650dd3b0",
      "metadata": {
        "id": "650dd3b0"
      },
      "source": [
        "## üîü Load GPT-2 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4291b65d",
      "metadata": {
        "id": "4291b65d",
        "outputId": "6fd519e1-f2e3-4d0b-bb8b-2822a216c308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573,
          "referenced_widgets": [
            "2c7c82dbee9c4dafbb970dbe98310222",
            "d8e3ccef99564f65b70ee5f3949e8f44",
            "6626d0be87a740fdbcb6f66f54409bbf",
            "c217437b59144a2b9f1c16cb6baaf06c",
            "19f13940f080464ba80a5f1c5c976f86",
            "7cb91d5eee7e4900890f8082f1584b4e",
            "675e68b8f64949dbb2cdea68cb612efb",
            "da79914d3b71461588fdaa537c70610e",
            "d8224cf9875a4444bb743f707a074f92",
            "08c0e962d0dc4fdb9c4f2f4e591d4f7e",
            "359c6fe0a49542a78a29d317bcd286b5"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c7c82dbee9c4dafbb970dbe98310222"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50258, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d45d334",
      "metadata": {
        "id": "1d45d334"
      },
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4ba8c533",
      "metadata": {
        "id": "4ba8c533"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4pAhKXfcbLdE"
      },
      "id": "4pAhKXfcbLdE"
    },
    {
      "cell_type": "markdown",
      "id": "e5f7127b",
      "metadata": {
        "id": "e5f7127b"
      },
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Training Arguments (Expert Settings)\n",
        "\n",
        "These are tuned for stability + minimal overfitting.\n",
        "\n",
        "**Why these values:**\n",
        "- `5e-5` ‚Üí safe for fine-tuning\n",
        "- `cosine decay` ‚Üí smoother convergence\n",
        "- `warmup` prevents early divergence\n",
        "- `weight_decay` combats overfitting\n",
        "- `gradient accumulation` for stable large batch\n",
        "- `load_best_model_at_end` ensures best val checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dcacb980",
      "metadata": {
        "id": "dcacb980",
        "outputId": "828dec67-6e56-4a02-8a1d-c09a881aa843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"poetry-gpt2\",\n",
        "    #overwrite_output_dir=True,\n",
        "\n",
        "    num_train_epochs=8,\n",
        "    per_device_train_batch_size=24,\n",
        "    per_device_eval_batch_size=8,\n",
        "\n",
        "    gradient_accumulation_steps=8,  # effective batch 192\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    logging_steps=50,\n",
        "\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "\n",
        "    fp16=True,\n",
        "\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a6729d",
      "metadata": {
        "id": "01a6729d"
      },
      "source": [
        "## 1Ô∏è‚É£3Ô∏è‚É£ Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "tLBVYWOXZr-P"
      },
      "id": "tLBVYWOXZr-P",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5308cc8",
      "metadata": {
        "id": "c5308cc8",
        "outputId": "5925f49d-d28d-4ca1-bf5f-5fbb31a6c3b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 30/952 02:24 < 1:19:23, 0.19 it/s, Epoch 0.24/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2380f7d5",
      "metadata": {
        "id": "2380f7d5"
      },
      "source": [
        "## üìâ Monitoring Overfitting\n",
        "\n",
        "Watch:\n",
        "- **training loss**\n",
        "- **validation loss**\n",
        "\n",
        "If validation loss:\n",
        "- **decreases then increases** ‚Üí stop early\n",
        "- **flat** ‚Üí LR too low\n",
        "- **exploding** ‚Üí LR too high\n",
        "\n",
        "For 10M tokens, 3‚Äì5 epochs is usually enough."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65dea410",
      "metadata": {
        "id": "65dea410"
      },
      "source": [
        "## üé≠ Generate Poetry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b504f5",
      "metadata": {
        "id": "81b504f5"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_length=200):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(generate(\"Moonlight spills across the silent river\\n\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8c68ae",
      "metadata": {
        "id": "de8c68ae"
      },
      "source": [
        "## üéØ Best Practices Summary\n",
        "\n",
        "### To prevent overfitting:\n",
        "- ‚úÖ Proper 95/5 split\n",
        "- ‚úÖ Cosine decay\n",
        "- ‚úÖ Weight decay 0.01\n",
        "- ‚úÖ No more than 5 epochs\n",
        "- ‚úÖ Load best checkpoint\n",
        "- ‚úÖ Large effective batch\n",
        "- ‚úÖ Monitor validation loss\n",
        "- ‚úÖ Do not crank LR above 1e-4\n",
        "\n",
        "### To improve poetic quality:\n",
        "- ‚úÖ Preserve line breaks\n",
        "- ‚úÖ Add `<STANZA>` token\n",
        "- ‚úÖ Use temperature 0.8‚Äì1.0\n",
        "- ‚úÖ Adjust top-p for creativity\n",
        "\n",
        "### If You Want Even Better Stability\n",
        "\n",
        "Upgrade:\n",
        "- Use `bitsandbytes` 8-bit Adam\n",
        "- Use gradient clipping (1.0)\n",
        "- Use Flash Attention (if GPU supports)\n",
        "\n",
        "**Now you have:**\n",
        "- Industrial-grade fine-tuning setup\n",
        "- Proper data pipeline\n",
        "- Anti-overfitting strategy\n",
        "- Stable optimization\n",
        "- Controlled generation\n",
        "\n",
        "This is how I would deploy a stylistically consistent GPT-2 fine-tune in production."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00136cfd",
      "metadata": {
        "id": "00136cfd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c7c82dbee9c4dafbb970dbe98310222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8e3ccef99564f65b70ee5f3949e8f44",
              "IPY_MODEL_6626d0be87a740fdbcb6f66f54409bbf",
              "IPY_MODEL_c217437b59144a2b9f1c16cb6baaf06c"
            ],
            "layout": "IPY_MODEL_19f13940f080464ba80a5f1c5c976f86"
          }
        },
        "d8e3ccef99564f65b70ee5f3949e8f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb91d5eee7e4900890f8082f1584b4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_675e68b8f64949dbb2cdea68cb612efb",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "6626d0be87a740fdbcb6f66f54409bbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da79914d3b71461588fdaa537c70610e",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8224cf9875a4444bb743f707a074f92",
            "value": 148
          }
        },
        "c217437b59144a2b9f1c16cb6baaf06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c0e962d0dc4fdb9c4f2f4e591d4f7e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_359c6fe0a49542a78a29d317bcd286b5",
            "value": "‚Äá148/148‚Äá[00:00&lt;00:00,‚Äá496.89it/s,‚ÄáMaterializing‚Äáparam=transformer.wte.weight]"
          }
        },
        "19f13940f080464ba80a5f1c5c976f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cb91d5eee7e4900890f8082f1584b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675e68b8f64949dbb2cdea68cb612efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da79914d3b71461588fdaa537c70610e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8224cf9875a4444bb743f707a074f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "08c0e962d0dc4fdb9c4f2f4e591d4f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359c6fe0a49542a78a29d317bcd286b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}