{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-03.ipynb)\n",
    "\n",
    "# REINFORCE with Reward Shaping Demo\n",
    "\n",
    "This notebook demonstrates:\n",
    "- A small pretrained causal LM (distilgpt2) as policy\n",
    "- A sentiment classifier pipeline as reward oracle\n",
    "- REINFORCE-style policy updates (educational)\n",
    "- A clear reward-hacking demonstration (unshaped reward)\n",
    "- Safety reward shaping (repetition / profanity / short-response penalties)\n",
    "- Side-by-side visualizations and sample galleries\n",
    "\n",
    "**Runtime note**: This downloads models (~100–300MB). Use a GPU runtime in Colab for speed (optional but recommended). Keep iteration counts small (defaults are conservative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Colab cell\n",
    "%pip install -q transformers torch datasets sentencepiece matplotlib\n",
    "\n",
    "# Python cell\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "# reproducible-ish\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6c3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_name = \"distilgpt2\"  # small, fast for demo\n",
    "print(\"Loading policy:\", policy_name)\n",
    "policy_tok = AutoTokenizer.from_pretrained(policy_name)\n",
    "# ensure pad token exists\n",
    "if policy_tok.pad_token is None:\n",
    "    policy_tok.pad_token = policy_tok.eos_token\n",
    "\n",
    "policy = AutoModelForCausalLM.from_pretrained(policy_name).to(device)\n",
    "policy.train()  # we will update it via REINFORCE\n",
    "\n",
    "# reward oracle: sentiment classifier (SST-like)\n",
    "print(\"Loading sentiment pipeline (reward oracle)...\")\n",
    "sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    device=0 if device == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# optimizer for policy (small LR for safety)\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-5)\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Sampling helpers ----------\n",
    "def sample_with_logprobs_train(prompt_ids, max_new_tokens=20, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Sample step-by-step while retaining gradient on log-probs (for REINFORCE).\n",
    "    Returns:\n",
    "      generated_ids (tensor 1 x L), logprob_sum (tensor scalar)\n",
    "    \"\"\"\n",
    "    generated = prompt_ids.to(device)\n",
    "    logprob_sum = 0.0  # torch scalar to accumulate\n",
    "    for _ in range(max_new_tokens):\n",
    "        outputs = policy(generated)  # returns logits with grad\n",
    "        logits = outputs.logits  # (1, seq_len, vocab)\n",
    "        next_logits = logits[:, -1, :] / max(temperature, 1e-9)\n",
    "        if top_k is not None:\n",
    "            v, idx = torch.topk(next_logits, top_k)\n",
    "            mask = torch.full_like(next_logits, -1e9)\n",
    "            mask[:, idx[0]] = next_logits[:, idx[0]]\n",
    "            next_logits = mask\n",
    "        probs = F.softmax(next_logits, dim=-1)  # (1, V)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1,1)\n",
    "        # compute logprob of sampled token (this retains grad)\n",
    "        logp = torch.log(probs.gather(-1, next_token)).squeeze()\n",
    "        if isinstance(logprob_sum, float):\n",
    "            logprob_sum = logp\n",
    "        else:\n",
    "            logprob_sum = logprob_sum + logp\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated, logprob_sum\n",
    "\n",
    "def sample_for_eval(prompt, max_new_tokens=20, temperature=1.0, top_k=None):\n",
    "    \"\"\"Sampling for evaluation (no gradient, faster). Returns decoded string.\"\"\"\n",
    "    policy.eval()\n",
    "    input_ids = policy_tok(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generated = input_ids\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = policy(generated).logits\n",
    "            next_logits = logits[:, -1, :] / max(temperature, 1e-9)\n",
    "            if top_k is not None:\n",
    "                v, idx = torch.topk(next_logits, top_k)\n",
    "                mask = torch.full_like(next_logits, -1e9)\n",
    "                mask[:, idx[0]] = next_logits[:, idx[0]]\n",
    "                next_logits = mask\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "    return policy_tok.decode(generated[0].cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "# ---------- Reward oracle ----------\n",
    "def sentiment_reward(text):\n",
    "    \"\"\"Return sentiment reward in [-1, +1] (positive -> positive reward).\"\"\"\n",
    "    try:\n",
    "        out = sentiment(text[:512])  # limit input size for the classifier\n",
    "    except Exception as e:\n",
    "        # if pipeline hiccups, return neutral\n",
    "        return 0.0\n",
    "    lab = out[0][\"label\"].upper()\n",
    "    sc = float(out[0][\"score\"])\n",
    "    return sc if lab.startswith(\"POSITIVE\") else -sc\n",
    "\n",
    "# ---------- Simple penalties for shaping ----------\n",
    "BAD_WORDS = {\"idiot\", \"stupid\", \"damn\"}  # toy example blacklist\n",
    "\n",
    "def repetition_penalty(text, ngram=3):\n",
    "    toks = text.split()\n",
    "    if len(toks) < 2:\n",
    "        return 0.0\n",
    "    # longest identical-token run\n",
    "    max_run = 1\n",
    "    run = 1\n",
    "    for i in range(1, len(toks)):\n",
    "        if toks[i] == toks[i-1]:\n",
    "            run += 1\n",
    "            max_run = max(max_run, run)\n",
    "        else:\n",
    "            run = 1\n",
    "    # repeated ngram count\n",
    "    ngram_counts = {}\n",
    "    for i in range(len(toks)-ngram+1):\n",
    "        ng = tuple(toks[i:i+ngram])\n",
    "        ngram_counts[ng] = ngram_counts.get(ng, 0) + 1\n",
    "    rep_count = sum(1 for v in ngram_counts.values() if v > 1)\n",
    "    return 0.5 * max_run + 1.0 * rep_count\n",
    "\n",
    "def profanity_penalty(text):\n",
    "    toks = set(w.lower().strip(\".,!?;:\") for w in text.split())\n",
    "    hits = toks & BAD_WORDS\n",
    "    return float(len(hits))\n",
    "\n",
    "def length_penalty(text, min_len=5):\n",
    "    toks = text.split()\n",
    "    if len(toks) >= min_len:\n",
    "        return 0.0\n",
    "    return (min_len - len(toks)) * 0.5\n",
    "\n",
    "def shaped_reward(text, lambda_rep=0.7, lambda_prof=1.0, lambda_short=0.6):\n",
    "    base = sentiment_reward(text)\n",
    "    rep = repetition_penalty(text)\n",
    "    prof = profanity_penalty(text)\n",
    "    short = length_penalty(text)\n",
    "    return base - lambda_rep * rep - lambda_prof * prof - lambda_short * short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e553d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running baseline for variance reduction\n",
    "running_baseline = 0.0\n",
    "alpha_baseline = 0.02  # small smoothing factor\n",
    "\n",
    "def reinforce_update(prompt, shaped=False, max_new_tokens=12, temperature=0.8, top_k=None, scale=1.0):\n",
    "    \"\"\"\n",
    "    Single REINFORCE update step:\n",
    "     - sample tokens retaining logprobs\n",
    "     - compute scalar reward (shaped or base)\n",
    "     - compute loss = - (reward - baseline) * logprob_sum\n",
    "     - backprop & optimizer.step()\n",
    "    Returns generated_text, reward\n",
    "    \"\"\"\n",
    "    global running_baseline\n",
    "    policy.train()\n",
    "    inp = policy_tok(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generated_ids, logprob_sum = sample_with_logprobs_train(inp, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "    generated_text = policy_tok.decode(generated_ids[0].cpu().numpy(), skip_special_tokens=True)\n",
    "    # compute reward (outside gradient)\n",
    "    if shaped:\n",
    "        r = shaped_reward(generated_text)\n",
    "    else:\n",
    "        r = sentiment_reward(generated_text)\n",
    "    r = float(r) * scale\n",
    "    # update running baseline\n",
    "    running_baseline = (1 - alpha_baseline) * running_baseline + alpha_baseline * r\n",
    "    advantage = r - running_baseline\n",
    "    # loss is negative advantage times logprob sum (note logprob_sum is a tensor)\n",
    "    loss = - (advantage) * logprob_sum\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    return generated_text, r, advantage\n",
    "\n",
    "# visualization helpers\n",
    "def plot_rewards(history_unshaped, history_shaped):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(history_unshaped, label=\"unshaped (sentiment only)\")\n",
    "    plt.plot(history_shaped, label=\"shaped (penalties applied)\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"reward\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Reward over steps\")\n",
    "    plt.show()\n",
    "\n",
    "def show_samples(samples, title):\n",
    "    print(\"----\", title, \"----\")\n",
    "    for i, s in enumerate(samples):\n",
    "        print(f\"{i+1}.\", s.replace(\"\\n\", \" \")[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89209e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_prompts = [\n",
    "    \"The movie was\",\n",
    "    \"I felt that the film\",\n",
    "    \"The lead actor performance\",\n",
    "]\n",
    "\n",
    "print(\"=== Baseline samples (no policy updates yet) ===\")\n",
    "for p in seed_prompts:\n",
    "    out = sample_for_eval(p, max_new_tokens=12, temperature=0.8)\n",
    "    print(\"PROMPT:\", p)\n",
    "    print(\"GENERATED:\", out)\n",
    "    print(\"REWARD (sentiment):\", sentiment_reward(out))\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial weights to compare later\n",
    "initial_state = deepcopy(policy.state_dict())\n",
    "\n",
    "print(\"Starting reward-hacking phase (unshaped sentiment reward).\")\n",
    "steps = 40  # small, quick demo — increase to observe stronger hacking\n",
    "history_unshaped = []\n",
    "samples_unshaped = []\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(steps):\n",
    "    prompt = random.choice(seed_prompts)\n",
    "    txt, r, adv = reinforce_update(prompt, shaped=False, max_new_tokens=10, temperature=0.8)\n",
    "    history_unshaped.append(r)\n",
    "    if step % 5 == 0:\n",
    "        samples_unshaped.append(txt)\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[{step:02d}] reward {r:.3f} adv {adv:.3f} sample: {txt[:120]}\")\n",
    "print(\"Hacking phase done in {:.1f}s\".format(time.time()-start_time))\n",
    "\n",
    "# show some samples\n",
    "show_samples(samples_unshaped, \"Samples from hacked policy (periodic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetition scores for collected periodic samples\n",
    "rep_scores = [repetition_penalty(s) for s in samples_unshaped]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(history_unshaped, label=\"unshaped reward\")\n",
    "plt.title(\"Unshaped Reward over steps\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"sentiment reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(range(len(rep_scores)), rep_scores)\n",
    "plt.title(\"Repetition penalty for periodic hacked samples\")\n",
    "plt.xlabel(\"sample idx\")\n",
    "plt.ylabel(\"rep penalty\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Teaching note (brief): at this stage you will often see the policy produce short, \n",
    "# highly positive repeated phrases (e.g., \"great great great\"). That's exactly \n",
    "# reward-hacking: the policy finds easy high-reward patterns the oracle values, \n",
    "# even if they're undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore initial weights for fair comparison\n",
    "policy.load_state_dict(initial_state)\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-5)  # re-init optimizer\n",
    "\n",
    "print(\"Running shaped REINFORCE (penalties applied).\")\n",
    "history_shaped = []\n",
    "samples_shaped = []\n",
    "\n",
    "start_time = time.time()\n",
    "for step in range(40):  # same budget\n",
    "    prompt = random.choice(seed_prompts)\n",
    "    txt, r, adv = reinforce_update(prompt, shaped=True, max_new_tokens=10, temperature=0.8)\n",
    "    history_shaped.append(r)\n",
    "    if step % 5 == 0:\n",
    "        samples_shaped.append(txt)\n",
    "    if step % 10 == 0:\n",
    "        print(f\"[{step:02d}] shaped_reward {r:.3f} adv {adv:.3f} sample: {txt[:120]}\")\n",
    "print(\"Shaped phase done in {:.1f}s\".format(time.time()-start_time))\n",
    "\n",
    "show_samples(samples_shaped, \"Samples from shaped policy (periodic)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting rewards side-by-side\n",
    "plot_rewards(history_unshaped, history_shaped)\n",
    "\n",
    "# histograms of repetition penalty across samples (use more samples via evaluation)\n",
    "def collect_samples(policy_mode, n=50):\n",
    "    out_samples = []\n",
    "    for _ in range(n):\n",
    "        p = random.choice(seed_prompts)\n",
    "        s = sample_for_eval(p, max_new_tokens=12, temperature=0.8)\n",
    "        out_samples.append(s)\n",
    "    return out_samples\n",
    "\n",
    "samples_before = collect_samples(\"unshaped\", n=50)  # sampled from current shaped policy? we restored earlier - fine\n",
    "rep_before = [repetition_penalty(s) for s in samples_before]\n",
    "samples_after = collect_samples(\"shaped\", n=50)\n",
    "rep_after = [repetition_penalty(s) for s in samples_after]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.hist(rep_before, bins=10, alpha=0.6, label=\"unshaped (eval)\")\n",
    "plt.hist(rep_after, bins=10, alpha=0.6, label=\"shaped (eval)\")\n",
    "plt.legend()\n",
    "plt.title(\"Repetition penalty distribution (eval samples)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Avg rep (unshaped eval):\", np.mean(rep_before))\n",
    "print(\"Avg rep (shaped eval):\", np.mean(rep_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511fc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fair side-by-side, restore initial state, then sample:\n",
    "policy.load_state_dict(initial_state)  # fresh start\n",
    "# generate unshaped samples by applying a few REINFORCE unshaped steps to a copy\n",
    "policy_unshaped = deepcopy(policy)\n",
    "optimizer_un = torch.optim.AdamW(policy_unshaped.parameters(), lr=1e-5)\n",
    "# perform a few unshaped updates on copy to simulate hacking policy\n",
    "# temporarily swap global policy to use the copy\n",
    "orig_policy_state = deepcopy(policy.state_dict())\n",
    "policy.load_state_dict(policy_unshaped.state_dict())\n",
    "for _ in range(25):\n",
    "    prompt = random.choice(seed_prompts)\n",
    "    # sample with logprobs using policy_unshaped (via global policy swap)\n",
    "    generated_ids, logprob_sum = sample_with_logprobs_train(policy_tok(prompt, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=10)\n",
    "    gen_text = policy_tok.decode(generated_ids[0].cpu().numpy(), skip_special_tokens=True)\n",
    "    r = sentiment_reward(gen_text)\n",
    "    # simple REINFORCE update on copy\n",
    "    loss = - (r - 0.0) * logprob_sum\n",
    "    optimizer_un.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_unshaped.parameters(), 1.0)\n",
    "    optimizer_un.step()\n",
    "    # sync global policy with copy\n",
    "    policy.load_state_dict(policy_unshaped.state_dict())\n",
    "\n",
    "# now shaped policy by running shaped updates on another copy\n",
    "policy.load_state_dict(initial_state)  # reset to initial\n",
    "policy_shaped = deepcopy(policy)\n",
    "optimizer_sh = torch.optim.AdamW(policy_shaped.parameters(), lr=1e-5)\n",
    "policy.load_state_dict(policy_shaped.state_dict())\n",
    "for _ in range(25):\n",
    "    prompt = random.choice(seed_prompts)\n",
    "    generated_ids, logprob_sum = sample_with_logprobs_train(policy_tok(prompt, return_tensors=\"pt\").input_ids.to(device), max_new_tokens=10)\n",
    "    gen_text = policy_tok.decode(generated_ids[0].cpu().numpy(), skip_special_tokens=True)\n",
    "    r = shaped_reward(gen_text)\n",
    "    loss = - (r - 0.0) * logprob_sum\n",
    "    optimizer_sh.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(policy_shaped.parameters(), 1.0)\n",
    "    optimizer_sh.step()\n",
    "    # sync global policy with copy\n",
    "    policy.load_state_dict(policy_shaped.state_dict())\n",
    "\n",
    "# now sample from both\n",
    "print(\"=== Samples from hacked (unshaped-trained) policy copy ===\")\n",
    "policy.load_state_dict(policy_unshaped.state_dict())\n",
    "for p in seed_prompts:\n",
    "    txt = sample_for_eval(p, max_new_tokens=12, temperature=0.8)\n",
    "    print(\"-\", txt)\n",
    "\n",
    "print(\"\\n=== Samples from shaped-trained policy copy ===\")\n",
    "policy.load_state_dict(policy_shaped.state_dict())\n",
    "for p in seed_prompts:\n",
    "    txt = sample_for_eval(p, max_new_tokens=12, temperature=0.8)\n",
    "    print(\"-\", txt)\n",
    "\n",
    "\n",
    "# Note: the gallery above uses small update budgets (25 steps) to keep the demo quick. \n",
    "# In practice, reward-hacking becomes more extreme with more updates; shaping penalties \n",
    "# scale with usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54407c20",
   "metadata": {},
   "source": [
    "### What you just saw (short)\n",
    "- The policy optimizes whatever scalar reward you give it. A naive sentiment oracle can be easily exploited (repetition, short slogans).  \n",
    "- Reward shaping (penalties for repetition, profanity, short answers) changes the optimization objective so that the policy prefers more natural outputs.  \n",
    "- This demo is a minimal educational REINFORCE loop — real RLHF uses learned reward models, large human-labeled preference datasets, advantage normalization, and PPO-like updates with KL-constraints to limit policy drift.  \n",
    "- Important engineering lessons: always inspect outputs, track simple metrics (repetition score, profanity hits, length), and combine multiple orthogonal reward/signals rather than a single scalar.\n",
    "\n",
    "### Next steps you can drop into the notebook\n",
    "- Replace the toy profanity list with a real classifier.\n",
    "- Train a small reward model from human preference pairs instead of using sentiment.\n",
    "- Swap REINFORCE for a tiny PPO implementation (requires value baseline).\n",
    "- Add KL-penalty to keep the tuned policy close to the original LM (prevents collapse)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
