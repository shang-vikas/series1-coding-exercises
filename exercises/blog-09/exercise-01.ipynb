{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a52f799",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-01.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 3 â€” Real Pretraining on IMDB (Next-Token Prediction)\n",
    "\n",
    "We train a small decoder-only Transformer as a language model on IMDB reviews.\n",
    "\n",
    "This is real pretraining objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31988119",
   "metadata": {},
   "source": [
    "## Install + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers accelerate -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import math\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f49bb23",
   "metadata": {},
   "source": [
    "## Load Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888747b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Use unsupervised split for language modeling\n",
    "train_texts = dataset[\"unsupervised\"][\"text\"][:20000]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch, truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\") for t in train_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902b050",
   "metadata": {},
   "source": [
    "## Build Simple Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(128, d_model)\n",
    "        decoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(decoder_layer, num_layers)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t = x.shape\n",
    "        pos = torch.arange(t, device=x.device).unsqueeze(0)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "        x = self.transformer(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n",
    "\n",
    "model = TinyGPT(tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00b4be",
   "metadata": {},
   "source": [
    "## Training Loop (Real Pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(encoded, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} | Loss {avg_loss:.4f} | Perplexity {math.exp(avg_loss):.2f} | Time {time.time()-start:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d241c73",
   "metadata": {},
   "source": [
    "## Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f44a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_tokens=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        logits = model(input_ids)\n",
    "        next_token_logits = logits[:, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate(\"This movie was absolutely\", temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c9821",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What They Will See\n",
    "\n",
    "- Loss decreases\n",
    "- Perplexity decreases\n",
    "- Model generates IMDB-style continuation\n",
    "- No concept of \"answering\" â€” just continuation\n",
    "\n",
    "This is real pretraining behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8bb2b",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 4 â€” Real Instruction Tuning (SFT)\n",
    "\n",
    "Now we fine-tune the same model on real instruction data.\n",
    "\n",
    "We use a small open Alpaca subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4543d4",
   "metadata": {},
   "source": [
    "## Load Instruction Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7930bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:5000]\")\n",
    "\n",
    "def format_example(example):\n",
    "    prompt = f\"### Instruction:\\n{example['instruction']}\\n### Response:\\n{example['output']}\"\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "sft_data = [format_example(ex) for ex in alpaca]\n",
    "sft_loader = DataLoader(sft_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e9257",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in sft_loader:\n",
    "        input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "        inputs = input_ids[:, :-1]\n",
    "        targets = input_ids[:, 1:]\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(\"SFT Loss:\", total_loss / len(sft_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b39906",
   "metadata": {},
   "source": [
    "## Compare Before vs After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86f1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\"Explain gravity simply.\", temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a142769e",
   "metadata": {},
   "source": [
    "Now the model answers.\n",
    "\n",
    "Because instructionâ†’response patterns were injected.\n",
    "\n",
    "No architecture changed.\n",
    "\n",
    "Only distribution changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead0c4d",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 5 â€” Preference Steering (Simplified Real Ranking)\n",
    "\n",
    "We simulate preference optimization using IMDB sentiment.\n",
    "\n",
    "Goal: teach model to prefer positive tone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b2675",
   "metadata": {},
   "source": [
    "## Create Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_reviews = [t for t in train_texts if \"good\" in t.lower()][:1000]\n",
    "negative_reviews = [t for t in train_texts if \"bad\" in t.lower()][:1000]\n",
    "\n",
    "# We treat positive as preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d2da2",
   "metadata": {},
   "source": [
    "## Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849aad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        return self.linear(pooled)\n",
    "\n",
    "reward_model = RewardModel(256).to(device)\n",
    "reward_optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebd3fa7",
   "metadata": {},
   "source": [
    "## Train Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    pos = tokenizer(positive_reviews[i], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids.to(device)\n",
    "    neg = tokenizer(negative_reviews[i], truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    pos_hidden = model.token_emb(pos)\n",
    "    neg_hidden = model.token_emb(neg)\n",
    "\n",
    "    pos_score = reward_model(pos_hidden)\n",
    "    neg_score = reward_model(neg_hidden)\n",
    "\n",
    "    loss = F.relu(1 - pos_score + neg_score).mean()\n",
    "\n",
    "    reward_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    reward_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46700f5",
   "metadata": {},
   "source": [
    "## Use Reward to Reweight LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.0\n",
    "\n",
    "for batch in loader:\n",
    "    input_ids = batch[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "    inputs = input_ids[:, :-1]\n",
    "    targets = input_ids[:, 1:]\n",
    "\n",
    "    logits = model(inputs)\n",
    "    lm_loss = criterion(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
    "\n",
    "    hidden = model.token_emb(inputs)\n",
    "    reward = reward_model(hidden).mean()\n",
    "\n",
    "    weighted_loss = lm_loss - alpha * reward\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    weighted_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44025d",
   "metadata": {},
   "source": [
    "Now test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c751f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\"This movie was\", temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff856b9",
   "metadata": {},
   "source": [
    "Tone shifts slightly positive.\n",
    "\n",
    "This demonstrates alignment mechanics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209a499",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 6 â€” Show Hallucination Mechanically\n",
    "\n",
    "Ask model something outside IMDB distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416898bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(\"The capital of Atlantis is\", temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a91649",
   "metadata": {},
   "source": [
    "It will invent.\n",
    "\n",
    "Because it learned patterns of answering, not truth retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f30fe",
   "metadata": {},
   "source": [
    "## ðŸ’¡ What This Entire Notebook Demonstrates\n",
    "\n",
    "- Pretraining = next-token prediction on real text\n",
    "- SFT = behavior steering via distribution shift\n",
    "- Preference signals = probability reweighting\n",
    "- No truth module\n",
    "- Hallucination is structural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b95bca",
   "metadata": {},
   "source": [
    "## ðŸ’¡ What This Does NOT Demonstrate\n",
    "\n",
    "- Billion-scale representation power\n",
    "- RLHF full pipeline\n",
    "- True large-scale generalization\n",
    "\n",
    "But the mechanisms are identical.\n",
    "\n",
    "Scale changes capability.\n",
    "\n",
    "Not objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42edb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0eb0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da7df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1836035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b16d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ad02e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
