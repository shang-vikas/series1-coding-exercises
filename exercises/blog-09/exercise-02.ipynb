{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ec4e92",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-02.ipynb)\n",
    "\n",
    "# üöÄ Notebook: From Pretraining to Tiny RLHF\n",
    "\n",
    "This will include:\n",
    "\n",
    "- Real dataset (Tiny Shakespeare)\n",
    "- Tiny decoder Transformer\n",
    "- Log-loss & perplexity visualization\n",
    "- Gradient norm tracking\n",
    "- Token probability inspection\n",
    "- Confidence calibration analysis\n",
    "- Minimal RLHF-style PPO loop (educational, not production)\n",
    "\n",
    "Everything wired end-to-end.\n",
    "\n",
    "This is long because it's real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec63c84",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch transformers datasets matplotlib numpy\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3b646",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Real Dataset (Tiny Shakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ecf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "text = dataset[\"train\"][0][\"text\"]\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548fb7ae",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eac844",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "print(\"Total tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e236ef",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Dataset Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d43745",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 64\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size):\n",
    "        self.tokens = tokens\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.tokens[idx:idx+self.block_size]\n",
    "        y = self.tokens[idx+1:idx+self.block_size+1]\n",
    "        return x, y\n",
    "\n",
    "train_dataset = TextDataset(tokens, block_size)\n",
    "loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509f524",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tiny Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e2451",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=256,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(layer, num_layers=n_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "\n",
    "        pos = torch.arange(0, T, device=x.device).unsqueeze(0)\n",
    "        x = self.token_emb(x) + self.pos_emb(pos)\n",
    "\n",
    "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
    "        x = self.transformer(x, mask)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a243d0",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c580fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = TinyTransformer(tokenizer.vocab_size).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7521126",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Pretraining Loop with Full Instrumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "perplexity_history = []\n",
    "grad_history = []\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item() ** 2\n",
    "        grad_norm = total_norm ** 0.5\n",
    "        grad_history.append(grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    loss_history.append(avg_loss)\n",
    "    perplexity_history.append(math.exp(avg_loss))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss {avg_loss:.4f} | Perplexity {math.exp(avg_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819accde",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Log-Loss & Perplexity Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b6c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title(\"Training Log Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(perplexity_history)\n",
    "plt.title(\"Perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b578ae",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Token Probability Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "prompt = \"To be or not to\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids)\n",
    "    probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "topk = torch.topk(probs, 10)\n",
    "\n",
    "for idx, prob in zip(topk.indices[0], topk.values[0]):\n",
    "    print(tokenizer.decode(idx.item()), float(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9839d1a3",
   "metadata": {},
   "source": [
    "This shows probability mass ‚Äî not \"answer retrieval.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ece8c",
   "metadata": {},
   "source": [
    "## üîü Confidence Calibration Analysis\n",
    "\n",
    "We compare predicted confidence vs actual correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b85ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "confidences = []\n",
    "correctness = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        preds = probs.argmax(dim=-1)\n",
    "        max_probs = probs.max(dim=-1).values\n",
    "\n",
    "        correct = (preds == y).float()\n",
    "\n",
    "        confidences.extend(max_probs.cpu().flatten().numpy())\n",
    "        correctness.extend(correct.cpu().flatten().numpy())\n",
    "\n",
    "confidences = np.array(confidences)\n",
    "correctness = np.array(correctness)\n",
    "\n",
    "bins = np.linspace(0, 1, 10)\n",
    "bin_ids = np.digitize(confidences, bins)\n",
    "\n",
    "bin_acc = []\n",
    "bin_conf = []\n",
    "\n",
    "for b in range(1, len(bins)):\n",
    "    mask = bin_ids == b\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc.append(correctness[mask].mean())\n",
    "        bin_conf.append(confidences[mask].mean())\n",
    "\n",
    "plt.plot(bin_conf, bin_acc, marker=\"o\")\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Calibration Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f685f9",
   "metadata": {},
   "source": [
    "Perfect calibration would lie on diagonal.\n",
    "\n",
    "It won't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a16b7",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Minimal RLHF-Style PPO Loop (Educational)\n",
    "\n",
    "This is simplified.\n",
    "\n",
    "We simulate a \"reward model\" that prefers shorter responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b654a",
   "metadata": {},
   "source": [
    "### Generate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07c5e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ids(model, input_ids, max_new_tokens=20):\n",
    "    model.eval()\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(input_ids)\n",
    "        probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03077a",
   "metadata": {},
   "source": [
    "### Fake Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356adda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(generated_ids):\n",
    "    return -generated_ids.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9e1d9f",
   "metadata": {},
   "source": [
    "### PPO-Like Update (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for step in range(5):\n",
    "    prompt = \"To be\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    generated = generate_ids(model, input_ids)\n",
    "\n",
    "    reward = reward_function(generated)\n",
    "\n",
    "    logits = model(generated[:, :-1])\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    selected_log_probs = log_probs.gather(\n",
    "        2, generated[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    policy_loss = -(selected_log_probs.mean() * reward)\n",
    "\n",
    "    ppo_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    ppo_optimizer.step()\n",
    "\n",
    "    print(\"Step\", step, \"Reward\", reward.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094bf0a",
   "metadata": {},
   "source": [
    "This is not production PPO.\n",
    "\n",
    "It demonstrates:\n",
    "\n",
    "- sampling\n",
    "- reward\n",
    "- policy gradient\n",
    "- small update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516d54b",
   "metadata": {},
   "source": [
    "## üß† What This Notebook Actually Teaches\n",
    "\n",
    "You now showed:\n",
    "\n",
    "- Cross-entropy minimization\n",
    "- Perplexity\n",
    "- Gradient norms\n",
    "- Calibration gaps\n",
    "- Probabilistic decoding\n",
    "- Reward-based fine-tuning\n",
    "\n",
    "This is the entire LLM training story at educational scale.\n",
    "\n",
    "Not magic.\n",
    "\n",
    "Optimization + probability + scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b3e618",
   "metadata": {},
   "source": [
    "vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0543b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92808d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f9903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab072c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff9af37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bb704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0d8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a1b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338e65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545afb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b8406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283ee3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5cce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02112673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba701257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abba35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9d2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06db21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cdf36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
