{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d371940a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-00.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 1 â€” Tiny next-token pretraining (toy Transformer)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Make next-token prediction explicit: train a tiny Transformer on tiny Shakespeare (or short corpus), plot loss â†’ perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "551ae40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found tiny_shakespeare.py",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# --- Data: tiny shakespeare (small slice) ---\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtiny_shakespeare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m50000\u001b[39m]  \u001b[38;5;66;03m# small\u001b[39;00m\n\u001b[1;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ds)[:\u001b[38;5;241m20000\u001b[39m]  \u001b[38;5;66;03m# keep tiny for speed\u001b[39;00m\n\u001b[1;32m     13\u001b[0m chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(text)))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/datasets/load.py:1488\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1484\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1485\u001b[0m )\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1488\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/datasets/load.py:1133\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1133\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/datasets/load.py:1032\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1028\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1029\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1032\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/datasets/load.py:992\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m     api\u001b[38;5;241m.\u001b[39mhf_hub_download(\n\u001b[1;32m    986\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mpath,\n\u001b[1;32m    987\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    991\u001b[0m     )\n\u001b[0;32m--> 992\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset scripts are no longer supported, but found tiny_shakespeare.py"
     ]
    }
   ],
   "source": [
    "# minimal dependencies\n",
    "%pip install datasets -q\n",
    "\n",
    "import torch, math, time\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- Data: tiny shakespeare (small slice) ---\n",
    "ds = load_dataset(\"tiny_shakespeare\")[\"train\"][\"text\"][:50000]  # small\n",
    "text = \"\\n\".join(ds)[:20000]  # keep tiny for speed\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "def encode(s): return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "def decode(t): return \"\".join(itos[int(x)] for x in t)\n",
    "\n",
    "seq_len = 64\n",
    "examples = [encode(text[i:i+seq_len+1]) for i in range(0, len(text)-seq_len, seq_len)]\n",
    "loader = DataLoader(examples, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f25946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tiny Transformer LM ---\n",
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab, d=128, nhead=4, nlayers=2):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab, d)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=d, nhead=nhead, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=nlayers)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.out = nn.Linear(d, vocab)\n",
    "    def forward(self, x):\n",
    "        b, t = x.shape\n",
    "        positions = torch.arange(t, device=x.device).unsqueeze(0)\n",
    "        h = self.tok_emb(x) + self.pos_emb(positions)\n",
    "        h = self.enc(h)\n",
    "        h = self.ln(h)\n",
    "        return self.out(h)  # logits shape (b, t, vocab)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TinyLM(len(chars)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train one epoch quickly ---\n",
    "model.train()\n",
    "start=time.time()\n",
    "avg_loss=0.0\n",
    "for i,b in enumerate(loader):\n",
    "    b = b.to(device)\n",
    "    inp = b[:, :-1]\n",
    "    tgt = b[:, 1:]\n",
    "    logits = model(inp)  # (B, T, V)\n",
    "    loss = loss_f(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    avg_loss += loss.item()\n",
    "    if i>200: break\n",
    "print(\"avg loss:\", avg_loss/(i+1), \"perplexity:\", math.exp(avg_loss/(i+1)), \"time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8735f5",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "Training minimizes cross-entropy (NLL).\n",
    "\n",
    "Report NLL and perplexity = exp(NLL). Perplexity is the \"effective branching factor.\"\n",
    "\n",
    "This is exactly next-token prediction.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "We Emphasize that the model never \"answers\" â€” it learns statistical continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceff6f0",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 2 â€” Sampling & decoding controls (temperature, top-k, top-p)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Show how the same probability distribution yields very different answers depending on decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_logits(model, prompt, length=100, temp=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    idxs = encode(prompt).unsqueeze(0).to(device)\n",
    "    out = idxs\n",
    "    for _ in range(length):\n",
    "        logits = model(out)[0, -1] / (temp if temp>0 else 1e-9)\n",
    "        # top-k\n",
    "        if top_k>0:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[-1]] = -1e10\n",
    "        # top-p (nucleus)\n",
    "        if top_p>0.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumsum = probs.cumsum(dim=0)\n",
    "            sorted_logits[cumsum > top_p] = -1e10\n",
    "            logits = torch.zeros_like(logits).scatter_(0, sorted_idx, sorted_logits)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        out = torch.cat([out, next_id.unsqueeze(0)], dim=1)\n",
    "    return decode(out[0].cpu().numpy())\n",
    "\n",
    "print(\"temp=0.2:\", sample_logits(model, \"To be, or not\", length=80, temp=0.2))\n",
    "print(\"temp=1.0:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0))\n",
    "print(\"top_k=5:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0, top_k=5))\n",
    "print(\"top_p=0.9:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56abfd",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "Low temperature â†’ conservative, repetitive text (closer to argmax).\n",
    "\n",
    "High temperature â†’ creative but often incoherent.\n",
    "\n",
    "Top-k / top-p shape the tails and control hallucination vs creativity.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "This is why phrasing matters and why LLMs can sound confident but be wrong â€” sampling draws from the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00ab4a",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 3 â€” Pretrain â†’ Instruction-tune (tiny simulation)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Demonstrate how instruction-tuning is just more next-token examples, and how behaviour shifts.\n",
    "\n",
    "## Plan\n",
    "\n",
    "Pretrain toy LM (exercise 1).\n",
    "\n",
    "Create a tiny supervised dataset of instructionâ†’response pairs ( ~100 examples).\n",
    "\n",
    "Fine-tune the same LM on concatenated \"<INST>prompt</INST>response\" examples with teacher forcing (maximize likelihood of response tokens given prompt).\n",
    "\n",
    "Compare model outputs on test prompts before/after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume `model` from Ex 1 is available and device set\n",
    "# 1) make toy SFT dataset\n",
    "sft_pairs = [\n",
    "    (\"Summarize: Why is the sky blue?\", \"Because air scatters sunlight; blue scatters more.\"),\n",
    "    (\"Explain like I'm 5: gravity\", \"Gravity pulls things together; heavy things pull stronger.\"),\n",
    "    # ... add ~100 small pairs; keep tiny\n",
    "]\n",
    "\n",
    "def encode_pair(prompt, response):\n",
    "    seq = (prompt + \" \" + response)\n",
    "    return torch.tensor([stoi[c] for c in seq if c in stoi], dtype=torch.long)\n",
    "\n",
    "sft_examples = [encode_pair(p,r) for p,r in sft_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10562382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) fine-tune with teacher forcing (very short)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(2):\n",
    "    for ex in sft_examples:\n",
    "        ex = ex.to(device)\n",
    "        inp = ex[:-1].unsqueeze(0)\n",
    "        tgt = ex[1:].unsqueeze(0)\n",
    "        logits = model(inp)\n",
    "        loss = loss_f(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "# 3) sample before/after: use sample_logits() earlier\n",
    "print(\"Post SFT sample:\", sample_logits(model, \"Summarize: Why is the sky blue?\", length=60, temp=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9330f8",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "After SFT, the model is more likely to produce helpful continuations matching training style.\n",
    "\n",
    "It's still next-token prediction; you shifted the distribution of contexts it sees.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "SFT reweights model behaviour; it does not implant new internal modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42479936",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "After SFT, the model is more likely to produce helpful continuations matching training style.\n",
    "\n",
    "It's still next-token prediction; you shifted the distribution of contexts it sees.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "SFT reweights model behaviour; it does not implant new internal modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
