{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d371940a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-09/exercise-00.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 1 â€” Tiny next-token pretraining (toy Transformer)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Make next-token prediction explicit: train a tiny Transformer on tiny Shakespeare (or short corpus), plot loss â†’ perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551ae40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimal dependencies\n",
    "%pip install datasets -q\n",
    "\n",
    "import torch, math, time\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# --- Data: tiny shakespeare (small slice) ---\n",
    "ds = load_dataset(\"tiny_shakespeare\")[\"train\"][\"text\"][:50000]  # small\n",
    "text = \"\\n\".join(ds)[:20000]  # keep tiny for speed\n",
    "chars = sorted(list(set(text)))\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for c,i in stoi.items()}\n",
    "\n",
    "def encode(s): return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "def decode(t): return \"\".join(itos[int(x)] for x in t)\n",
    "\n",
    "seq_len = 64\n",
    "examples = [encode(text[i:i+seq_len+1]) for i in range(0, len(text)-seq_len, seq_len)]\n",
    "loader = DataLoader(examples, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f25946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tiny Transformer LM ---\n",
    "class TinyLM(nn.Module):\n",
    "    def __init__(self, vocab, d=128, nhead=4, nlayers=2):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab, d)\n",
    "        self.pos_emb = nn.Embedding(seq_len, d)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=d, nhead=nhead, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, num_layers=nlayers)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "        self.out = nn.Linear(d, vocab)\n",
    "    def forward(self, x):\n",
    "        b, t = x.shape\n",
    "        positions = torch.arange(t, device=x.device).unsqueeze(0)\n",
    "        h = self.tok_emb(x) + self.pos_emb(positions)\n",
    "        h = self.enc(h)\n",
    "        h = self.ln(h)\n",
    "        return self.out(h)  # logits shape (b, t, vocab)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TinyLM(len(chars)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b70793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- train one epoch quickly ---\n",
    "model.train()\n",
    "start=time.time()\n",
    "avg_loss=0.0\n",
    "for i,b in enumerate(loader):\n",
    "    b = b.to(device)\n",
    "    inp = b[:, :-1]\n",
    "    tgt = b[:, 1:]\n",
    "    logits = model(inp)  # (B, T, V)\n",
    "    loss = loss_f(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    avg_loss += loss.item()\n",
    "    if i>200: break\n",
    "print(\"avg loss:\", avg_loss/(i+1), \"perplexity:\", math.exp(avg_loss/(i+1)), \"time:\", time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8735f5",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "Training minimizes cross-entropy (NLL).\n",
    "\n",
    "Report NLL and perplexity = exp(NLL). Perplexity is the \"effective branching factor.\"\n",
    "\n",
    "This is exactly next-token prediction.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "We Emphasize that the model never \"answers\" â€” it learns statistical continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceff6f0",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 2 â€” Sampling & decoding controls (temperature, top-k, top-p)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Show how the same probability distribution yields very different answers depending on decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_logits(model, prompt, length=100, temp=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    idxs = encode(prompt).unsqueeze(0).to(device)\n",
    "    out = idxs\n",
    "    for _ in range(length):\n",
    "        logits = model(out)[0, -1] / (temp if temp>0 else 1e-9)\n",
    "        # top-k\n",
    "        if top_k>0:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[-1]] = -1e10\n",
    "        # top-p (nucleus)\n",
    "        if top_p>0.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(logits, descending=True)\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cumsum = probs.cumsum(dim=0)\n",
    "            sorted_logits[cumsum > top_p] = -1e10\n",
    "            logits = torch.zeros_like(logits).scatter_(0, sorted_idx, sorted_logits)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        out = torch.cat([out, next_id.unsqueeze(0)], dim=1)\n",
    "    return decode(out[0].cpu().numpy())\n",
    "\n",
    "print(\"temp=0.2:\", sample_logits(model, \"To be, or not\", length=80, temp=0.2))\n",
    "print(\"temp=1.0:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0))\n",
    "print(\"top_k=5:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0, top_k=5))\n",
    "print(\"top_p=0.9:\", sample_logits(model, \"To be, or not\", length=80, temp=1.0, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56abfd",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "Low temperature â†’ conservative, repetitive text (closer to argmax).\n",
    "\n",
    "High temperature â†’ creative but often incoherent.\n",
    "\n",
    "Top-k / top-p shape the tails and control hallucination vs creativity.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "This is why phrasing matters and why LLMs can sound confident but be wrong â€” sampling draws from the learned distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a00ab4a",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 3 â€” Pretrain â†’ Instruction-tune (tiny simulation)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Demonstrate how instruction-tuning is just more next-token examples, and how behaviour shifts.\n",
    "\n",
    "## Plan\n",
    "\n",
    "Pretrain toy LM (exercise 1).\n",
    "\n",
    "Create a tiny supervised dataset of instructionâ†’response pairs ( ~100 examples).\n",
    "\n",
    "Fine-tune the same LM on concatenated \"<INST>prompt</INST>response\" examples with teacher forcing (maximize likelihood of response tokens given prompt).\n",
    "\n",
    "Compare model outputs on test prompts before/after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eb5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume `model` from Ex 1 is available and device set\n",
    "# 1) make toy SFT dataset\n",
    "sft_pairs = [\n",
    "    (\"Summarize: Why is the sky blue?\", \"Because air scatters sunlight; blue scatters more.\"),\n",
    "    (\"Explain like I'm 5: gravity\", \"Gravity pulls things together; heavy things pull stronger.\"),\n",
    "    # ... add ~100 small pairs; keep tiny\n",
    "]\n",
    "\n",
    "def encode_pair(prompt, response):\n",
    "    seq = (prompt + \" \" + response)\n",
    "    return torch.tensor([stoi[c] for c in seq if c in stoi], dtype=torch.long)\n",
    "\n",
    "sft_examples = [encode_pair(p,r) for p,r in sft_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10562382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) fine-tune with teacher forcing (very short)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "for epoch in range(2):\n",
    "    for ex in sft_examples:\n",
    "        ex = ex.to(device)\n",
    "        inp = ex[:-1].unsqueeze(0)\n",
    "        tgt = ex[1:].unsqueeze(0)\n",
    "        logits = model(inp)\n",
    "        loss = loss_f(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "# 3) sample before/after: use sample_logits() earlier\n",
    "print(\"Post SFT sample:\", sample_logits(model, \"Summarize: Why is the sky blue?\", length=60, temp=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9330f8",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "After SFT, the model is more likely to produce helpful continuations matching training style.\n",
    "\n",
    "It's still next-token prediction; you shifted the distribution of contexts it sees.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "SFT reweights model behaviour; it does not implant new internal modules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42479936",
   "metadata": {},
   "source": [
    "## ðŸ”Ž What to Observe\n",
    "\n",
    "After SFT, the model is more likely to produce helpful continuations matching training style.\n",
    "\n",
    "It's still next-token prediction; you shifted the distribution of contexts it sees.\n",
    "\n",
    "## ðŸ’¡ Teaching Note\n",
    "\n",
    "SFT reweights model behaviour; it does not implant new internal modules."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
