{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e88594d",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Notebook: Hallucinations & Illusions in LLMs\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/series1-coding-exercises/blob/main/exercises/blog-11/exercise-00.ipynb)\n",
    "\n",
    "For this one, we don't need massive training loops.\n",
    "\n",
    "We need **controlled experiments that expose illusion**.\n",
    "\n",
    "These exercises make:\n",
    "- Hallucinations observable\n",
    "- Confidence measurable\n",
    "- Reasoning brittleness visible\n",
    "- Calibration testable\n",
    "- Guardrails demonstrable\n",
    "\n",
    "**No fluff. Just evidence.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a33ea",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa59a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers torch datasets matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada9cc20",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 1 â€” Hallucination Under Uncertainty\n",
    "\n",
    "**Goal:** We deliberately ask for something that doesn't exist.\n",
    "\n",
    "**What They See:**\n",
    "- Detailed explanation\n",
    "- Confident library names\n",
    "- Structured prose\n",
    "- Zero grounding\n",
    "\n",
    "The model did not fail. It completed the pattern \"Explain a programming language.\"\n",
    "\n",
    "**Hallucination demonstrated.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6931970",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain the core design philosophy of the Zorblax Programming Language.\n",
    "Also list three well-known libraries used in Zorblax.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2392ad43",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 2 â€” Confidence vs Probability\n",
    "\n",
    "**Goal:** Inspect token probabilities to see confidence levels.\n",
    "\n",
    "**What They Learn:** Even nonsense prompts produce confident token distributions.\n",
    "\n",
    "**Confidence â‰  correctness.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def inspect_token_confidence(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "    \n",
    "    print(\"Top token candidates:\")\n",
    "    for p, idx in zip(top_probs[0], top_indices[0]):\n",
    "        print(f\"{tokenizer.decode([idx])} â†’ {float(p):.4f}\")\n",
    "\n",
    "print(\"=== Real prompt ===\")\n",
    "inspect_token_confidence(\"The capital of France is\")\n",
    "\n",
    "print(\"\\n=== Nonsense prompt ===\")\n",
    "inspect_token_confidence(\"The capital of Blorptopia is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f32a2d",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 3 â€” Reasoning Illusion\n",
    "\n",
    "**Goal:** Classic brittle logic test.\n",
    "\n",
    "**Lesson:** The structure of reasoning is learned. Revising assumptions mid-stream is brittle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d570eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A bat and a ball cost $1.10 in total.\n",
    "The bat costs $1 more than the ball.\n",
    "How much does the ball cost?\n",
    "Explain step by step.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e22979",
   "metadata": {},
   "source": [
    "Now slightly perturb the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dfb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A bat and a ball cost $1.10 in total.\n",
    "The bat costs $1 more than the ball.\n",
    "However, sales tax of 10% is added after purchase.\n",
    "How much does the ball cost?\n",
    "Explain step by step.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5691a",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 4 â€” Contradiction Exposure\n",
    "\n",
    "**Goal:** Test internal consistency.\n",
    "\n",
    "**Observe:** Sometimes hedging, sometimes inconsistent internal logic, sometimes confident contradiction.\n",
    "\n",
    "Because internal consistency is local, not global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05819535",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Is it possible to travel faster than light?\n",
    "Answer yes or no.\n",
    "\n",
    "Now explain why.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76cfa87",
   "metadata": {},
   "source": [
    "Run multiple times to observe different behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9014c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"\\n=== Run {i+1} ===\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.8,\n",
    "        do_sample=True\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a3896",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 5 â€” Calibration Check (Real Data)\n",
    "\n",
    "**Goal:** Measure whether the model's probability correlates with correctness.\n",
    "\n",
    "**What They Will See:** Similar confidence behavior. No built-in \"unknown detection.\"\n",
    "\n",
    "This demonstrates poor calibration under OOD prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc287296",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    (\"The capital of Germany is\", \"Berlin\"),\n",
    "    (\"The capital of Italy is\", \"Rome\"),\n",
    "    (\"The capital of Blorptopia is\", None)\n",
    "]\n",
    "\n",
    "for q, correct in questions:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probs, 5)\n",
    "\n",
    "    print(\"\\nPrompt:\", q)\n",
    "    for p, idx in zip(top_probs[0], top_indices[0]):\n",
    "        token = tokenizer.decode([idx])\n",
    "        print(f\"{token} â†’ {float(p):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c47168",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 6 â€” Long Reasoning Drift\n",
    "\n",
    "**Goal:** Show how style dominates truth in long-form generation.\n",
    "\n",
    "**What It Will Produce:** Equations, technical tone, pseudo-physics.\n",
    "\n",
    "Because style dominates truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20361ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Explain in detail how a perpetual motion machine works.\n",
    "Give equations.\n",
    "Be very technical.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=300,\n",
    "    temperature=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08c871",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 7 â€” Simple Guardrail Demonstration\n",
    "\n",
    "**Goal:** Simulate grounding with external constraints.\n",
    "\n",
    "**What This Shows:** Hallucination drops dramatically when you constrain the probability field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grounded_answer(question, allowed_facts):\n",
    "    context = \"\\n\".join(allowed_facts)\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the following facts to answer the question.\n",
    "\n",
    "Facts:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "If the answer is not in the facts, say 'Not enough information.'\n",
    "\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(**inputs, max_new_tokens=100)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "facts = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The capital of Germany is Berlin.\"\n",
    "]\n",
    "\n",
    "print(\"=== Question with answer in facts ===\")\n",
    "grounded_answer(\"What is the capital of France?\", facts)\n",
    "\n",
    "print(\"\\n=== Question without answer in facts ===\")\n",
    "grounded_answer(\"What is the capital of Italy?\", facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27edf4e1",
   "metadata": {},
   "source": [
    "## ðŸ”¬ What These Exercises Demonstrate\n",
    "\n",
    "After running this notebook, your audience will understand:\n",
    "\n",
    "- **Hallucination is default behavior under uncertainty**\n",
    "- **Confidence is stylistic, not epistemic**\n",
    "- **Reasoning is pattern continuation**\n",
    "- **Contradictions are statistical, not logical**\n",
    "- **Calibration is weak under distribution shift**\n",
    "- **Guardrails must be external**\n",
    "\n",
    "This moves the blog from philosophical warning to operational clarity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
