{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-01/exercise-01.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ numpy is already installed\n",
            "✓ scikit-learn is already installed\n",
            "Installing pandas...\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shang/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/shang/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n",
            "✓ pandas installed successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"✓ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✓ {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"numpy\")\n",
        "install_if_missing(\"scikit-learn\", \"sklearn\")\n",
        "install_if_missing(\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the dataset:\n",
            "\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Dataset shape: (569, 31)\n",
            "\n",
            "Class distribution:\n",
            "\n",
            "target\n",
            "1    357\n",
            "0    212\n",
            "Name: count, dtype: int64\n",
            "\n",
            "===== Decision Tree =====\n",
            "Accuracy: 0.9415204678362573\n",
            "Precision: 0.9711538461538461\n",
            "Recall: 0.9351851851851852\n",
            "F1 Score: 0.9528301886792453\n",
            "ROC-AUC: 0.9437830687830687\n",
            "Confusion Matrix:\n",
            " [[ 60   3]\n",
            " [  7 101]]\n",
            "\n",
            "===== Random Forest =====\n",
            "Accuracy: 0.9707602339181286\n",
            "Precision: 0.963963963963964\n",
            "Recall: 0.9907407407407407\n",
            "F1 Score: 0.9771689497716894\n",
            "ROC-AUC: 0.9968400940623163\n",
            "Confusion Matrix:\n",
            " [[ 59   4]\n",
            " [  1 107]]\n",
            "\n",
            "===== SVM (Linear) =====\n",
            "Accuracy: 0.9766081871345029\n",
            "Precision: 0.9814814814814815\n",
            "Recall: 0.9814814814814815\n",
            "F1 Score: 0.9814814814814815\n",
            "ROC-AUC: 0.9964726631393298\n",
            "Confusion Matrix:\n",
            " [[ 61   2]\n",
            " [  2 106]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "print(\"First 5 rows of the dataset:\\n\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "print(\"\\nClass distribution:\\n\")\n",
        "print(df[\"target\"].value_counts())\n",
        "\n",
        "# Split data\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale for SVM (important)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM (Linear)\": SVC(kernel=\"linear\", probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    \n",
        "    if \"SVM\" in name:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding Classification Metrics\n",
        "\n",
        "This is where understanding separates button-pushers from practitioners.\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "The confusion matrix is the raw truth table of predictions. Everything else is built on top of it.\n",
        "\n",
        "It shows:\n",
        "\n",
        "True Positives (TP) – Correctly predicted malignant\n",
        "\n",
        "True Negatives (TN) – Correctly predicted benign\n",
        "\n",
        "False Positives (FP) – Predicted malignant but actually benign\n",
        "\n",
        "False Negatives (FN) – Predicted benign but actually malignant\n",
        "\n",
        "In cancer detection, false negatives are extremely dangerous. That’s not just math — that’s a missed diagnosis.\n",
        "\n",
        "Accuracy\n",
        "Accuracy=Correct PredictionsTotal Predictions\n",
        "Accuracy=\n",
        "Total Predictions\n",
        "Correct Predictions\n",
        "\t​\n",
        "\n",
        "\n",
        "Accuracy is intuitive. It answers:\n",
        "\n",
        "“How often is the model right?”\n",
        "\n",
        "The problem? It can be misleading when classes are imbalanced.\n",
        "\n",
        "If 95% of patients are healthy, a model that always predicts “healthy” gets 95% accuracy.\n",
        "That model is useless.\n",
        "\n",
        "Accuracy ignores what kind of mistakes are being made.\n",
        "\n",
        "Precision\n",
        "Precision=TPTP+FP\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Out of all predicted positives, how many were actually positive?\n",
        "\n",
        "Precision answers:\n",
        "\n",
        "“When the model says ‘malignant,’ how often is it correct?”\n",
        "\n",
        "If precision is low, you’re raising too many false alarms.\n",
        "\n",
        "Recall (Sensitivity)\n",
        "Recall=TPTP+FN\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "\t​\n",
        "\n",
        "\n",
        "Out of all actual positives, how many did you catch?\n",
        "\n",
        "Recall answers:\n",
        "\n",
        "“Of all the malignant cases, how many did we detect?”\n",
        "\n",
        "In medical diagnosis, recall often matters more than precision.\n",
        "Missing cancer is worse than ordering an unnecessary follow-up test.\n",
        "\n",
        "F1 Score\n",
        "F1=Harmonic Mean of Precision and Recall\n",
        "F1=Harmonic Mean of Precision and Recall\n",
        "\n",
        "The harmonic mean penalizes extreme imbalance between precision and recall.\n",
        "\n",
        "If one is high and the other is low, F1 drops sharply.\n",
        "\n",
        "F1 is useful when you want a balanced trade-off between catching positives and avoiding false alarms.\n",
        "\n",
        "ROC–AUC\n",
        "\n",
        "ROC stands for Receiver Operating Characteristic.\n",
        "AUC stands for Area Under the Curve.\n",
        "\n",
        "ROC–AUC measures how well the model separates classes across all possible thresholds.\n",
        "\n",
        "1.0 → Perfect separation\n",
        "\n",
        "0.5 → Random guessing\n",
        "\n",
        "It is threshold-independent, meaning it evaluates ranking ability, not just final yes/no predictions.\n",
        "\n",
        "It answers:\n",
        "\n",
        "“How well does the model distinguish malignant from benign overall?”\n",
        "\n",
        "Model Characteristics\n",
        "Decision Tree\n",
        "\n",
        "Interpretable (like a flowchart)\n",
        "\n",
        "Easy to explain\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Many trees voting together\n",
        "\n",
        "Reduces variance\n",
        "\n",
        "Usually strong accuracy and AUC\n",
        "\n",
        "Harder to interpret\n",
        "\n",
        "SVM (Support Vector Machine)\n",
        "\n",
        "Strong theoretical foundation\n",
        "\n",
        "Performs well in high-dimensional space (like 30-feature medical data)\n",
        "\n",
        "Sensitive to feature scaling\n",
        "\n",
        "Less interpretable\n",
        "\n",
        "The Real Point\n",
        "\n",
        "Machine learning is not about picking “the best model.”\n",
        "\n",
        "It’s about choosing the right trade-off for the problem.\n",
        "\n",
        "In cancer detection → optimize for recall\n",
        "\n",
        "In spam detection → optimize for precision\n",
        "\n",
        "In finance → false positives and false negatives have different dollar costs\n",
        "\n",
        "Metrics are not just numbers.\n",
        "\n",
        "They encode priorities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
