{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-01/exercise-01.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ numpy is already installed\n",
            "âœ“ scikit-learn is already installed\n",
            "Installing pandas...\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/shang/Library/Python/3.10/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/shang/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.3\n",
            "âœ“ pandas installed successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"âœ“ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ“ {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"numpy\")\n",
        "install_if_missing(\"scikit-learn\", \"sklearn\")\n",
        "install_if_missing(\"pandas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the dataset:\n",
            "\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "Dataset shape: (569, 31)\n",
            "\n",
            "Class distribution:\n",
            "\n",
            "target\n",
            "1    357\n",
            "0    212\n",
            "Name: count, dtype: int64\n",
            "\n",
            "===== Decision Tree =====\n",
            "Accuracy: 0.9415204678362573\n",
            "Precision: 0.9711538461538461\n",
            "Recall: 0.9351851851851852\n",
            "F1 Score: 0.9528301886792453\n",
            "ROC-AUC: 0.9437830687830687\n",
            "Confusion Matrix:\n",
            " [[ 60   3]\n",
            " [  7 101]]\n",
            "\n",
            "===== Random Forest =====\n",
            "Accuracy: 0.9707602339181286\n",
            "Precision: 0.963963963963964\n",
            "Recall: 0.9907407407407407\n",
            "F1 Score: 0.9771689497716894\n",
            "ROC-AUC: 0.9968400940623163\n",
            "Confusion Matrix:\n",
            " [[ 59   4]\n",
            " [  1 107]]\n",
            "\n",
            "===== SVM (Linear) =====\n",
            "Accuracy: 0.9766081871345029\n",
            "Precision: 0.9814814814814815\n",
            "Recall: 0.9814814814814815\n",
            "F1 Score: 0.9814814814814815\n",
            "ROC-AUC: 0.9964726631393298\n",
            "Confusion Matrix:\n",
            " [[ 61   2]\n",
            " [  2 106]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame for readability\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"] = data.target\n",
        "\n",
        "print(\"First 5 rows of the dataset:\\n\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset shape:\", df.shape)\n",
        "print(\"\\nClass distribution:\\n\")\n",
        "print(df[\"target\"].value_counts())\n",
        "\n",
        "# Split data\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale for SVM (important)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "models = {\n",
        "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"SVM (Linear)\": SVC(kernel=\"linear\", probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    \n",
        "    if \"SVM\" in name:\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_prob))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding Classification Metrics\n",
        "\n",
        "> **The difference between a practitioner and a button-pusher:** Understanding *why* metrics matter, not just *what* they calculate.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š The Foundation: Confusion Matrix\n",
        "\n",
        "The **confusion matrix** is the fundamental truth table of predictions. Every other metric derives from it.\n",
        "\n",
        "### Components\n",
        "\n",
        "| Term | Definition | Example (Cancer Detection) |\n",
        "|------|------------|----------------------------|\n",
        "| **True Positive (TP)** | Correctly predicted positive | Correctly identified malignant tumor |\n",
        "| **True Negative (TN)** | Correctly predicted negative | Correctly identified benign tumor |\n",
        "| **False Positive (FP)** | Predicted positive, actually negative | False alarm: predicted malignant, but benign |\n",
        "| **False Negative (FN)** | Predicted negative, actually positive | **Most dangerous:** Missed cancer diagnosis |\n",
        "\n",
        "### Why It Matters\n",
        "\n",
        "In medical diagnosis, **false negatives are life-threatening**. This isn't just mathematicsâ€”it's a missed diagnosis that could cost a life.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Core Metrics Explained\n",
        "\n",
        "### 1. Accuracy\n",
        "\n",
        "**Formula:** `Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
        "\n",
        "**What it answers:** *\"How often is the model correct overall?\"*\n",
        "\n",
        "**The Problem:**\n",
        "- Can be **misleading with imbalanced classes**\n",
        "- Example: If 95% of patients are healthy, a model that always predicts \"healthy\" achieves 95% accuracy\n",
        "- **That model is useless**â€”it never catches cancer!\n",
        "\n",
        "**When to use:** Balanced datasets where all error types have similar cost.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Precision\n",
        "\n",
        "**Formula:** `Precision = TP / (TP + FP)`\n",
        "\n",
        "**What it answers:** *\"When the model says 'malignant,' how often is it correct?\"*\n",
        "\n",
        "**Interpretation:**\n",
        "- **High precision** = Few false alarms\n",
        "- **Low precision** = Too many false positives (unnecessary anxiety, tests, costs)\n",
        "\n",
        "**When to optimize:** Spam detection, fraud detection (where false alarms are costly)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Recall (Sensitivity)\n",
        "\n",
        "**Formula:** `Recall = TP / (TP + FN)`\n",
        "\n",
        "**What it answers:** *\"Of all the actual malignant cases, how many did we catch?\"*\n",
        "\n",
        "**Interpretation:**\n",
        "- **High recall** = Catching most positive cases\n",
        "- **Low recall** = Missing too many positive cases\n",
        "\n",
        "**When to optimize:** Medical diagnosis, search engines (missing results is worse than extra results)\n",
        "\n",
        "> **Key Insight:** In cancer detection, **recall often matters more than precision**. Missing cancer is worse than ordering an unnecessary follow-up test.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. F1 Score\n",
        "\n",
        "**Formula:** `F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)`\n",
        "\n",
        "**What it is:** Harmonic mean of precision and recall\n",
        "\n",
        "**Why harmonic mean?**\n",
        "- Penalizes **extreme imbalance** between precision and recall\n",
        "- If one is high and the other is low, F1 drops sharply\n",
        "- Forces a **balanced trade-off**\n",
        "\n",
        "**When to use:** When you need a single metric that balances both precision and recall\n",
        "\n",
        "---\n",
        "\n",
        "### 5. ROC-AUC\n",
        "\n",
        "**What it stands for:** Receiver Operating Characteristic - Area Under the Curve\n",
        "\n",
        "**What it measures:** How well the model **separates classes** across all possible classification thresholds\n",
        "\n",
        "**Scale:**\n",
        "- **1.0** â†’ Perfect class separation\n",
        "- **0.5** â†’ Random guessing (no better than coin flip)\n",
        "- **< 0.5** â†’ Model is worse than random (likely inverted)\n",
        "\n",
        "**Key Properties:**\n",
        "- **Threshold-independent** â€” evaluates ranking ability, not just final predictions\n",
        "- Measures the model's ability to **distinguish** between classes overall\n",
        "\n",
        "**What it answers:** *\"How well does the model distinguish malignant from benign across all possible decision thresholds?\"*\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ¤– Model Characteristics\n",
        "\n",
        "### Decision Tree\n",
        "- âœ… **Interpretable** (visual flowchart-like structure)\n",
        "- âœ… **Easy to explain** to non-technical stakeholders\n",
        "- âš ï¸ **Prone to overfitting** (memorizes training data)\n",
        "- âš ï¸ **High variance** (small data changes â†’ different tree)\n",
        "\n",
        "### Random Forest\n",
        "- âœ… **Reduces variance** (ensemble of many trees)\n",
        "- âœ… **Strong accuracy and AUC** (often top performer)\n",
        "- âœ… **Handles non-linearity** well\n",
        "- âš ï¸ **Less interpretable** (black box of many trees)\n",
        "- âš ï¸ **Slower inference** than single tree\n",
        "\n",
        "### SVM (Support Vector Machine)\n",
        "- âœ… **Strong theoretical foundation** (maximizes margin)\n",
        "- âœ… **Excellent in high-dimensional spaces** (like our 30-feature medical data)\n",
        "- âœ… **Memory efficient** (only uses support vectors)\n",
        "- âš ï¸ **Sensitive to feature scaling** (requires normalization)\n",
        "- âš ï¸ **Less interpretable** (complex decision boundaries)\n",
        "- âš ï¸ **Slow on large datasets**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ’¡ The Principal Engineer's Perspective\n",
        "\n",
        "### The Real Point\n",
        "\n",
        "**Machine learning is not about picking \"the best model.\"**\n",
        "\n",
        "It's about **choosing the right trade-off** for your specific problem and business context.\n",
        "\n",
        "### Context Matters\n",
        "\n",
        "| Problem Domain | Priority Metric | Why |\n",
        "|----------------|----------------|-----|\n",
        "| **Cancer Detection** | **Recall** | Missing cancer is catastrophic |\n",
        "| **Spam Detection** | **Precision** | False alarms annoy users |\n",
        "| **Fraud Detection** | **F1 Score** | Balance catching fraud vs. false alarms |\n",
        "| **Search Engines** | **Recall** | Missing relevant results is worse than extra results |\n",
        "| **Financial Trading** | **Custom Cost Function** | False positives and negatives have different dollar costs |\n",
        "\n",
        "### Remember\n",
        "\n",
        "> **Metrics are not just numbersâ€”they encode priorities.**\n",
        "\n",
        "Every metric you choose tells a story about what you value:\n",
        "- High recall? You prioritize catching all positive cases.\n",
        "- High precision? You prioritize avoiding false alarms.\n",
        "- High F1? You want a balanced approach.\n",
        "\n",
        "**Choose your metrics based on your problem's real-world consequences, not just mathematical convenience.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
