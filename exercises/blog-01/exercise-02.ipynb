{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c1cebb",
   "metadata": {},
   "source": [
    "# Exercise 02: Neural Network from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-01/exercise-02.ipynb)\n",
    "\n",
    "This notebook demonstrates building a simple neural network from scratch using only NumPy.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the basic structure of a neural network\n",
    "- Implement forward propagation manually with multiple hidden layers\n",
    "- See how matrix multiplication drives neural networks\n",
    "- Apply activation functions (ReLU) across layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4aad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ numpy is already installed\n",
      "✓ scikit-learn is already installed\n",
      "✓ pandas is already installed\n"
     ]
    }
   ],
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b01dea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ numpy is already installed\n",
      "✓ scikit-learn is already installed\n",
      "✓ pandas is already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages using the kernel's Python interpreter\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "\n",
    "def install_if_missing(package, import_name=None):\n",
    "    \"\"\"Install package if it's not already installed.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"✓ {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "\n",
    "# Install required packages\n",
    "install_if_missing(\"numpy\")\n",
    "install_if_missing(\"scikit-learn\", \"sklearn\")\n",
    "install_if_missing(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19459f",
   "metadata": {},
   "source": [
    "## 3. Define Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bf1464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2,)\n",
      "Input: [2. 3.]\n"
     ]
    }
   ],
   "source": [
    "# Example input (2 features)\n",
    "x = np.array([2.0, 3.0])\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input: {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48426b0f",
   "metadata": {},
   "source": [
    "## 4. Define Layer 1 (Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b4df3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (2, 3)\n",
      "b1 shape: (3,)\n"
     ]
    }
   ],
   "source": [
    "# Layer 1 weights: shape (2, 3) - 2 inputs, 3 neurons\n",
    "W1 = np.array([\n",
    "    [0.5, -0.2, 0.1],\n",
    "    [0.3, 0.8, -0.5]\n",
    "])\n",
    "\n",
    "# Layer 1 bias: shape (3,)\n",
    "b1 = np.array([0.1, -0.1, 0.05])\n",
    "\n",
    "print(f\"W1 shape: {W1.shape}\")\n",
    "print(f\"b1 shape: {b1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1bf30",
   "metadata": {},
   "source": [
    "## 5. Forward Propagation Through Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c3eb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 pre-activation (z1): [ 2.    1.9  -1.25]\n",
      "Layer 1 output (a1): [2.  1.9 0. ]\n"
     ]
    }
   ],
   "source": [
    "# Linear transformation: z1 = x @ W1 + b1\n",
    "z1 = x @ W1 + b1\n",
    "\n",
    "# Apply ReLU activation function\n",
    "a1 = np.maximum(0, z1)  # ReLU: max(0, z)\n",
    "\n",
    "print(f\"Layer 1 pre-activation (z1): {z1}\")\n",
    "print(f\"Layer 1 output (a1): {a1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c313dd1",
   "metadata": {},
   "source": [
    "## 6. Define Layer 2 (Second Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42ea7777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 shape: (3, 4)\n",
      "b2 shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "# Layer 2 weights: shape (3, 4) - 3 inputs from layer 1, 4 neurons\n",
    "W2 = np.array([\n",
    "    [0.4, -0.1, 0.3, 0.2],\n",
    "    [-0.2, 0.5, -0.1, 0.4],\n",
    "    [0.1, -0.3, 0.2, -0.1]\n",
    "])\n",
    "\n",
    "# Layer 2 bias: shape (4,)\n",
    "b2 = np.array([0.05, -0.05, 0.1, -0.1])\n",
    "\n",
    "print(f\"W2 shape: {W2.shape}\")\n",
    "print(f\"b2 shape: {b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6df7c",
   "metadata": {},
   "source": [
    "## 7. Forward Propagation Through Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7166562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2 pre-activation (z2): [0.47 0.7  0.51 1.06]\n",
      "Layer 2 output (a2): [0.47 0.7  0.51 1.06]\n"
     ]
    }
   ],
   "source": [
    "# Linear transformation: z2 = a1 @ W2 + b2\n",
    "z2 = a1 @ W2 + b2\n",
    "\n",
    "# Apply ReLU activation function\n",
    "a2 = np.maximum(0, z2)  # ReLU: max(0, z)\n",
    "\n",
    "print(f\"Layer 2 pre-activation (z2): {z2}\")\n",
    "print(f\"Layer 2 output (a2): {a2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd6268e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W3 shape: (4, 1)\n",
      "b3 shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# Output layer weights: shape (4, 1) - 4 inputs from layer 2, 1 output\n",
    "W3 = np.array([\n",
    "    [0.7],\n",
    "    [-0.3],\n",
    "    [0.2],\n",
    "    [0.1]\n",
    "])\n",
    "\n",
    "# Output layer bias: shape (1,)\n",
    "b3 = np.array([0.05])\n",
    "\n",
    "print(f\"W3 shape: {W3.shape}\")\n",
    "print(f\"b3 shape: {b3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ab4f2",
   "metadata": {},
   "source": [
    "## 9. Forward Propagation Through Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e3a05ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output: [0.377]\n",
      "Output shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# Linear transformation: z3 = a2 @ W3 + b3\n",
    "z3 = a2 @ W3 + b3\n",
    "output = z3\n",
    "\n",
    "print(f\"Final output: {output}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0f045",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "You performed a complete forward pass through a **multi-layer neural network**:\n",
    "\n",
    "**Input → Linear Transform → ReLU → Linear Transform → ReLU → Linear Transform → Output**\n",
    "\n",
    "That's a deep neural network in its essence:\n",
    "- **No brains**\n",
    "- **No magic**  \n",
    "- **Just matrix multiplications + simple functions stacked together**\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "```\n",
    "Input (2 features)\n",
    "    ↓\n",
    "Layer 1 (Hidden): 2 → 3 neurons (with ReLU activation)\n",
    "    ↓\n",
    "Layer 2 (Hidden): 3 → 4 neurons (with ReLU activation)\n",
    "    ↓\n",
    "Output Layer: 4 → 1 neuron\n",
    "    ↓\n",
    "Final Output\n",
    "```\n",
    "\n",
    "### Key Concepts Demonstrated\n",
    "\n",
    "1. **Matrix Multiplication**: The core operation (`@` operator) connecting layers\n",
    "2. **Bias Terms**: Added to shift the activation function at each layer\n",
    "3. **Activation Functions**: ReLU introduces non-linearity in hidden layers\n",
    "4. **Forward Propagation**: Data flows sequentially through multiple layers\n",
    "5. **Deep Networks**: Stacking multiple hidden layers allows the network to learn more complex patterns\n",
    "\n",
    "### Why Multiple Hidden Layers?\n",
    "\n",
    "- **Layer 1** learns basic features from raw input\n",
    "- **Layer 2** combines those features into more complex representations\n",
    "- **Output Layer** makes the final prediction based on these complex features\n",
    "\n",
    "Each layer builds upon the previous one, enabling the network to model increasingly sophisticated relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
