{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8415b3b8",
      "metadata": {
        "id": "8415b3b8"
      },
      "source": [
        "# Exercise 01: Train a Real CNN (Minimal, Honest)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-04/exercise-01.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16db06a5",
      "metadata": {
        "id": "16db06a5"
      },
      "source": [
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "216bb5d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "216bb5d0",
        "outputId": "846a8d36-a606-4b11-a072-125c7fe6f07b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì torch is already installed\n",
            "‚úì torchvision is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"‚úì {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}....\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"‚úì {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"torch\")\n",
        "install_if_missing(\"torchvision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935e2c0d",
      "metadata": {
        "id": "935e2c0d"
      },
      "source": [
        "## üß™ Final Exercise ‚Äî Train a Real CNN (Minimal, Honest)\n",
        "\n",
        "We'll use:\n",
        "\n",
        "**Fashion MNIST**\n",
        "(Real dataset, harder than MNIST digits)\n",
        "\n",
        "**Why?**\n",
        "\n",
        "- Small\n",
        "- Real images\n",
        "- Clear spatial patterns\n",
        "- Good demo for CNN vs MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7611949e",
      "metadata": {
        "id": "7611949e"
      },
      "source": [
        "### Part 1 ‚Äî Show the Architecture First\n",
        "\n",
        "Before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "729a3cdd",
      "metadata": {
        "id": "729a3cdd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3),   # 28x28 ‚Üí 26x26\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                   # 26x26 ‚Üí 13x13\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3),  # 13x13 ‚Üí 11x11\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                    # 11x11 ‚Üí 5x5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 5 * 5, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44f5d05",
      "metadata": {
        "id": "f44f5d05"
      },
      "source": [
        "**Print Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "680bba04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "680bba04",
        "outputId": "e3719e14-14dd-4a9a-ae68-5175c9fce1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=800, out_features=64, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = SimpleCNN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7acd1d6",
      "metadata": {
        "id": "f7acd1d6"
      },
      "source": [
        "Please inspect:\n",
        "\n",
        "- Conv ‚Üí ReLU ‚Üí Pool\n",
        "- Conv ‚Üí ReLU ‚Üí Pool\n",
        "- Dense layers\n",
        "\n",
        "Then:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "81c81868",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c81868",
        "outputId": "64be8940-334b-46fc-8013-94fdf8c0d3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters: 56714\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d9d814",
      "metadata": {
        "id": "87d9d814"
      },
      "source": [
        "You will see actual size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "586783eb",
      "metadata": {
        "id": "586783eb"
      },
      "source": [
        "### Part 2 ‚Äî Load Real Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "442be843",
      "metadata": {
        "id": "442be843"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00034b8c",
      "metadata": {
        "id": "00034b8c"
      },
      "source": [
        "### Part 3 ‚Äî Train It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ac300954",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac300954",
        "outputId": "45ecf69c-e7f9-42ba-9bd6-785ca5a3da7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.2847\n",
            "Epoch 2 | Loss: 0.2623\n",
            "Epoch 3 | Loss: 0.2501\n",
            "Epoch 4 | Loss: 0.2398\n",
            "Epoch 5 | Loss: 0.2280\n",
            "Epoch 6 | Loss: 0.2167\n",
            "Epoch 7 | Loss: 0.2102\n",
            "Epoch 8 | Loss: 0.1992\n",
            "Epoch 9 | Loss: 0.1902\n",
            "Epoch 10 | Loss: 0.1844\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731ccbb7",
      "metadata": {
        "id": "731ccbb7"
      },
      "source": [
        "### Part 4 ‚Äî Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e533cfd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e533cfd0",
        "outputId": "0111cb61-6d76-4efd-a0b6-50e20e9e2db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.9025\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Test Accuracy:\", correct / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006ef31d",
      "metadata": {
        "id": "006ef31d"
      },
      "source": [
        "**Expected: ~88‚Äì92% with very simple CNN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae002852",
      "metadata": {
        "id": "ae002852"
      },
      "source": [
        "## üî• Now Let's Do the Important Comparison\n",
        "\n",
        "Replace CNN with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2bff83b8",
      "metadata": {
        "id": "2bff83b8"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "oZeyeufF2atV",
      "metadata": {
        "id": "oZeyeufF2atV"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "8369347e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8369347e",
        "outputId": "85d35bab-10c0-43e5-ccff-e46b2e936b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP Parameters: 101770\n"
          ]
        }
      ],
      "source": [
        "\n",
        "mlp_model = SimpleMLP().to(device)\n",
        "print(\"MLP Parameters:\", count_parameters(mlp_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "eccafadb",
      "metadata": {
        "id": "eccafadb"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(\"Test Accuracy:\", correct / total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "21da7bba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21da7bba",
        "outputId": "04ce2cf0-bdd2-4fcf-ae9c-805a3b5bb3ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training MLP\n",
            "Epoch 1 | Loss: 0.5406\n",
            "Epoch 2 | Loss: 0.3995\n",
            "Epoch 3 | Loss: 0.3605\n",
            "Epoch 4 | Loss: 0.3334\n",
            "Epoch 5 | Loss: 0.3157\n",
            "Epoch 6 | Loss: 0.3001\n",
            "Epoch 7 | Loss: 0.2858\n",
            "Epoch 8 | Loss: 0.2760\n",
            "Epoch 9 | Loss: 0.2655\n",
            "Epoch 10 | Loss: 0.2567\n",
            "Test Accuracy: 0.8778\n",
            "MLP has 101,770 parameters\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining MLP\")\n",
        "train_model(mlp_model, train_loader, test_loader, epochs=10)\n",
        "print(f\"MLP has {count_parameters(mlp_model):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "Rewq9orO4gSx",
      "metadata": {
        "id": "Rewq9orO4gSx"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "yo-Obydk4guI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo-Obydk4guI",
        "outputId": "8865a3ec-c989-40b0-dcfe-1be380a2ba53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating MLP\n",
            "Test Accuracy: 0.8778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8778"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\nEvaluating MLP\")\n",
        "evaluate_model(mlp_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "KVmtbMr84g6q",
      "metadata": {
        "id": "KVmtbMr84g6q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3a7d4416",
      "metadata": {
        "id": "3a7d4416"
      },
      "source": [
        "\n",
        "**Compare:**\n",
        "\n",
        "- Parameter count\n",
        "- Accuracy\n",
        "- Training speed\n",
        "\n",
        "You should see:\n",
        "\n",
        "**CNN wins with fewer parameters.**\n",
        "\n",
        "Because structure matters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9641feca",
      "metadata": {
        "id": "9641feca"
      },
      "source": [
        "## üí° Architecture Diagram\n",
        "\n",
        "```\n",
        "Input (1x28x28)\n",
        "      ‚Üì\n",
        "Conv(3x3,16)\n",
        "      ‚Üì\n",
        "ReLU\n",
        "      ‚Üì\n",
        "MaxPool(2x2)\n",
        "      ‚Üì\n",
        "Conv(3x3,32)\n",
        "      ‚Üì\n",
        "ReLU\n",
        "      ‚Üì\n",
        "MaxPool(2x2)\n",
        "      ‚Üì\n",
        "Flatten\n",
        "      ‚Üì\n",
        "Dense(64)\n",
        "      ‚Üì\n",
        "Dense(10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d129b2",
      "metadata": {
        "id": "f5d129b2"
      },
      "source": [
        "## Why This Is Powerful\n",
        "\n",
        "See:\n",
        "\n",
        "- CNN enforces locality\n",
        "- Weight sharing reduces parameters\n",
        "- Pooling builds invariance\n",
        "- Hierarchy builds abstraction\n",
        "- Architecture improves optimization\n",
        "\n",
        "Not philosophy.\n",
        "\n",
        "Not hype.\n",
        "\n",
        "Structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8873c263",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "collapsed": true,
        "id": "8873c263",
        "outputId": "5e099ea0-b4a9-45ea-d43b-6fca6d5c8a2d"
      },
      "source": [
        "## üîç Four Upgrades: Visualize What the Network Learned\n",
        "\n",
        "We'll add four upgrades:\n",
        "\n",
        "1. Visualize first-layer filters\n",
        "2. Show intermediate feature maps\n",
        "3. Plot misclassified examples\n",
        "4. Show a simple adversarial attack\n",
        "\n",
        "All clean. All practical."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ee2fd16",
      "metadata": {
        "id": "3ee2fd16"
      },
      "source": [
        "### 1Ô∏è‚É£ Visualize First-Layer Filters\n",
        "\n",
        "After training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416295ad",
      "metadata": {
        "id": "416295ad"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get first conv layer weights\n",
        "filters = model.features[0].weight.data.cpu()\n",
        "\n",
        "num_filters = filters.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, min(num_filters, 8), figsize=(15, 3))\n",
        "\n",
        "for i in range(min(num_filters, 8)):\n",
        "    axes[i].imshow(filters[i][0], cmap='gray')\n",
        "    axes[i].set_title(f\"Filter {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1HEKj1s12P48",
      "metadata": {
        "id": "1HEKj1s12P48"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "UbQbXhop2SEW",
      "metadata": {
        "id": "UbQbXhop2SEW"
      },
      "source": [
        "**What you should see:**\n",
        "\n",
        "- Edge detectors\n",
        "- Directional gradients\n",
        "- Texture patterns\n",
        "\n",
        "You can say:\n",
        "\n",
        "**The network wasn't told to detect edges.**\n",
        "**Optimization discovered they reduce loss.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba6518a",
      "metadata": {
        "id": "6ba6518a"
      },
      "source": [
        "### 2Ô∏è‚É£ Visualize Feature Maps\n",
        "\n",
        "Pick one image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e25ea",
      "metadata": {
        "id": "5b3e25ea"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "image, label = test_data[0]\n",
        "image = image.unsqueeze(0).to(device)\n",
        "\n",
        "# Forward manually through first conv\n",
        "with torch.no_grad():\n",
        "    conv1_output = model.features[0](image)\n",
        "    relu_output = model.features[1](conv1_output)\n",
        "\n",
        "feature_maps = relu_output.cpu()\n",
        "\n",
        "fig, axes = plt.subplots(1, 8, figsize=(15,3))\n",
        "\n",
        "for i in range(8):\n",
        "    axes[i].imshow(feature_maps[0][i], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc9dcdfd",
      "metadata": {
        "id": "bc9dcdfd"
      },
      "source": [
        "Now you see:\n",
        "\n",
        "- Different filters responding to different parts of image.\n",
        "- Some maps light up strongly.\n",
        "- Others remain quiet.\n",
        "\n",
        "This makes hierarchy visible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ceada31",
      "metadata": {
        "id": "6ceada31"
      },
      "source": [
        "### 3Ô∏è‚É£ Plot Misclassified Examples\n",
        "\n",
        "Very important for honesty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba60592",
      "metadata": {
        "id": "fba60592"
      },
      "outputs": [],
      "source": [
        "misclassified = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            if predicted[i] != labels[i]:\n",
        "                misclassified.append((images[i].cpu(), predicted[i].cpu(), labels[i].cpu()))\n",
        "\n",
        "            if len(misclassified) >= 6:\n",
        "                break\n",
        "        if len(misclassified) >= 6:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51417481",
      "metadata": {
        "id": "51417481"
      },
      "source": [
        "Plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce69820b",
      "metadata": {
        "id": "ce69820b"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 6, figsize=(15,3))\n",
        "\n",
        "for i in range(6):\n",
        "    img, pred, true = misclassified[i]\n",
        "    axes[i].imshow(img.squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f\"P:{pred} T:{true}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5011648b",
      "metadata": {
        "id": "5011648b"
      },
      "source": [
        "Now we will explote:\n",
        "\n",
        "- Confusion\n",
        "- Ambiguous clothing\n",
        "- Texture bias\n",
        "\n",
        "This reinforces limitations section beautifully."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47322e76",
      "metadata": {
        "id": "47322e76"
      },
      "source": [
        "### 4Ô∏è‚É£ Simple Adversarial Attack (FGSM)\n",
        "\n",
        "We will change some numbers by a small amount but the image will still look likes it the same, but our model will predict different class.\n",
        "\n",
        "**Fast Gradient Sign Method.**\n",
        "\n",
        "Minimal code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba93ef3",
      "metadata": {
        "id": "bba93ef3"
      },
      "source": [
        "**Step 1 ‚Äî Enable gradient on input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751d153e",
      "metadata": {
        "id": "751d153e"
      },
      "outputs": [],
      "source": [
        "image, label = test_data[0]\n",
        "image = image.unsqueeze(0).to(device)\n",
        "label = torch.tensor([label]).to(device)\n",
        "\n",
        "image.requires_grad = True\n",
        "\n",
        "output = model(image)\n",
        "loss = criterion(output, label)\n",
        "\n",
        "model.zero_grad()\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b726bd",
      "metadata": {
        "id": "25b726bd"
      },
      "source": [
        "**Step 2 ‚Äî Create Adversarial Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ed2903",
      "metadata": {
        "id": "f7ed2903"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "\n",
        "data_grad = image.grad.data\n",
        "perturbed_image = image + epsilon * data_grad.sign()\n",
        "perturbed_image = torch.clamp(perturbed_image, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee338738",
      "metadata": {
        "id": "ee338738"
      },
      "source": [
        "**Step 3 ‚Äî Test Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c79fd96",
      "metadata": {
        "id": "0c79fd96"
      },
      "outputs": [],
      "source": [
        "output_adv = model(perturbed_image)\n",
        "_, predicted_adv = torch.max(output_adv, 1)\n",
        "\n",
        "print(\"Original:\", label.item())\n",
        "print(\"Adversarial Prediction:\", predicted_adv.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4a5aaa",
      "metadata": {
        "id": "5c4a5aaa"
      },
      "source": [
        "**Step 4 ‚Äî Show Both Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a592e3b",
      "metadata": {
        "id": "8a592e3b"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "axes[0].imshow(image.detach().cpu().squeeze(), cmap='gray')\n",
        "axes[0].set_title(\"Original\")\n",
        "\n",
        "axes[1].imshow(perturbed_image.detach().cpu().squeeze(), cmap='gray')\n",
        "axes[1].set_title(\"Adversarial\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c0453b",
      "metadata": {
        "id": "f0c0453b"
      },
      "source": [
        "Often:\n",
        "\n",
        "- Image looks identical to human.\n",
        "- Model prediction flips.\n",
        "\n",
        "These are some limitations people discovered early during when CNN architectures were exploding in market."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b00c007",
      "metadata": {
        "id": "8b00c007"
      },
      "source": [
        "## What This Teaches Visually\n",
        "\n",
        "- **Filters** ‚Üí low-level patterns\n",
        "- **Feature maps** ‚Üí hierarchical response\n",
        "- **Misclassifications** ‚Üí brittle boundaries\n",
        "- **Adversarial** ‚Üí local evidence stacking weakness\n",
        "\n",
        "**CNNs are microscopes, not minds.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
