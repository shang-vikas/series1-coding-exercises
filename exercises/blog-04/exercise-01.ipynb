{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8415b3b8",
   "metadata": {},
   "source": [
    "# Exercise 01: Train a Real CNN (Minimal, Honest)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-04/exercise-01.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db06a5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages using the kernel's Python interpreter\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def install_if_missing(package, import_name=None):\n",
    "    \"\"\"Install package if it's not already installed.\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package\n",
    "\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        print(f\"‚úì {package} is already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}....\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "install_if_missing(\"torch\")\n",
    "install_if_missing(\"torchvision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e2c0d",
   "metadata": {},
   "source": [
    "## üß™ Final Exercise ‚Äî Train a Real CNN (Minimal, Honest)\n",
    "\n",
    "We'll use:\n",
    "\n",
    "**Fashion MNIST**\n",
    "(Real dataset, harder than MNIST digits)\n",
    "\n",
    "**Why?**\n",
    "\n",
    "- Small\n",
    "- Real images\n",
    "- Clear spatial patterns\n",
    "- Good demo for CNN vs MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611949e",
   "metadata": {},
   "source": [
    "### Part 1 ‚Äî Show the Architecture First\n",
    "\n",
    "Before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3),   # 28x28 ‚Üí 26x26\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                   # 26x26 ‚Üí 13x13\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3),  # 13x13 ‚Üí 11x11\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                    # 11x11 ‚Üí 5x5\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 5 * 5, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f5d05",
   "metadata": {},
   "source": [
    "**Print Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7acd1d6",
   "metadata": {},
   "source": [
    "Have readers inspect:\n",
    "\n",
    "- Conv ‚Üí ReLU ‚Üí Pool\n",
    "- Conv ‚Üí ReLU ‚Üí Pool\n",
    "- Dense layers\n",
    "\n",
    "Then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c81868",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total parameters:\", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9d814",
   "metadata": {},
   "source": [
    "They see actual size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586783eb",
   "metadata": {},
   "source": [
    "### Part 2 ‚Äî Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442be843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00034b8c",
   "metadata": {},
   "source": [
    "### Part 3 ‚Äî Train It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac300954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ccbb7",
   "metadata": {},
   "source": [
    "### Part 4 ‚Äî Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\"Test Accuracy:\", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ef31d",
   "metadata": {},
   "source": [
    "**Expected: ~88‚Äì92% with very simple CNN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae002852",
   "metadata": {},
   "source": [
    "## üî• Now Do the Important Comparison\n",
    "\n",
    "Replace CNN with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff83b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7d4416",
   "metadata": {},
   "source": [
    "Train same way.\n",
    "\n",
    "**Compare:**\n",
    "\n",
    "- Parameter count\n",
    "- Accuracy\n",
    "- Training speed\n",
    "\n",
    "Engineers will see:\n",
    "\n",
    "**CNN wins with fewer parameters.**\n",
    "\n",
    "Because structure matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641feca",
   "metadata": {},
   "source": [
    "## üí° Architecture Diagram\n",
    "\n",
    "```\n",
    "Input (1x28x28)\n",
    "      ‚Üì\n",
    "Conv(3x3,16)\n",
    "      ‚Üì\n",
    "ReLU\n",
    "      ‚Üì\n",
    "MaxPool(2x2)\n",
    "      ‚Üì\n",
    "Conv(3x3,32)\n",
    "      ‚Üì\n",
    "ReLU\n",
    "      ‚Üì\n",
    "MaxPool(2x2)\n",
    "      ‚Üì\n",
    "Flatten\n",
    "      ‚Üì\n",
    "Dense(64)\n",
    "      ‚Üì\n",
    "Dense(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d129b2",
   "metadata": {},
   "source": [
    "## Why This Is Powerful\n",
    "\n",
    "They now see:\n",
    "\n",
    "- CNN enforces locality\n",
    "- Weight sharing reduces parameters\n",
    "- Pooling builds invariance\n",
    "- Hierarchy builds abstraction\n",
    "- Architecture improves optimization\n",
    "\n",
    "Not philosophy.\n",
    "\n",
    "Not hype.\n",
    "\n",
    "Structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "## üîç Four Upgrades: Visualize What the Network Learned\n",
    "\n",
    "We'll add four upgrades:\n",
    "\n",
    "1. Visualize first-layer filters\n",
    "2. Show intermediate feature maps\n",
    "3. Plot misclassified examples\n",
    "4. Show a simple adversarial attack\n",
    "\n",
    "All clean. All practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee2fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1Ô∏è‚É£ Visualize First-Layer Filters\n",
    "\n",
    "After training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416295ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get first conv layer weights\n",
    "filters = model.features[0].weight.data.cpu()\n",
    "\n",
    "num_filters = filters.shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, min(num_filters, 8), figsize=(15, 3))\n",
    "\n",
    "for i in range(min(num_filters, 8)):\n",
    "    axes[i].imshow(filters[i][0], cmap='gray')\n",
    "    axes[i].set_title(f\"Filter {i}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "**What readers will see:**\n",
    "\n",
    "- Edge detectors\n",
    "- Directional gradients\n",
    "- Texture patterns\n",
    "\n",
    "You can say:\n",
    "\n",
    "**The network wasn't told to detect edges.**\n",
    "**Optimization discovered they reduce loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2Ô∏è‚É£ Visualize Feature Maps\n",
    "\n",
    "Pick one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "image, label = test_data[0]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "# Forward manually through first conv\n",
    "with torch.no_grad():\n",
    "    conv1_output = model.features[0](image)\n",
    "    relu_output = model.features[1](conv1_output)\n",
    "\n",
    "feature_maps = relu_output.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(1, 8, figsize=(15,3))\n",
    "\n",
    "for i in range(8):\n",
    "    axes[i].imshow(feature_maps[0][i], cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9dcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now readers see:\n",
    "\n",
    "- Different filters responding to different parts of image.\n",
    "- Some maps light up strongly.\n",
    "- Others remain quiet.\n",
    "\n",
    "This makes hierarchy visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceada31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3Ô∏è‚É£ Plot Misclassified Examples\n",
    "\n",
    "Very important for honesty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba60592",
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            if predicted[i] != labels[i]:\n",
    "                misclassified.append((images[i].cpu(), predicted[i].cpu(), labels[i].cpu()))\n",
    "\n",
    "            if len(misclassified) >= 6:\n",
    "                break\n",
    "        if len(misclassified) >= 6:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51417481",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 6, figsize=(15,3))\n",
    "\n",
    "for i in range(6):\n",
    "    img, pred, true = misclassified[i]\n",
    "    axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"P:{pred} T:{true}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011648b",
   "metadata": {},
   "source": [
    "Now show:\n",
    "\n",
    "- Confusion\n",
    "- Ambiguous clothing\n",
    "- Texture bias\n",
    "\n",
    "This reinforces limitations section beautifully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47322e76",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Simple Adversarial Attack (FGSM)\n",
    "\n",
    "This will make your blog elite.\n",
    "\n",
    "We use:\n",
    "\n",
    "**Fast Gradient Sign Method.**\n",
    "\n",
    "Minimal code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba93ef3",
   "metadata": {},
   "source": [
    "**Step 1 ‚Äî Enable gradient on input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = test_data[0]\n",
    "image = image.unsqueeze(0).to(device)\n",
    "label = torch.tensor([label]).to(device)\n",
    "\n",
    "image.requires_grad = True\n",
    "\n",
    "output = model(image)\n",
    "loss = criterion(output, label)\n",
    "\n",
    "model.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b726bd",
   "metadata": {},
   "source": [
    "**Step 2 ‚Äî Create Adversarial Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed2903",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "data_grad = image.grad.data\n",
    "perturbed_image = image + epsilon * data_grad.sign()\n",
    "perturbed_image = torch.clamp(perturbed_image, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee338738",
   "metadata": {},
   "source": [
    "**Step 3 ‚Äî Test Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_adv = model(perturbed_image)\n",
    "_, predicted_adv = torch.max(output_adv, 1)\n",
    "\n",
    "print(\"Original:\", label.item())\n",
    "print(\"Adversarial Prediction:\", predicted_adv.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a5aaa",
   "metadata": {},
   "source": [
    "**Step 4 ‚Äî Show Both Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a592e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
    "\n",
    "axes[0].imshow(image.detach().cpu().squeeze(), cmap='gray')\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "axes[1].imshow(perturbed_image.detach().cpu().squeeze(), cmap='gray')\n",
    "axes[1].set_title(\"Adversarial\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0453b",
   "metadata": {},
   "source": [
    "Often:\n",
    "\n",
    "- Image looks identical to human.\n",
    "- Model prediction flips.\n",
    "\n",
    "Now your limitation section becomes undeniable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00c007",
   "metadata": {},
   "source": [
    "## What This Teaches Visually\n",
    "\n",
    "- **Filters** ‚Üí low-level patterns\n",
    "- **Feature maps** ‚Üí hierarchical response\n",
    "- **Misclassifications** ‚Üí brittle boundaries\n",
    "- **Adversarial** ‚Üí local evidence stacking weakness\n",
    "\n",
    "This connects perfectly to your earlier line:\n",
    "\n",
    "**CNNs are microscopes, not minds.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
