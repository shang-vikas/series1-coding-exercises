{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8415b3b8",
      "metadata": {
        "id": "8415b3b8"
      },
      "source": [
        "# Exercise 01: Train a Real CNN (Minimal, Honest)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-04/exercise-01.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16db06a5",
      "metadata": {
        "id": "16db06a5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "216bb5d0",
      "metadata": {
        "id": "216bb5d0",
        "outputId": "b99566ae-c741-4f5a-fd11-0d9e0d45e659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì torch is already installed\n",
            "‚úì torchvision is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"‚úì {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}....\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"‚úì {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"torch\")\n",
        "install_if_missing(\"torchvision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "935e2c0d",
      "metadata": {
        "id": "935e2c0d"
      },
      "source": [
        "## üß™ Final Exercise ‚Äî Train a Real CNN (Minimal, Honest)\n",
        "\n",
        "We'll use:\n",
        "\n",
        "**Fashion MNIST**\n",
        "(Real dataset, harder than MNIST digits)\n",
        "\n",
        "**Why?**\n",
        "\n",
        "- Small\n",
        "- Real images\n",
        "- Clear spatial patterns\n",
        "- Good demo for CNN vs MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7611949e",
      "metadata": {
        "id": "7611949e"
      },
      "source": [
        "### Part 1 ‚Äî Show the Architecture First\n",
        "\n",
        "Before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "729a3cdd",
      "metadata": {
        "id": "729a3cdd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3),   # 28x28 ‚Üí 26x26\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                   # 26x26 ‚Üí 13x13\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3),  # 13x13 ‚Üí 11x11\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)                    # 11x11 ‚Üí 5x5\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 5 * 5, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f44f5d05",
      "metadata": {
        "id": "f44f5d05"
      },
      "source": [
        "**Print Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "680bba04",
      "metadata": {
        "id": "680bba04",
        "outputId": "12912ac4-0fe4-4c3c-ba22-d314b192fa16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=800, out_features=64, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = SimpleCNN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7acd1d6",
      "metadata": {
        "id": "f7acd1d6"
      },
      "source": [
        "Have readers inspect:\n",
        "\n",
        "- Conv ‚Üí ReLU ‚Üí Pool\n",
        "- Conv ‚Üí ReLU ‚Üí Pool\n",
        "- Dense layers\n",
        "\n",
        "Then:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "81c81868",
      "metadata": {
        "id": "81c81868",
        "outputId": "30c8e9c0-5679-4b71-f037-1cea3c4f706e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 56714\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d9d814",
      "metadata": {
        "id": "87d9d814"
      },
      "source": [
        "They see actual size."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "586783eb",
      "metadata": {
        "id": "586783eb"
      },
      "source": [
        "### Part 2 ‚Äî Load Real Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "442be843",
      "metadata": {
        "id": "442be843",
        "outputId": "790e3282-4f79-4c13-a43d-b805e3ac5c3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 12.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 269kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:00<00:00, 5.04MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 3.11MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00034b8c",
      "metadata": {
        "id": "00034b8c"
      },
      "source": [
        "### Part 3 ‚Äî Train It"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac300954",
      "metadata": {
        "id": "ac300954",
        "outputId": "3473ac2c-7d50-48b2-f20b-1214c7cdc55c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.6087\n",
            "Epoch 2 | Loss: 0.3971\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731ccbb7",
      "metadata": {
        "id": "731ccbb7"
      },
      "source": [
        "### Part 4 ‚Äî Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e533cfd0",
      "metadata": {
        "id": "e533cfd0"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Test Accuracy:\", correct / total)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "006ef31d",
      "metadata": {
        "id": "006ef31d"
      },
      "source": [
        "**Expected: ~88‚Äì92% with very simple CNN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae002852",
      "metadata": {
        "id": "ae002852"
      },
      "source": [
        "## üî• Now Do the Important Comparison\n",
        "\n",
        "Replace CNN with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bff83b8",
      "metadata": {
        "id": "2bff83b8"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7d4416",
      "metadata": {
        "id": "3a7d4416"
      },
      "source": [
        "Train same way.\n",
        "\n",
        "**Compare:**\n",
        "\n",
        "- Parameter count\n",
        "- Accuracy\n",
        "- Training speed\n",
        "\n",
        "Engineers will see:\n",
        "\n",
        "**CNN wins with fewer parameters.**\n",
        "\n",
        "Because structure matters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9641feca",
      "metadata": {
        "id": "9641feca"
      },
      "source": [
        "## üí° Architecture Diagram\n",
        "\n",
        "```\n",
        "Input (1x28x28)\n",
        "      ‚Üì\n",
        "Conv(3x3,16)\n",
        "      ‚Üì\n",
        "ReLU\n",
        "      ‚Üì\n",
        "MaxPool(2x2)\n",
        "      ‚Üì\n",
        "Conv(3x3,32)\n",
        "      ‚Üì\n",
        "ReLU\n",
        "      ‚Üì\n",
        "MaxPool(2x2)\n",
        "      ‚Üì\n",
        "Flatten\n",
        "      ‚Üì\n",
        "Dense(64)\n",
        "      ‚Üì\n",
        "Dense(10)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d129b2",
      "metadata": {
        "id": "f5d129b2"
      },
      "source": [
        "## Why This Is Powerful\n",
        "\n",
        "They now see:\n",
        "\n",
        "- CNN enforces locality\n",
        "- Weight sharing reduces parameters\n",
        "- Pooling builds invariance\n",
        "- Hierarchy builds abstraction\n",
        "- Architecture improves optimization\n",
        "\n",
        "Not philosophy.\n",
        "\n",
        "Not hype.\n",
        "\n",
        "Structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8873c263",
      "metadata": {
        "id": "8873c263"
      },
      "outputs": [],
      "source": [
        "## üîç Four Upgrades: Visualize What the Network Learned\n",
        "\n",
        "We'll add four upgrades:\n",
        "\n",
        "1. Visualize first-layer filters\n",
        "2. Show intermediate feature maps\n",
        "3. Plot misclassified examples\n",
        "4. Show a simple adversarial attack\n",
        "\n",
        "All clean. All practical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee2fd16",
      "metadata": {
        "id": "3ee2fd16"
      },
      "outputs": [],
      "source": [
        "### 1Ô∏è‚É£ Visualize First-Layer Filters\n",
        "\n",
        "After training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416295ad",
      "metadata": {
        "id": "416295ad"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get first conv layer weights\n",
        "filters = model.features[0].weight.data.cpu()\n",
        "\n",
        "num_filters = filters.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, min(num_filters, 8), figsize=(15, 3))\n",
        "\n",
        "for i in range(min(num_filters, 8)):\n",
        "    axes[i].imshow(filters[i][0], cmap='gray')\n",
        "    axes[i].set_title(f\"Filter {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18c9499",
      "metadata": {
        "id": "d18c9499"
      },
      "outputs": [],
      "source": [
        "**What readers will see:**\n",
        "\n",
        "- Edge detectors\n",
        "- Directional gradients\n",
        "- Texture patterns\n",
        "\n",
        "You can say:\n",
        "\n",
        "**The network wasn't told to detect edges.**\n",
        "**Optimization discovered they reduce loss.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ba6518a",
      "metadata": {
        "id": "6ba6518a"
      },
      "outputs": [],
      "source": [
        "### 2Ô∏è‚É£ Visualize Feature Maps\n",
        "\n",
        "Pick one image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b3e25ea",
      "metadata": {
        "id": "5b3e25ea"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "image, label = test_data[0]\n",
        "image = image.unsqueeze(0).to(device)\n",
        "\n",
        "# Forward manually through first conv\n",
        "with torch.no_grad():\n",
        "    conv1_output = model.features[0](image)\n",
        "    relu_output = model.features[1](conv1_output)\n",
        "\n",
        "feature_maps = relu_output.cpu()\n",
        "\n",
        "fig, axes = plt.subplots(1, 8, figsize=(15,3))\n",
        "\n",
        "for i in range(8):\n",
        "    axes[i].imshow(feature_maps[0][i], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9dcdfd",
      "metadata": {
        "id": "bc9dcdfd"
      },
      "outputs": [],
      "source": [
        "Now readers see:\n",
        "\n",
        "- Different filters responding to different parts of image.\n",
        "- Some maps light up strongly.\n",
        "- Others remain quiet.\n",
        "\n",
        "This makes hierarchy visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ceada31",
      "metadata": {
        "id": "6ceada31"
      },
      "outputs": [],
      "source": [
        "### 3Ô∏è‚É£ Plot Misclassified Examples\n",
        "\n",
        "Very important for honesty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fba60592",
      "metadata": {
        "id": "fba60592"
      },
      "outputs": [],
      "source": [
        "misclassified = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            if predicted[i] != labels[i]:\n",
        "                misclassified.append((images[i].cpu(), predicted[i].cpu(), labels[i].cpu()))\n",
        "\n",
        "            if len(misclassified) >= 6:\n",
        "                break\n",
        "        if len(misclassified) >= 6:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51417481",
      "metadata": {
        "id": "51417481"
      },
      "outputs": [],
      "source": [
        "Plot them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce69820b",
      "metadata": {
        "id": "ce69820b"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 6, figsize=(15,3))\n",
        "\n",
        "for i in range(6):\n",
        "    img, pred, true = misclassified[i]\n",
        "    axes[i].imshow(img.squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f\"P:{pred} T:{true}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5011648b",
      "metadata": {
        "id": "5011648b"
      },
      "source": [
        "Now show:\n",
        "\n",
        "- Confusion\n",
        "- Ambiguous clothing\n",
        "- Texture bias\n",
        "\n",
        "This reinforces limitations section beautifully."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47322e76",
      "metadata": {
        "id": "47322e76"
      },
      "source": [
        "### 4Ô∏è‚É£ Simple Adversarial Attack (FGSM)\n",
        "\n",
        "This will make your blog elite.\n",
        "\n",
        "We use:\n",
        "\n",
        "**Fast Gradient Sign Method.**\n",
        "\n",
        "Minimal code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba93ef3",
      "metadata": {
        "id": "bba93ef3"
      },
      "source": [
        "**Step 1 ‚Äî Enable gradient on input**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751d153e",
      "metadata": {
        "id": "751d153e"
      },
      "outputs": [],
      "source": [
        "image, label = test_data[0]\n",
        "image = image.unsqueeze(0).to(device)\n",
        "label = torch.tensor([label]).to(device)\n",
        "\n",
        "image.requires_grad = True\n",
        "\n",
        "output = model(image)\n",
        "loss = criterion(output, label)\n",
        "\n",
        "model.zero_grad()\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b726bd",
      "metadata": {
        "id": "25b726bd"
      },
      "source": [
        "**Step 2 ‚Äî Create Adversarial Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ed2903",
      "metadata": {
        "id": "f7ed2903"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "\n",
        "data_grad = image.grad.data\n",
        "perturbed_image = image + epsilon * data_grad.sign()\n",
        "perturbed_image = torch.clamp(perturbed_image, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee338738",
      "metadata": {
        "id": "ee338738"
      },
      "source": [
        "**Step 3 ‚Äî Test Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c79fd96",
      "metadata": {
        "id": "0c79fd96"
      },
      "outputs": [],
      "source": [
        "output_adv = model(perturbed_image)\n",
        "_, predicted_adv = torch.max(output_adv, 1)\n",
        "\n",
        "print(\"Original:\", label.item())\n",
        "print(\"Adversarial Prediction:\", predicted_adv.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4a5aaa",
      "metadata": {
        "id": "5c4a5aaa"
      },
      "source": [
        "**Step 4 ‚Äî Show Both Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a592e3b",
      "metadata": {
        "id": "8a592e3b"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "axes[0].imshow(image.detach().cpu().squeeze(), cmap='gray')\n",
        "axes[0].set_title(\"Original\")\n",
        "\n",
        "axes[1].imshow(perturbed_image.detach().cpu().squeeze(), cmap='gray')\n",
        "axes[1].set_title(\"Adversarial\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c0453b",
      "metadata": {
        "id": "f0c0453b"
      },
      "source": [
        "Often:\n",
        "\n",
        "- Image looks identical to human.\n",
        "- Model prediction flips.\n",
        "\n",
        "Now your limitation section becomes undeniable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b00c007",
      "metadata": {
        "id": "8b00c007"
      },
      "source": [
        "## What This Teaches Visually\n",
        "\n",
        "- **Filters** ‚Üí low-level patterns\n",
        "- **Feature maps** ‚Üí hierarchical response\n",
        "- **Misclassifications** ‚Üí brittle boundaries\n",
        "- **Adversarial** ‚Üí local evidence stacking weakness\n",
        "\n",
        "This connects perfectly to your earlier line:\n",
        "\n",
        "**CNNs are microscopes, not minds.**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}