{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1ca803c",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-06/exercise-00.ipynb)\n",
    "\n",
    "# üß™ Exercise ‚Äî IMDB Sentiment with LSTM & GRU\n",
    "\n",
    "You already trained a vanilla RNN and saw:\n",
    "\n",
    "- Works decently\n",
    "- Gradients decay\n",
    "- Long reviews hurt\n",
    "- Sequential bottleneck remains\n",
    "\n",
    "\n",
    "\n",
    "- Replace RNN ‚Üí LSTM\n",
    "- Replace RNN ‚Üí GRU\n",
    "- Compare performance\n",
    "- Visualize gradients again\n",
    "- Measure speed\n",
    "- Discuss real tradeoffs\n",
    "\n",
    "All on the same IMDB pipeline.\n",
    "\n",
    "No theory fluff. Just architecture evolution you can feel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a826574",
   "metadata": {},
   "source": [
    "We reuse your dataset pipeline.\n",
    "\n",
    "Only model changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23af7d",
   "metadata": {},
   "source": [
    "## üîπ 1Ô∏è‚É£ Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29e8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchtext -q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load IMDB dataset\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_data = list(train_iter)\n",
    "test_data = list(test_iter)\n",
    "\n",
    "print(\"Train samples:\", len(train_data))\n",
    "print(\"Test samples:\", len(test_data))\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabulary\n",
    "counter = Counter()\n",
    "for label, text in train_data:\n",
    "    tokens = tokenizer(text)\n",
    "    counter.update(tokens)\n",
    "\n",
    "vocab_size = 20000\n",
    "most_common = counter.most_common(vocab_size - 2)\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(most_common)}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "\n",
    "def encode(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    texts, labels = [], []\n",
    "    for label, text in batch:\n",
    "        encoded = torch.tensor(encode(text))\n",
    "        texts.append(encoded)\n",
    "        labels.append(1 if label == \"pos\" else 0)\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_data, batch_size=64, collate_fn=collate_batch)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fa61c",
   "metadata": {},
   "source": [
    "## üîπ 2Ô∏è‚É£ Vanilla RNN Model (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c09960",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        final_hidden = hidden.squeeze(0)\n",
    "        return self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df515a",
   "metadata": {},
   "source": [
    "## üîπ 3Ô∏è‚É£ LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2c18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        final_hidden = h_n[-1]\n",
    "        return self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135dea1",
   "metadata": {},
   "source": [
    "## üîπ 4Ô∏è‚É£ GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h_n = self.gru(x)\n",
    "        \n",
    "        final_hidden = h_n[-1]\n",
    "        return self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56d7e2",
   "metadata": {},
   "source": [
    "## üîπ 5Ô∏è‚É£ Training & Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc783a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = torch.sigmoid(model(texts).squeeze())\n",
    "            preds = (outputs > 0.5).long()\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "def train_model(model, epochs=5):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device).float()\n",
    "            \n",
    "            outputs = model(texts).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        acc = evaluate_model(model)\n",
    "        print(f\"Epoch {epoch+1} | Loss {total_loss/len(train_loader):.4f} | Acc {acc:.4f} | Time {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393e089",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = VanillaRNN(vocab_size, 100, 128)\n",
    "lstm_model = LSTMModel(vocab_size, 100, 128)\n",
    "gru_model = GRUModel(vocab_size, 100, 128)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Training RNN\")\n",
    "print(\"=\" * 60)\n",
    "train_model(rnn_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training LSTM\")\n",
    "print(\"=\" * 60)\n",
    "train_model(lstm_model)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training GRU\")\n",
    "print(\"=\" * 60)\n",
    "train_model(gru_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee560d",
   "metadata": {},
   "source": [
    "## üîç Expected Results\n",
    "\n",
    "| Model | Accuracy | Training Speed |\n",
    "|-------|----------|----------------|\n",
    "| RNN   | ~80-85%  | Fast           |\n",
    "| LSTM  | ~86-89%  | Slower         |\n",
    "| GRU   | ~85-88%  | Slightly faster than LSTM |\n",
    "\n",
    "Numbers vary ‚Äî but pattern holds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee661800",
   "metadata": {},
   "source": [
    "## üî¨ 7Ô∏è‚É£ Compare Gradient Flow\n",
    "\n",
    "Reuse earlier gradient visualization.\n",
    "\n",
    "Modify models to save outputs for gradient inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModelGrad(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        out.retain_grad()\n",
    "        self.saved_outputs = out\n",
    "        \n",
    "        final_hidden = h_n[-1]\n",
    "        return self.fc(final_hidden)\n",
    "\n",
    "class GRUModelGrad(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h_n = self.gru(x)\n",
    "        \n",
    "        out.retain_grad()\n",
    "        self.saved_outputs = out\n",
    "        \n",
    "        final_hidden = h_n[-1]\n",
    "        return self.fc(final_hidden)\n",
    "\n",
    "class VanillaRNNGrad(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        \n",
    "        out.retain_grad()\n",
    "        self.saved_outputs = out\n",
    "        \n",
    "        final_hidden = hidden.squeeze(0)\n",
    "        return self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42433ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_decay(model, loader, model_name):\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    texts, labels = next(iter(loader))\n",
    "    texts, labels = texts.to(device), labels.to(device).float()\n",
    "    \n",
    "    outputs = model(texts).squeeze()\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradients w.r.t. hidden outputs\n",
    "    grads = model.saved_outputs.grad\n",
    "    \n",
    "    if grads is None:\n",
    "        print(f\"{model_name}: Enable requires_grad for saved_outputs\")\n",
    "        return None\n",
    "    \n",
    "    # Average gradient magnitude per timestep\n",
    "    grad_magnitudes = grads.abs().mean(dim=(0,2)).detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(grad_magnitudes, label=model_name)\n",
    "    plt.title(f\"Gradient Magnitude Across Time Steps - {model_name}\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Average Gradient Magnitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return grad_magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ed1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models for gradient inspection\n",
    "rnn_grad = VanillaRNNGrad(vocab_size, 100, 128).to(device)\n",
    "lstm_grad = LSTMModelGrad(vocab_size, 100, 128).to(device)\n",
    "gru_grad = GRUModelGrad(vocab_size, 100, 128).to(device)\n",
    "\n",
    "print(\"RNN Gradient Flow:\")\n",
    "rnn_grads = visualize_gradient_decay(rnn_grad, train_loader, \"RNN\")\n",
    "\n",
    "print(\"\\nLSTM Gradient Flow:\")\n",
    "lstm_grads = visualize_gradient_decay(lstm_grad, train_loader, \"LSTM\")\n",
    "\n",
    "print(\"\\nGRU Gradient Flow:\")\n",
    "gru_grads = visualize_gradient_decay(gru_grad, train_loader, \"GRU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c31152",
   "metadata": {},
   "source": [
    "You'll notice:\n",
    "\n",
    "- **RNN** ‚Üí steep decay\n",
    "- **LSTM** ‚Üí flatter curve\n",
    "- **GRU** ‚Üí similar but slightly noisier\n",
    "\n",
    "That's gated memory protecting gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04314597",
   "metadata": {},
   "source": [
    "## üïí 8Ô∏è‚É£ Measure Serial Bottleneck\n",
    "\n",
    "Time per epoch:\n",
    "\n",
    "Increase max sequence length.\n",
    "\n",
    "RNN, LSTM, GRU all slow down similarly.\n",
    "\n",
    "Because:\n",
    "\n",
    "- All are sequential.\n",
    "- None parallelize across time.\n",
    "\n",
    "That bottleneck survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_long(batch, max_len=400):\n",
    "    texts, labels = [], []\n",
    "    for label, text in batch:\n",
    "        encoded = torch.tensor(encode(text)[:max_len])\n",
    "        texts.append(encoded)\n",
    "        labels.append(1 if label == \"pos\" else 0)\n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    return texts, labels\n",
    "\n",
    "# Test with different sequence lengths\n",
    "for max_len in [100, 200, 400]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing with max sequence length: {max_len}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    train_loader_test = DataLoader(\n",
    "        train_data[:1000],  # subset for speed\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_batch_long(b, max_len)\n",
    "    )\n",
    "    \n",
    "    # Test RNN\n",
    "    rnn_test = VanillaRNN(vocab_size, 100, 128).to(device)\n",
    "    start = time.time()\n",
    "    for texts, labels in train_loader_test:\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        _ = rnn_test(texts)\n",
    "    rnn_time = time.time() - start\n",
    "    \n",
    "    # Test LSTM\n",
    "    lstm_test = LSTMModel(vocab_size, 100, 128).to(device)\n",
    "    start = time.time()\n",
    "    for texts, labels in train_loader_test:\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        _ = lstm_test(texts)\n",
    "    lstm_time = time.time() - start\n",
    "    \n",
    "    # Test GRU\n",
    "    gru_test = GRUModel(vocab_size, 100, 128).to(device)\n",
    "    start = time.time()\n",
    "    for texts, labels in train_loader_test:\n",
    "        texts, labels = texts.to(device), labels.to(device).float()\n",
    "        _ = gru_test(texts)\n",
    "    gru_time = time.time() - start\n",
    "    \n",
    "    print(f\"RNN:  {rnn_time:.2f}s\")\n",
    "    print(f\"LSTM: {lstm_time:.2f}s\")\n",
    "    print(f\"GRU:  {gru_time:.2f}s\")\n",
    "    print(f\"All scale roughly linearly with sequence length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e391f",
   "metadata": {},
   "source": [
    "## üß† What Improved Over RNN?\n",
    "\n",
    "### ‚úÖ Better long-term memory\n",
    "\n",
    "Earlier tokens influence prediction more reliably.\n",
    "\n",
    "### ‚úÖ More stable gradients\n",
    "\n",
    "Less vanishing.\n",
    "\n",
    "### ‚úÖ Higher accuracy\n",
    "\n",
    "Especially on longer reviews.\n",
    "\n",
    "## ‚ö†Ô∏è But They Still Have Limits\n",
    "\n",
    "### ‚ùå Still sequential\n",
    "\n",
    "Time step t must finish before t+1.\n",
    "\n",
    "GPU underutilized.\n",
    "\n",
    "### ‚ùå Still compress entire history\n",
    "\n",
    "Final hidden state is fixed size.\n",
    "\n",
    "Information loss still exists.\n",
    "\n",
    "### ‚ùå Still struggle with very long dependencies\n",
    "\n",
    "Improved ‚â† solved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ca01d",
   "metadata": {},
   "source": [
    "## üß© When GRU vs LSTM?\n",
    "\n",
    "### LSTM Pros\n",
    "\n",
    "- Slightly more expressive\n",
    "- Better when data is complex\n",
    "\n",
    "### LSTM Cons\n",
    "\n",
    "- More parameters\n",
    "- Slower\n",
    "\n",
    "### GRU Pros\n",
    "\n",
    "- Simpler\n",
    "- Faster\n",
    "- Often similar accuracy\n",
    "\n",
    "### GRU Cons\n",
    "\n",
    "- Slightly less expressive\n",
    "- Sometimes underperforms on very complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301157f0",
   "metadata": {},
   "source": [
    "## üß® The Structural Ceiling\n",
    "\n",
    "Even with gates:\n",
    "\n",
    "```\n",
    "Token1 ‚Üí Token2 ‚Üí Token3 ‚Üí ... ‚Üí TokenN\n",
    "```\n",
    "\n",
    "Information still walks step by step.\n",
    "\n",
    "Memory still compressed.\n",
    "\n",
    "Gradient still multiplied through time.\n",
    "\n",
    "That's the real limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c5043b",
   "metadata": {},
   "source": [
    "## üß† Why This Matters Before Transformers\n",
    "\n",
    "You now saw:\n",
    "\n",
    "- **RNN** ‚Üí works but fragile\n",
    "- **LSTM/GRU** ‚Üí stabilizes memory\n",
    "- But sequential nature remains\n",
    "\n",
    "So the next question becomes inevitable:\n",
    "\n",
    "**What if we removed recurrence entirely?**\n",
    "\n",
    "And that's where attention and Transformers enter."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
