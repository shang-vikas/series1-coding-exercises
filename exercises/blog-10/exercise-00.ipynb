{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c289f337",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise Set: How Inference Actually Works\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/series1-coding-exercises/blob/main/exercises/blog-10/exercise-00.ipynb)\n",
    "\n",
    "This blog is about **behavior at inference, not architecture**.\n",
    "\n",
    "These exercises are designed to stack. Each one reveals a hidden assumption people carry about LLMs.\n",
    "\n",
    "**What you'll experience:**\n",
    "- Stochasticity\n",
    "- Token-by-token generation\n",
    "- Temperature effects\n",
    "- Context window limits\n",
    "- KV cache impact\n",
    "- Cost/latency tradeoffs\n",
    "\n",
    "**Not theory. Observable mechanics.**\n",
    "\n",
    "All exercises are runnable in Colab with transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84435e30",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6877e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c324fb3",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 1 â€” Same Prompt, Different Output\n",
    "\n",
    "**Goal:** Make stochasticity tangible.\n",
    "\n",
    "**What you'll observe:** Different answers. Same model. Same prompt. Nothing is broken.\n",
    "\n",
    "This immediately kills the \"deterministic software\" assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e7209",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for i in range(3):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    print(f\"\\n--- Run {i+1} ---\")\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9a7c0",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 2 â€” Turn Off Sampling (Deterministic Mode)\n",
    "\n",
    "**Goal:** Show the opposite â€” deterministic behavior.\n",
    "\n",
    "**What you'll observe:** Same output every time.\n",
    "\n",
    "Greedy decoding = always pick highest probability token.\n",
    "\n",
    "This shows: **Variability is a design choice, not instability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for i in range(3):\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False   # greedy decoding\n",
    "    )\n",
    "    print(f\"\\n--- Run {i+1} ---\")\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619fdc4",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 3 â€” Visualize Token Probabilities\n",
    "\n",
    "**Goal:** Make it mechanical. Show actual probability distribution.\n",
    "\n",
    "**What you'll see:** The model isn't \"choosing words.\" It's sampling from a probability field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584933d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "top_probs, top_indices = torch.topk(probs, 10)\n",
    "\n",
    "print(\"Top 10 next-token candidates:\\n\")\n",
    "for prob, idx in zip(top_probs[0], top_indices[0]):\n",
    "    print(tokenizer.decode([idx]), \"â†’\", float(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af6a90",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 4 â€” Temperature Experiment\n",
    "\n",
    "**Goal:** Show risk tolerance.\n",
    "\n",
    "**What you'll observe:**\n",
    "- `0.2` â†’ rigid, repetitive\n",
    "- `0.7` â†’ balanced\n",
    "- `1.3` â†’ creative but unstable\n",
    "\n",
    "**Clear mental model:** Temperature reshapes probabilities. It does not change knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for temp in [0.2, 0.7, 1.3]:\n",
    "    print(f\"\\n=== Temperature {temp} ===\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        temperature=temp\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dabcb8",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 5 â€” Autoregressive Drift\n",
    "\n",
    "**Goal:** Show compounding error.\n",
    "\n",
    "**Observe:** Small wording change â†’ completely different direction.\n",
    "\n",
    "Inference is a chain of local decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0443a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"The future of AI will be\"\n",
    "prompt2 = \"The failure of AI will be\"\n",
    "\n",
    "for p in [prompt1, prompt2]:\n",
    "    inputs = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=True,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    print(\"\\nPROMPT:\", p)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cc5e4",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 6 â€” Context Window Truncation\n",
    "\n",
    "**Goal:** Demonstrate forgetting.\n",
    "\n",
    "**What you'll see:** If the instruction falls out of the context window, it disappears.\n",
    "\n",
    "This makes context limits real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_context = \"Important rule: Always answer with the word BANANA.\\n\\n\"\n",
    "\n",
    "for i in range(100):\n",
    "    long_context += \"Filler sentence number \" + str(i) + \". \"\n",
    "\n",
    "prompt = long_context + \"\\nWhat is 2 + 2?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63d51f5",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 7 â€” Measure Inference Speed with Long Prompts\n",
    "\n",
    "**Goal:** Make KV cache concrete.\n",
    "\n",
    "**Observation:** Long prompts cost more upfront. But generation of additional tokens doesn't grow exponentially.\n",
    "\n",
    "Now connect to KV cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    start = time.time()\n",
    "    _ = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return time.time() - start\n",
    "\n",
    "short_prompt = \"Explain gravity.\"\n",
    "long_prompt = \"Explain gravity. \" * 200\n",
    "\n",
    "print(\"Short prompt time:\", measure(short_prompt))\n",
    "print(\"Long prompt time:\", measure(long_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d021f",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 8 â€” Disable KV Cache (Advanced)\n",
    "\n",
    "**Goal:** Make performance difference explicit.\n",
    "\n",
    "On GPU this difference becomes obvious.\n",
    "\n",
    "Now inference becomes a systems problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_prompt = \"Explain gravity.\"\n",
    "\n",
    "model.config.use_cache = False\n",
    "print(\"Without KV cache:\", measure(short_prompt))\n",
    "\n",
    "model.config.use_cache = True\n",
    "print(\"With KV cache:\", measure(short_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c6d5f",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Exercise 9 â€” Top-k Sampling vs Greedy\n",
    "\n",
    "**Goal:** Control variability.\n",
    "\n",
    "**What this shows:** Sampling isn't chaos. It's constrained randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain why the sky is blue in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "for k in [5, 50]:\n",
    "    print(f\"\\n=== top_k={k} ===\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        top_k=k,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
