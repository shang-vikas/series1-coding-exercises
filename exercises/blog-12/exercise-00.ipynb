{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed091180",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Notebook: Robust Tiny LLM System\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/series1-coding-exercises/blob/main/exercises/blog-12/exercise-00.ipynb)\n",
    "\n",
    "**Colab-ready, minimal hardcoding**\n",
    "\n",
    "This notebook demonstrates a production-ready LLM system architecture with:\n",
    "- RAG (Retrieval-Augmented Generation)\n",
    "- Intent classification\n",
    "- Entity extraction\n",
    "- Deterministic business logic tools\n",
    "- Hybrid LLM routing (local + API)\n",
    "- Observability and logging\n",
    "- Evaluation harness\n",
    "- Drift detection\n",
    "\n",
    "**Key principle:** Business logic and data live outside the model. The orchestrator coordinates deterministic tools, RAG retrieval, and LLM calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a755b",
   "metadata": {},
   "source": [
    "## 0) Install Dependencies\n",
    "\n",
    "Run this cell once to install all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e53f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers sentence-transformers faiss-cpu openai datasets sqlalchemy prettytable scikit-learn dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d23d62",
   "metadata": {},
   "source": [
    "## 1) Imports + Config\n",
    "\n",
    "This cell sets up the environment. We avoid hardcoding model backends â€” `OPENAI_KEY` is optional. In production you'd manage keys via secret managers and rotate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Imports ==\n",
    "import os, time, json, sqlite3, re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dateparser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Small ML utilities for a lightweight intent classifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Optional OpenAI (ChatGPT) client (function-calling style possible)\n",
    "import openai\n",
    "\n",
    "# == Config ==\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")  # set in Colab if you want live ChatGPT\n",
    "if OPENAI_KEY:\n",
    "    openai.api_key = OPENAI_KEY\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e09ba",
   "metadata": {},
   "source": [
    "## 2) Tiny \"Real-ish\" Knowledge Base\n",
    "\n",
    "Simulate a tiny knowledge base as JSON-like objects (easy to extend or load from files/db).\n",
    "\n",
    "**Why load like this?** Real systems keep KBs externally (CMS, DB, knowledge store). The orchestrator fetches and indexes them; you never bake business facts into code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bc959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a tiny knowledge base as JSON-like objects (easy to extend or load from files/db)\n",
    "kb = [\n",
    "    {\"id\":\"policy_standard\",\"title\":\"Refund policy - standard\",\"text\":\"Refunds: standard plans eligible within 30 days of purchase. Proof required.\"},\n",
    "    {\"id\":\"policy_premium\",\"title\":\"Refund policy - premium\",\"text\":\"Premium plans are non-refundable except hardware failures, reviewed case-by-case.\"},\n",
    "    {\"id\":\"faq_refund\",\"title\":\"How to request refund\",\"text\":\"To request a refund, email billing@company.com with order id and reason. Processing: up to 7 business days.\"},\n",
    "    {\"id\":\"faq_support\",\"title\":\"Support hours\",\"text\":\"Technical support: Mon-Fri, 9:00-18:00 local time.\"}\n",
    "]\n",
    "\n",
    "# You can persist / update this KB externally (S3, DB) and re-index when it changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b3f33",
   "metadata": {},
   "source": [
    "## 3) Build RAG Index\n",
    "\n",
    "Choose embedding model (cheap & effective). In prod, consider larger encoders or a managed vector DB (e.g., Pinecone, Weaviate).\n",
    "\n",
    "**Real-world note:** you'll likely move to a vector DB (Pinecone/Weaviate/FAISS-on-cloud) for scale, persistence, filtering, and metadata scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ea579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding model (cheap & effective). In prod, consider larger encoders or a managed vector DB (e.g., Pinecone, Weaviate).\n",
    "EMBED_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "\n",
    "# Index creation helper\n",
    "def build_rag_index(documents: List[Dict[str,str]]):\n",
    "    texts = [d[\"text\"] for d in documents]\n",
    "    embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    id_map = {i: documents[i] for i in range(len(documents))}\n",
    "    return index, id_map\n",
    "\n",
    "index, id_map = build_rag_index(kb)\n",
    "\n",
    "def retrieve(query: str, k=3):\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [id_map[int(i)] for i in I[0]]\n",
    "\n",
    "# quick check (non-hardcoded query)\n",
    "print([d['title'] for d in retrieve(\"How do I get a refund?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d2317",
   "metadata": {},
   "source": [
    "## 4) Intent Classification\n",
    "\n",
    "We use a tiny TFâ€“IDF â†’ logistic regression model. This demonstrates how production systems often start with small classifiers and then graduate to model-based intent detection.\n",
    "\n",
    "**Real-world:** intent classifier can be improved by:\n",
    "- gathering labeled logs and retraining\n",
    "- using transfer learning (fine-tune small transformer)\n",
    "- or zero-shot classification via an LLM for cold start (e.g., OpenAI zero-shot labeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be61cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small labeled training set (expandable). Keep this outside code in a config or labeled dataset.\n",
    "intent_samples = [\n",
    "    (\"I want a refund for my $120 purchase\", \"refund\"),\n",
    "    (\"How can I get my money back?\", \"refund\"),\n",
    "    (\"When is support available?\", \"support\"),\n",
    "    (\"Who do I contact for invoices?\", \"billing\"),\n",
    "    (\"Is premium refundable?\", \"policy_query\"),\n",
    "    (\"What's the refund policy?\", \"policy_query\"),\n",
    "    (\"Calculate refund for $50 bought 20 days ago\", \"refund_calc\"),\n",
    "    (\"Get refund amount for $99 in 40 days\", \"refund_calc\")\n",
    "]\n",
    "\n",
    "texts, labels = zip(*intent_samples)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=1)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "clf = LogisticRegression(max_iter=500).fit(X, labels)\n",
    "\n",
    "# helper\n",
    "def predict_intent(query: str, threshold=0.5):\n",
    "    x = vectorizer.transform([query])\n",
    "    probs = clf.predict_proba(x)[0]\n",
    "    pred = clf.classes_[probs.argmax()]\n",
    "    return pred, probs.max()\n",
    "\n",
    "# test\n",
    "for q in [\n",
    "    \"I paid $200, what is my refund?\",\n",
    "    \"What time is tech support open?\",\n",
    "    \"calculate refund $40\"\n",
    "]:\n",
    "    print(q, \"->\", predict_intent(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527967d2",
   "metadata": {},
   "source": [
    "## 5) Robust Entity Extraction\n",
    "\n",
    "**Strategy:**\n",
    "- Try fast deterministic parsers (regex for dollar amounts)\n",
    "- Try dateparser for natural-language dates\n",
    "- If missing, fallback to an LLM function call to extract structured fields\n",
    "\n",
    "**Real-world options:**\n",
    "- Duckling (by Rasa) for dates, amounts, durations â€” robust and fast\n",
    "- spaCy NER or HuggingFace token-classifier for custom entities\n",
    "- LLM function-calling as a fallback for messy, ambiguous queries (structured JSON returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0994b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount regex\n",
    "AMOUNT_RE = re.compile(r\"\\$(\\d+(?:\\.\\d{1,2})?)\")\n",
    "\n",
    "def extract_amount(query: str):\n",
    "    m = AMOUNT_RE.search(query)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def extract_days(query: str):\n",
    "    # look for \"X days\" or natural phrases like \"30 days after\"\n",
    "    m = re.search(r\"(\\d+)\\s+days\", query)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    # fallback to natural date parsing: try to parse \"bought on 2025-01-01\" etc.\n",
    "    # dateparser lets us compute deltas if needed (left as an exercise)\n",
    "    return None\n",
    "\n",
    "def extract_entities(query: str):\n",
    "    amount = extract_amount(query)\n",
    "    days = extract_days(query)\n",
    "    return {\"amount\": amount, \"days\": days}\n",
    "\n",
    "# test\n",
    "print(extract_entities(\"I paid $200 twenty days ago, do I get a refund?\"))\n",
    "print(extract_entities(\"Refund $45\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fefcbb8",
   "metadata": {},
   "source": [
    "## 6) Deterministic Tool Implementations\n",
    "\n",
    "Parameterized deterministic business logic. In production this belongs to a service with robust tests.\n",
    "\n",
    "**Real-world:** business rules live in services/feature flags, versioned, tested, and audited â€” not inline scripts. Edge cases logged and escalated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455caaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_refund(amount: float, days_since_purchase: int, plan_tier: str = \"standard\"):\n",
    "    \"\"\"\n",
    "    Parameterized deterministic business logic. In production this belongs to a service with robust tests.\n",
    "    \"\"\"\n",
    "    if amount is None:\n",
    "        return {\"error\":\"amount_missing\"}\n",
    "    if plan_tier.lower() == \"premium\":\n",
    "        return {\"refund\": 0.0, \"reason\": \"premium_non_refundable\"}\n",
    "    if days_since_purchase is None:\n",
    "        # If days unknown, be conservative: require more info\n",
    "        return {\"error\":\"days_missing\"}\n",
    "    refund = amount if days_since_purchase <= 30 else 0.0\n",
    "    return {\"refund\": refund}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5bc88",
   "metadata": {},
   "source": [
    "## 7) Local Small LM Fallback + Optional OpenAI Call\n",
    "\n",
    "Local small LM for low-risk completions / demo: distilgpt2\n",
    "\n",
    "**Real-world:** many shops use a hybrid approach:\n",
    "- local small models for cheap generation, filtering, and previews\n",
    "- API models (OpenAI, Anthropic) for higher-quality responses where cost justified\n",
    "- function-calling / tool-enabled chat for structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb96e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local small LM for low-risk completions / demo: distilgpt2\n",
    "LOCAL_MODEL_NAME = \"distilgpt2\"\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_NAME)\n",
    "local_model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_NAME).to(DEVICE)\n",
    "local_model.eval()\n",
    "\n",
    "def call_local_llm(prompt: str, max_new_tokens=80, temperature=0.7):\n",
    "    inputs = local_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = local_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=50)\n",
    "    return local_tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "def call_openai_completion(prompt: str, max_tokens=150, temperature=0.2):\n",
    "    # Minimal wrapper (user can swap to chat api + function calling)\n",
    "    resp = openai.Completion.create(engine=\"text-davinci-003\", prompt=prompt, max_tokens=max_tokens, temperature=temperature)\n",
    "    return resp.choices[0].text.strip() if resp and resp.choices else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16925fa6",
   "metadata": {},
   "source": [
    "## 8) Orchestrator: Decision Logic\n",
    "\n",
    "The orchestrator uses the classifier to choose path; entity extraction is layered; deterministic tool handles finance; RAG grounds policy queries; OpenAI used if available; everything logged for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging DB (sqlite) - still fine for prototyping\n",
    "conn = sqlite3.connect(\"llm_system_logs_v2.db\", check_same_thread=False)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS logs(\n",
    " id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    " ts TEXT, query TEXT, intent TEXT, intent_conf REAL,\n",
    " retrieved TEXT, tool_called TEXT, model_used TEXT, response TEXT, latency REAL\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def log_event(row: dict):\n",
    "    cur.execute(\n",
    "        \"INSERT INTO logs(ts, query, intent, intent_conf, retrieved, tool_called, model_used, response, latency) VALUES(?,?,?,?,?,?,?,?,?)\",\n",
    "        (row.get(\"ts\"), row.get(\"query\"), row.get(\"intent\"), row.get(\"intent_conf\"),\n",
    "         json.dumps(row.get(\"retrieved\")), row.get(\"tool_called\"), row.get(\"model_used\"),\n",
    "         row.get(\"response\"), row.get(\"latency\"))\n",
    "    )\n",
    "    conn.commit()\n",
    "\n",
    "def orchestrate(query: str, use_openai=True):\n",
    "    start = time.time()\n",
    "    # 1) intent\n",
    "    intent, conf = predict_intent(query)\n",
    "    tool_called = None\n",
    "    retrieved = []\n",
    "    response = \"\"\n",
    "    model_used = None\n",
    "\n",
    "    # 2) If intent suggests refund calculation, extract entities and call deterministic tool:\n",
    "    if intent in (\"refund_calc\", \"refund\"):\n",
    "        ents = extract_entities(query)\n",
    "        # If critical entity missing, try LLM-assisted extraction via function-call pattern (simple fallback)\n",
    "        if ents[\"amount\"] is None or ents[\"days\"] is None:\n",
    "            # Try using LLM to extract structured JSON - here we use local LLM as simple fallback; in prod use ChatGPT function-calling\n",
    "            prompt = f\"Extract amount and days from this query as JSON: \\\"{query}\\\". Return {{\\\"amount\\\":..., \\\"days\\\":...}}\"\n",
    "            model_used = \"local_extractor\"\n",
    "            raw = call_local_llm(prompt, max_new_tokens=40, temperature=0.0)\n",
    "            # naive parse for numbers in returned text\n",
    "            a = AMOUNT_RE.search(raw)\n",
    "            d = re.search(r\"(\\d+)\\s+days\", raw)\n",
    "            ents[\"amount\"] = ents[\"amount\"] or (float(a.group(1)) if a else None)\n",
    "            ents[\"days\"] = ents[\"days\"] or (int(d.group(1)) if d else None)\n",
    "\n",
    "        # call deterministic business logic\n",
    "        tool_called = f\"calculate_refund(amount={ents['amount']}, days={ents['days']})\"\n",
    "        res = calculate_refund(ents[\"amount\"], ents[\"days\"])\n",
    "        response = f\"Refund result: {res}\"\n",
    "        model_used = model_used or \"deterministic_tool\"\n",
    "    else:\n",
    "        # 3) RAG-grounded answer: retrieve context and call an LLM (openai preferred if configured)\n",
    "        retrieved = retrieve(query, k=3)\n",
    "        context = \"\\n\".join([r[\"text\"] for r in retrieved])\n",
    "        prompt = f\"Use only the context to answer. Context:\\n{context}\\nQuestion: {query}\\nIf not answerable, say 'Not enough information.'\"\n",
    "        if use_openai and OPENAI_KEY:\n",
    "            model_used = \"openai_api\"\n",
    "            try:\n",
    "                response = call_openai_completion(prompt, temperature=0.2)\n",
    "            except Exception as e:\n",
    "                model_used = \"local_fallback\"\n",
    "                response = call_local_llm(prompt, temperature=0.3)\n",
    "        else:\n",
    "            model_used = \"local_model\"\n",
    "            response = call_local_llm(prompt, temperature=0.3)\n",
    "\n",
    "    latency = time.time() - start\n",
    "    log_event({\n",
    "        \"ts\": datetime.utcnow().isoformat(),\n",
    "        \"query\": query,\n",
    "        \"intent\": intent,\n",
    "        \"intent_conf\": float(conf),\n",
    "        \"retrieved\": [r[\"id\"] for r in retrieved],\n",
    "        \"tool_called\": tool_called,\n",
    "        \"model_used\": model_used,\n",
    "        \"response\": response,\n",
    "        \"latency\": latency\n",
    "    })\n",
    "    return response\n",
    "\n",
    "# quick demo\n",
    "queries = [\n",
    "    \"I paid $120 thirty days ago. How much will be refunded?\",\n",
    "    \"How do I request a refund?\",\n",
    "    \"Are premium subscriptions refundable?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", orchestrate(q))\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d189e889",
   "metadata": {},
   "source": [
    "## 9) Evaluation Harness\n",
    "\n",
    "Test set stored as data (expandable); not baked into orchestration.\n",
    "\n",
    "**Real-world evaluation** uses curated question sets, continuous A/B, and human review for edge cases. Store tests in files and run nightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b90054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set stored as data (expandable); not baked into orchestration.\n",
    "tests = [\n",
    "    {\"query\":\"What is the refund window?\", \"expected_substr\":\"30\"},\n",
    "    {\"query\":\"Who do I email for a refund?\", \"expected_substr\":\"billing@company.com\"},\n",
    "    {\"query\":\"If I paid $150 and it's been 20 days, what's my refund?\", \"expected_substr\":\"150\"}\n",
    "]\n",
    "\n",
    "def evaluate_system(test_cases):\n",
    "    results = []\n",
    "    for t in test_cases:\n",
    "        ans = orchestrate(t[\"query\"])\n",
    "        ok = t[\"expected_substr\"].lower() in ans.lower()\n",
    "        results.append({\"query\":t[\"query\"], \"answer\":ans, \"expected\":t[\"expected_substr\"], \"ok\":ok})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "df_results = evaluate_system(tests)\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e90e43",
   "metadata": {},
   "source": [
    "## 10) Drift Simulation\n",
    "\n",
    "Simulate KB change (drift) by appending a new policy version.\n",
    "\n",
    "**Ops note:** In production the KB update should trigger reindexing and a policy that notifies product/ops teams (CI/CD style). Observability surfaces drift (sudden changes in answer distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate KB change (drift) by appending a new policy version\n",
    "new_doc = {\"id\":\"policy_update_2025\",\"title\":\"Refund policy update\",\"text\":\"As of 2025-01-01 refunds are allowed within 14 days for standard plans.\"}\n",
    "kb.append(new_doc)\n",
    "\n",
    "# Rebuild index (in production you'd update index incrementally)\n",
    "index, id_map = build_rag_index(kb)\n",
    "\n",
    "# Query\n",
    "print(\"After policy change:\", orchestrate(\"What is the refund window?\"))\n",
    "\n",
    "# Inspect last logs\n",
    "logs_df = pd.read_sql_query(\"SELECT * FROM logs ORDER BY id DESC LIMIT 10\", conn)\n",
    "logs_df[['ts','query','intent','intent_conf','retrieved','tool_called','model_used','latency']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8036f",
   "metadata": {},
   "source": [
    "## 11) Observability & Alert Ideas\n",
    "\n",
    "Example: compute percent of \"Not enough information\" answers or \"fallback to local\" per day.\n",
    "\n",
    "**Set alerts when:**\n",
    "- `openai_api` calls spike (cost)\n",
    "- `local_fallback` increases (quality drop)\n",
    "- fraction of \"Not enough information\" grows (coverage gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98d278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: compute percent of \"Not enough information\" answers or \"fallback to local\" per day\n",
    "logs_df = pd.read_sql_query(\"SELECT * FROM logs\", conn)\n",
    "logs_df['date'] = pd.to_datetime(logs_df['ts']).dt.date\n",
    "summary = logs_df.groupby(['date','model_used']).size().unstack(fill_value=0)\n",
    "print(summary.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff1feb",
   "metadata": {},
   "source": [
    "## Final Notes: Why These Choices, and What Real Systems Use\n",
    "\n",
    "### Intent Detection\n",
    "- **Notebook:** tiny TFâ€“IDF + logistic regression (cheap, fast, transparent)\n",
    "- **Real world:** start small, then move to a fine-tuned transformer or zero-shot LLM classifier (for dozens of intents). Use labeled logs to retrain.\n",
    "\n",
    "### Entity Parsing\n",
    "- **Notebook:** regex + dateparser + local LLM fallback\n",
    "- **Real world:** Duckling (dates/amounts), spaCy/HuggingFace NER for custom entities, or function-calling LLM for complex extractions. Always prefer deterministic parsers for safety-critical fields (money, dates, identifiers).\n",
    "\n",
    "### RAG / Retrieval\n",
    "- **Notebook:** Sentence-Transformers + FAISS\n",
    "- **Real world:** managed vector DBs (Pinecone, Weaviate) for persistence, metadata filters, scalable nearest-neighbor, and multi-vector per doc support.\n",
    "\n",
    "### LLM Choices\n",
    "- **Notebook:** local small LM for cheap demos + optional ChatGPT for quality\n",
    "- **Real world:** hybrid â€” local models for first pass + high-quality APIs for critical answers; caching, rate limits, and cost-aware routing.\n",
    "\n",
    "### Tools / Deterministic Services\n",
    "- **Notebook:** `calculate_refund()` as a deterministic service\n",
    "- **Real world:** push business logic into microservices with tests, audit logs, and access control. LLMs should never be the source of truth for actions that change state without deterministic verification.\n",
    "\n",
    "### Observability & Evaluation\n",
    "- **Notebook:** SQLite logs + simple metrics\n",
    "- **Real world:** structured logs, dashboards, alerting, drift detection, automated regression tests, and periodic human review panels.\n",
    "\n",
    "### Safety & Guardrails\n",
    "- Always validate extracted critical entities before acting\n",
    "- Rate-limiting, authentication, and policy enforcement live outside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f54fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbe02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5d465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154e049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c816431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0302a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64eb940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f990e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13093a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd732613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db22d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43566648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b71967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d844a34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1683b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375863f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa1241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
