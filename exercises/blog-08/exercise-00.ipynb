{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a63bfd1",
      "metadata": {
        "id": "4a63bfd1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-08/exercise-00.ipynb)\n",
        "\n",
        "# ðŸ§ª Exercise 1 â€” Embeddings + Positional Encoding (Make Order Visible)\n",
        "\n",
        "## Goal\n",
        "\n",
        "Show that:\n",
        "\n",
        "Without positional encoding, order disappears.\n",
        "\n",
        "With positional encoding, order becomes numerically present."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b303ca40",
      "metadata": {
        "id": "b303ca40"
      },
      "source": [
        "## Step 1 â€” Fake Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4688616c",
      "metadata": {
        "id": "4688616c",
        "outputId": "25352bca-4c2a-4137-865a-71ba0346e1e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([1, 5, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "vocab_size = 100\n",
        "embed_dim = 16\n",
        "seq_len = 5\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "tokens = torch.tensor([[5, 8, 3, 2, 9]])  # shape (1, 5)\n",
        "x = embedding(tokens)\n",
        "\n",
        "print(\"Embeddings shape:\", x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9fdf31e",
      "metadata": {
        "id": "c9fdf31e"
      },
      "source": [
        "Now reorder tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7f5f5df8",
      "metadata": {
        "id": "7f5f5df8",
        "outputId": "2d4f5927-509e-4372-e11f-2d8a443ecbbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference norm: tensor(14.2764, grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "tokens_shuffled = torch.tensor([[3, 5, 9, 8, 2]])\n",
        "x_shuffled = embedding(tokens_shuffled)\n",
        "\n",
        "print(\"Difference norm:\", (x - x_shuffled).norm())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df6e4fc0",
      "metadata": {
        "id": "df6e4fc0"
      },
      "source": [
        "**Observation:**\n",
        "\n",
        "The model sees a different set of vectors but doesn't know their position."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85fb102d",
      "metadata": {
        "id": "85fb102d"
      },
      "source": [
        "## Step 2 â€” Add Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "92eac8dd",
      "metadata": {
        "id": "92eac8dd",
        "outputId": "aad87121-896e-4d1b-8515-7ac9a93b2895",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference norm: tensor(8.2165, grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "position = torch.arange(seq_len).unsqueeze(0)\n",
        "pos_embedding = nn.Embedding(seq_len, embed_dim)\n",
        "\n",
        "x_with_pos = x + pos_embedding(position)\n",
        "print(\"Difference norm:\", (x - x_with_pos).norm())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec767c19",
      "metadata": {
        "id": "ec767c19"
      },
      "source": [
        "Now order is baked into vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9f2041",
      "metadata": {
        "id": "dd9f2041"
      },
      "source": [
        "## ðŸ’¡ Teaching Point\n",
        "\n",
        "Order is not remembered.\n",
        "\n",
        "Order is injected numerically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "605f5d1b",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "605f5d1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e09fee",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "21e09fee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a943dcac",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a943dcac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383800fa",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "383800fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce637f25",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ce637f25"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}