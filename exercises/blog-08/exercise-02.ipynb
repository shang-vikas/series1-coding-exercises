{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103ef9e5",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-08/exercise-02.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 3 â€” Show Why MLP Is Necessary\n",
    "\n",
    "## Goal\n",
    "\n",
    "Prove that attention alone cannot transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5591d7e",
   "metadata": {},
   "source": [
    "Run attention twice without MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0befff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: tensor(15.5557)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = 1\n",
    "seq = 4\n",
    "dim = 8\n",
    "\n",
    "x = torch.randn(batch, seq, dim)\n",
    "\n",
    "Wq = torch.randn(dim, dim)\n",
    "Wk = torch.randn(dim, dim)\n",
    "Wv = torch.randn(dim, dim)\n",
    "\n",
    "Q = x @ Wq\n",
    "K = x @ Wk\n",
    "V = x @ Wv\n",
    "\n",
    "scores = Q @ K.transpose(-2, -1) / dim**0.5\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "out1 = weights @ V\n",
    "out2 = weights @ out1\n",
    "\n",
    "print(\"Difference:\", (out2 - out1).norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cbbc7",
   "metadata": {},
   "source": [
    "It mostly remixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eac6b1",
   "metadata": {},
   "source": [
    "Now add MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0b5162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After MLP difference: tensor(11.9028, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp = nn.Sequential(\n",
    "    nn.Linear(dim, dim*4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(dim*4, dim)\n",
    ")\n",
    "\n",
    "out_mlp = mlp(out1)\n",
    "print(\"After MLP difference:\", (out_mlp - out1).norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985f3c1",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Teaching Point\n",
    "\n",
    "Attention mixes.\n",
    "\n",
    "MLP transforms.\n",
    "\n",
    "Without MLP, depth collapses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626e400d",
   "metadata": {},
   "source": [
    "# ðŸ§ª Exercise 4 â€” Residual Connection Stability Demo\n",
    "\n",
    "## Goal\n",
    "\n",
    "Show residual protects gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589284e0",
   "metadata": {},
   "source": [
    "Without residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1f57f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm without residual: tensor(0.5977)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Sequential(\n",
    "    nn.Linear(dim, dim),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 10, dim, requires_grad=True)\n",
    "out = layer(layer(layer(x)))\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient norm without residual:\", x.grad.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1dabbd",
   "metadata": {},
   "source": [
    "Now with residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b8d83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient norm with residual: tensor(12.7411)\n"
     ]
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "def residual_block(x):\n",
    "    return x + layer(x)\n",
    "\n",
    "out = residual_block(residual_block(residual_block(x)))\n",
    "loss = out.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient norm with residual:\", x.grad.norm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4415a75",
   "metadata": {},
   "source": [
    "You'll see:\n",
    "\n",
    "Residual preserves gradient magnitude better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f57c61",
   "metadata": {},
   "source": [
    "## ðŸ’¡ Teaching Point\n",
    "\n",
    "Residuals make depth survivable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
