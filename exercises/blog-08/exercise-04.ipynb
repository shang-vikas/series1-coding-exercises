{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58a57f1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-08/exercise-04.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 4 â€” Build One Transformer Layer from Scratch\n",
    "\n",
    "In this exercise, we build a complete Transformer layer from scratch using only NumPy. We will:\n",
    "\n",
    "- Convert tokens into embeddings\n",
    "- Inject positional information\n",
    "- Compute Q, K, V projections\n",
    "- Perform scaled dot-product attention\n",
    "- Apply residual connections\n",
    "- Run a feed-forward MLP\n",
    "- Produce the final output of one Transformer layer\n",
    "\n",
    "Everything is small. Everything is explicit. Nothing is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e3ddb",
   "metadata": {},
   "source": [
    "## ðŸ§  Setup\n",
    "\n",
    "We start by importing NumPy and configuring clean printing for readable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57325d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8f3f2",
   "metadata": {},
   "source": [
    "## ðŸ§  Define Token Embeddings\n",
    "\n",
    "We represent tokens as vectors (learned embeddings). These are learned numbers. Not symbols. Not grammar. Geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21f1552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial token embeddings:\n",
      " [[0.2 0.1 0.  0.3]\n",
      " [0.9 0.7 0.1 0. ]\n",
      " [0.8 0.2 0.6 0.4]]\n"
     ]
    }
   ],
   "source": [
    "# Sentence: \"the cat sleeps\"\n",
    "\n",
    "X = np.array([\n",
    "    [0.2, 0.1, 0.0, 0.3],   # \"the\"\n",
    "    [0.9, 0.7, 0.1, 0.0],   # \"cat\"\n",
    "    [0.8, 0.2, 0.6, 0.4]    # \"sleeps\"\n",
    "])\n",
    "\n",
    "print(\"Initial token embeddings:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34006afe",
   "metadata": {},
   "source": [
    "## ðŸ§  Add Positional Embeddings\n",
    "\n",
    "We inject order information numerically by adding positional embeddings. Now meaning + position are baked into the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ede575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding positional embeddings:\n",
      " [[0.25 0.1  0.   0.3 ]\n",
      " [0.9  0.75 0.1  0.  ]\n",
      " [0.8  0.2  0.65 0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "pos = np.array([\n",
    "    [0.05, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.00]\n",
    "])\n",
    "\n",
    "X = X + pos\n",
    "\n",
    "print(\"After adding positional embeddings:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3affbf",
   "metadata": {},
   "source": [
    "## SELF-ATTENTION\n",
    "\n",
    "## ðŸ§  Define Attention Weight Matrices\n",
    "\n",
    "We define learned projection matrices for Q, K, V. These are learned during training. Change them, change behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8dc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = np.array([\n",
    "    [0.5, 0.1, 0.0, 0.2],\n",
    "    [0.0, 0.6, 0.1, 0.0],\n",
    "    [0.3, 0.0, 0.7, 0.1],\n",
    "    [0.0, 0.2, 0.0, 0.5]\n",
    "])\n",
    "\n",
    "W_K = np.array([\n",
    "    [0.4, 0.0, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.0, 0.1],\n",
    "    [0.2, 0.1, 0.6, 0.0],\n",
    "    [0.0, 0.2, 0.1, 0.4]\n",
    "])\n",
    "\n",
    "W_V = np.array([\n",
    "    [0.3, 0.2, 0.1, 0.0],\n",
    "    [0.0, 0.4, 0.2, 0.1],\n",
    "    [0.1, 0.0, 0.5, 0.2],\n",
    "    [0.2, 0.1, 0.0, 0.6]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75854cb",
   "metadata": {},
   "source": [
    "## ðŸ§  Compute Q, K, V\n",
    "\n",
    "We project tokens into query, key, and value spaces. Same tokens. Different roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1f74cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      " [[0.125 0.145 0.01  0.2  ]\n",
      " [0.48  0.54  0.145 0.19 ]\n",
      " [0.595 0.28  0.475 0.425]]\n",
      "\n",
      "K:\n",
      " [[0.11  0.11  0.055 0.18 ]\n",
      " [0.455 0.385 0.15  0.255]\n",
      " [0.47  0.245 0.51  0.34 ]]\n",
      "\n",
      "V:\n",
      " [[0.135 0.12  0.045 0.19 ]\n",
      " [0.28  0.48  0.29  0.095]\n",
      " [0.385 0.28  0.445 0.39 ]]\n"
     ]
    }
   ],
   "source": [
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"\\nK:\\n\", K)\n",
    "print(\"\\nV:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70561a2c",
   "metadata": {},
   "source": [
    "## ðŸ§  Compute Scaled Dot-Product Attention\n",
    "\n",
    "We measure similarity between tokens. Each entry (i, j) answers: How relevant is token j to token i?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "befdfec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention scores:\n",
      " [[0.033 0.083 0.084]\n",
      " [0.077 0.248 0.248]\n",
      " [0.099 0.279 0.368]]\n"
     ]
    }
   ],
   "source": [
    "d_model = 4\n",
    "\n",
    "scores = Q @ K.T\n",
    "scaled_scores = scores / np.sqrt(d_model)\n",
    "\n",
    "print(\"Scaled attention scores:\\n\", scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694458c",
   "metadata": {},
   "source": [
    "## ðŸ§  Apply Softmax\n",
    "\n",
    "We convert similarity scores into attention weights using softmax. Each row is now a probability distribution. Attention never picks one token. It blends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf070c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      " [[0.322 0.339 0.339]\n",
      " [0.296 0.352 0.352]\n",
      " [0.285 0.342 0.373]]\n",
      "Row sums (should equal 1):\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "A = softmax(scaled_scores)\n",
    "\n",
    "print(\"Attention weights:\\n\", A)\n",
    "print(\"Row sums (should equal 1):\\n\", A.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515f427",
   "metadata": {},
   "source": [
    "## ðŸ§  Compute Attention Output\n",
    "\n",
    "We mix value vectors according to attention weights. Each token is now contextualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231440f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output:\n",
      " [[0.269 0.296 0.264 0.226]\n",
      " [0.274 0.303 0.272 0.227]\n",
      " [0.278 0.303 0.278 0.232]]\n"
     ]
    }
   ],
   "source": [
    "attention_output = A @ V\n",
    "\n",
    "print(\"Attention output:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2d9e",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "## ðŸ§  Add Residual\n",
    "\n",
    "We add the residual connection to preserve original information while refining it. Depth becomes refinement, not replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224b2121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first residual connection:\n",
      " [[0.519 0.396 0.264 0.526]\n",
      " [1.174 1.053 0.372 0.227]\n",
      " [1.078 0.503 0.928 0.632]]\n"
     ]
    }
   ],
   "source": [
    "residual_1 = X + attention_output\n",
    "\n",
    "print(\"After first residual connection:\\n\", residual_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de66fe0",
   "metadata": {},
   "source": [
    "## MLP Block\n",
    "\n",
    "## ðŸ§  Define MLP Weights\n",
    "\n",
    "We define feedforward network weights. The MLP transforms each token independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20acb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding to hidden size 8, then projecting back to 4\n",
    "W1 = np.random.randn(4, 8) * 0.5\n",
    "W2 = np.random.randn(8, 4) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef93c0",
   "metadata": {},
   "source": [
    "## ðŸ§  MLP Forward Pass\n",
    "\n",
    "We apply nonlinear transformation to each token. Attention decides what to look at. MLP decides what to become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de59358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP output:\n",
      " [[-0.897 -0.223  0.175 -0.338]\n",
      " [-0.815 -0.383  0.554 -1.01 ]\n",
      " [-1.429 -0.394 -0.16  -0.939]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "mlp_hidden = relu(residual_1 @ W1)\n",
    "mlp_output = mlp_hidden @ W2\n",
    "\n",
    "print(\"MLP output:\\n\", mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b98cd",
   "metadata": {},
   "source": [
    "## Final Residual\n",
    "\n",
    "## ðŸ§  Final Output of One Transformer Layer\n",
    "\n",
    "We produce the final output by adding the residual connection. This completes one full Transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb7a7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final layer output:\n",
      " [[-0.378  0.173  0.438  0.188]\n",
      " [ 0.359  0.67   0.926 -0.783]\n",
      " [-0.351  0.108  0.768 -0.307]]\n"
     ]
    }
   ],
   "source": [
    "final_output = residual_1 + mlp_output\n",
    "\n",
    "print(\"Final layer output:\\n\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c1360",
   "metadata": {},
   "source": [
    "## ðŸŽ“ What We Built\n",
    "\n",
    "In this exercise, we constructed:\n",
    "\n",
    "- Token embeddings\n",
    "- Positional encoding\n",
    "- Q/K/V projections\n",
    "- Scaled dot-product attention\n",
    "- Residual connections\n",
    "- Feedforward MLP\n",
    "- Final layer output\n",
    "\n",
    "No abstractions.\n",
    "No deep learning framework.\n",
    "No mythology.\n",
    "\n",
    "Just linear algebra and nonlinearity arranged carefully.\n",
    "\n",
    "Run it. Modify weights. Change embeddings. Watch how attention shifts.\n",
    "\n",
    "The diagram stops being intimidating the moment you realize it's just disciplined matrix multiplication repeated at scale.\n",
    "\n",
    "And that's the strange elegance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
