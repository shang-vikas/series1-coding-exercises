{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a58a57f1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-08/exercise-04.ipynb)\n",
    "\n",
    "# ðŸ§ª Exercise 4 â€” Build One Transformer Layer from Scratch\n",
    "\n",
    "## ðŸŽ¯ Goal\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "- Convert tokens into embeddings\n",
    "- Inject positional information\n",
    "- Compute Q, K, V projections\n",
    "- Perform scaled dot-product attention\n",
    "- Apply residual connections\n",
    "- Run a feed-forward MLP\n",
    "- Produce the final output of one Transformer layer\n",
    "\n",
    "Everything is small. Everything is explicit. Nothing is hidden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e3ddb",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 1 â€” Setup\n",
    "\n",
    "**Goal:** Import numpy and configure clean printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57325d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8f3f2",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 2 â€” Define Token Embeddings\n",
    "\n",
    "**Goal:** Represent tokens as vectors (learned embeddings)\n",
    "\n",
    "These are learned numbers. Not symbols. Not grammar. Geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f1552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence: \"the cat sleeps\"\n",
    "\n",
    "X = np.array([\n",
    "    [0.2, 0.1, 0.0, 0.3],   # \"the\"\n",
    "    [0.9, 0.7, 0.1, 0.0],   # \"cat\"\n",
    "    [0.8, 0.2, 0.6, 0.4]    # \"sleeps\"\n",
    "])\n",
    "\n",
    "print(\"Initial token embeddings:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34006afe",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 3 â€” Add Positional Embeddings\n",
    "\n",
    "**Goal:** Inject order information numerically\n",
    "\n",
    "Now meaning + position are baked into the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ede575",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.array([\n",
    "    [0.05, 0.00, 0.00, 0.00],\n",
    "    [0.00, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.00]\n",
    "])\n",
    "\n",
    "X = X + pos\n",
    "\n",
    "print(\"After adding positional embeddings:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3affbf",
   "metadata": {},
   "source": [
    "## SELF-ATTENTION\n",
    "\n",
    "## ðŸ§  Cell 4 â€” Define Attention Weight Matrices\n",
    "\n",
    "**Goal:** Define learned projection matrices for Q, K, V\n",
    "\n",
    "These are learned during training.\n",
    "Change them, change behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = np.array([\n",
    "    [0.5, 0.1, 0.0, 0.2],\n",
    "    [0.0, 0.6, 0.1, 0.0],\n",
    "    [0.3, 0.0, 0.7, 0.1],\n",
    "    [0.0, 0.2, 0.0, 0.5]\n",
    "])\n",
    "\n",
    "W_K = np.array([\n",
    "    [0.4, 0.0, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.0, 0.1],\n",
    "    [0.2, 0.1, 0.6, 0.0],\n",
    "    [0.0, 0.2, 0.1, 0.4]\n",
    "])\n",
    "\n",
    "W_V = np.array([\n",
    "    [0.3, 0.2, 0.1, 0.0],\n",
    "    [0.0, 0.4, 0.2, 0.1],\n",
    "    [0.1, 0.0, 0.5, 0.2],\n",
    "    [0.2, 0.1, 0.0, 0.6]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75854cb",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 5 â€” Compute Q, K, V\n",
    "\n",
    "**Goal:** Project tokens into query, key, and value spaces\n",
    "\n",
    "Same tokens. Different roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1f74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"Q:\\n\", Q)\n",
    "print(\"\\nK:\\n\", K)\n",
    "print(\"\\nV:\\n\", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70561a2c",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 6 â€” Compute Scaled Dot-Product Attention\n",
    "\n",
    "**Goal:** Measure similarity between tokens\n",
    "\n",
    "Each entry (i, j) answers:\n",
    "How relevant is token j to token i?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "\n",
    "scores = Q @ K.T\n",
    "scaled_scores = scores / np.sqrt(d_model)\n",
    "\n",
    "print(\"Scaled attention scores:\\n\", scaled_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694458c",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 7 â€” Apply Softmax\n",
    "\n",
    "**Goal:** Convert similarity scores into attention weights\n",
    "\n",
    "Each row is now a probability distribution.\n",
    "\n",
    "Attention never picks one token.\n",
    "It blends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf070c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "A = softmax(scaled_scores)\n",
    "\n",
    "print(\"Attention weights:\\n\", A)\n",
    "print(\"Row sums (should equal 1):\\n\", A.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515f427",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 8 â€” Compute Attention Output\n",
    "\n",
    "**Goal:** Mix value vectors according to attention weights\n",
    "\n",
    "Each token is now contextualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231440f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = A @ V\n",
    "\n",
    "print(\"Attention output:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a2d9e",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "## ðŸ§  Cell 9 â€” Add Residual\n",
    "\n",
    "**Goal:** Preserve original information while refining it\n",
    "\n",
    "Depth becomes refinement, not replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_1 = X + attention_output\n",
    "\n",
    "print(\"After first residual connection:\\n\", residual_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de66fe0",
   "metadata": {},
   "source": [
    "## MLP Block\n",
    "\n",
    "## ðŸ§  Cell 10 â€” Define MLP Weights\n",
    "\n",
    "**Goal:** Define feedforward network weights\n",
    "\n",
    "The MLP transforms each token independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding to hidden size 8, then projecting back to 4\n",
    "W1 = np.random.randn(4, 8) * 0.5\n",
    "W2 = np.random.randn(8, 4) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef93c0",
   "metadata": {},
   "source": [
    "## ðŸ§  Cell 11 â€” MLP Forward Pass\n",
    "\n",
    "**Goal:** Apply nonlinear transformation to each token\n",
    "\n",
    "Attention decides what to look at.\n",
    "MLP decides what to become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "mlp_hidden = relu(residual_1 @ W1)\n",
    "mlp_output = mlp_hidden @ W2\n",
    "\n",
    "print(\"MLP output:\\n\", mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5b98cd",
   "metadata": {},
   "source": [
    "## Final Residual\n",
    "\n",
    "## ðŸ§  Cell 12 â€” Final Output of One Transformer Layer\n",
    "\n",
    "**Goal:** Produce final output of the Transformer layer\n",
    "\n",
    "You have just executed one complete Transformer layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = residual_1 + mlp_output\n",
    "\n",
    "print(\"Final layer output:\\n\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c1360",
   "metadata": {},
   "source": [
    "## ðŸŽ“ What You Just Built\n",
    "\n",
    "- Token embeddings\n",
    "- Positional encoding\n",
    "- Q/K/V projections\n",
    "- Scaled dot-product attention\n",
    "- Residual connections\n",
    "- Feedforward MLP\n",
    "- Final layer output\n",
    "\n",
    "No abstractions.\n",
    "No deep learning framework.\n",
    "No mythology.\n",
    "\n",
    "Just linear algebra and nonlinearity arranged carefully.\n",
    "\n",
    "Run it. Modify weights. Change embeddings. Watch how attention shifts.\n",
    "\n",
    "The diagram stops being intimidating the moment you realize it's just disciplined matrix multiplication repeated at scale.\n",
    "\n",
    "And that's the strange elegance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea629cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0b070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494fe10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d2710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1242aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a5acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bfc899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901babc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f992211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f66441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59515f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e554900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc1bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396a2a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ebfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0181d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8852dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd2236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e631a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
