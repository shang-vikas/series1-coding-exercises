{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 03: Go Beyond Full-Batch Training (Challenge Mode)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-031/exercise-03.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ numpy is already installed\n",
            "âœ“ scikit-learn is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"âœ“ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}....\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ“ {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"numpy\")\n",
        "install_if_missing(\"scikit-learn\", \"sklearn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You already trained using full-batch gradient descent.\n",
        "\n",
        "Now implement three real-world improvements manually:\n",
        "\n",
        "1. Mini-batch SGD\n",
        "2. L2 Regularization\n",
        "3. Overfitting Demonstration\n",
        "\n",
        "**No frameworks.**\n",
        "**Still NumPy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial Setup â€” Load Data and Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load real dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target.reshape(-1, 1)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define functions\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_loss(p, y):\n",
        "    eps = 1e-8\n",
        "    return -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))\n",
        "\n",
        "def accuracy(p, y):\n",
        "    preds = (p > 0.5).astype(int)\n",
        "    return np.mean(preds == y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¥ Part A â€” Implement Mini-Batch SGD\n",
        "\n",
        "Right now, you use:\n",
        "\n",
        "```\n",
        "dW = X.T @ dz / N\n",
        "```\n",
        "\n",
        "That's full-batch.\n",
        "\n",
        "Now split training data into mini-batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 â€” Create Mini-Batch Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6817 | Train Acc: 0.6286\n",
            "Epoch 10 | Loss: 0.1640 | Train Acc: 0.9626\n",
            "Epoch 20 | Loss: 0.0863 | Train Acc: 0.9824\n",
            "Epoch 30 | Loss: 0.0694 | Train Acc: 0.9846\n",
            "Epoch 40 | Loss: 0.0616 | Train Acc: 0.9824\n",
            "Epoch 50 | Loss: 0.0570 | Train Acc: 0.9824\n",
            "Epoch 60 | Loss: 0.0537 | Train Acc: 0.9824\n",
            "Epoch 70 | Loss: 0.0510 | Train Acc: 0.9824\n",
            "Epoch 80 | Loss: 0.0491 | Train Acc: 0.9824\n",
            "Epoch 90 | Loss: 0.0470 | Train Acc: 0.9824\n"
          ]
        }
      ],
      "source": [
        "# Initialize network\n",
        "input_dim = X_train.shape[1]\n",
        "hidden_dim = 16\n",
        "\n",
        "W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "b1 = np.zeros((1, hidden_dim))\n",
        "\n",
        "W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "lr = 0.05\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Shuffle data each epoch\n",
        "    indices = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    for start in range(0, len(X_train_shuffled), batch_size):\n",
        "        end = start + batch_size\n",
        "        X_batch = X_train_shuffled[start:end]\n",
        "        y_batch = y_train_shuffled[start:end]\n",
        "\n",
        "        # Forward\n",
        "        z1 = X_batch @ W1 + b1\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = a1 @ W2 + b2\n",
        "        p = sigmoid(z2)\n",
        "\n",
        "        # Backward\n",
        "        dz2 = p - y_batch\n",
        "        dW2 = a1.T @ dz2 / len(X_batch)\n",
        "        db2 = np.mean(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = dz2 @ W2.T\n",
        "        dz1 = da1 * relu_derivative(z1)\n",
        "\n",
        "        dW1 = X_batch.T @ dz1 / len(X_batch)\n",
        "        db1 = np.mean(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Update\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        # Evaluate on full training set\n",
        "        z1_eval = X_train @ W1 + b1\n",
        "        a1_eval = relu(z1_eval)\n",
        "        z2_eval = a1_eval @ W2 + b2\n",
        "        p_eval = sigmoid(z2_eval)\n",
        "        loss = compute_loss(p_eval, y_train)\n",
        "        train_acc = accuracy(p_eval, y_train)\n",
        "        print(f\"Epoch {epoch} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¥ Part B â€” Add L2 Regularization Manually\n",
        "\n",
        "Overfitting happens when weights grow large.\n",
        "\n",
        "We fix that by adding penalty:\n",
        "\n",
        "**Loss = original_loss + Î» * ||W||Â²**\n",
        "\n",
        "Gradient becomes:\n",
        "\n",
        "**dW += Î» * W**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modify Backward Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6814 | Train Acc: 0.6286\n",
            "Epoch 10 | Loss: 0.1798 | Train Acc: 0.9560\n",
            "Epoch 20 | Loss: 0.0956 | Train Acc: 0.9824\n",
            "Epoch 30 | Loss: 0.0766 | Train Acc: 0.9824\n",
            "Epoch 40 | Loss: 0.0691 | Train Acc: 0.9824\n",
            "Epoch 50 | Loss: 0.0650 | Train Acc: 0.9824\n",
            "Epoch 60 | Loss: 0.0623 | Train Acc: 0.9824\n",
            "Epoch 70 | Loss: 0.0600 | Train Acc: 0.9846\n",
            "Epoch 80 | Loss: 0.0587 | Train Acc: 0.9846\n",
            "Epoch 90 | Loss: 0.0575 | Train Acc: 0.9846\n"
          ]
        }
      ],
      "source": [
        "# Re-initialize network\n",
        "W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "b1 = np.zeros((1, hidden_dim))\n",
        "\n",
        "W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "lr = 0.05\n",
        "lambda_reg = 0.01\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Shuffle data each epoch\n",
        "    indices = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train[indices]\n",
        "\n",
        "    for start in range(0, len(X_train_shuffled), batch_size):\n",
        "        end = start + batch_size\n",
        "        X_batch = X_train_shuffled[start:end]\n",
        "        y_batch = y_train_shuffled[start:end]\n",
        "\n",
        "        # Forward\n",
        "        z1 = X_batch @ W1 + b1\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = a1 @ W2 + b2\n",
        "        p = sigmoid(z2)\n",
        "\n",
        "        # Backward\n",
        "        dz2 = p - y_batch\n",
        "        dW2 = a1.T @ dz2 / len(X_batch)\n",
        "        db2 = np.mean(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = dz2 @ W2.T\n",
        "        dz1 = da1 * relu_derivative(z1)\n",
        "\n",
        "        dW1 = X_batch.T @ dz1 / len(X_batch)\n",
        "        db1 = np.mean(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Add L2 regularization\n",
        "        dW2 += lambda_reg * W2\n",
        "        dW1 += lambda_reg * W1\n",
        "\n",
        "        # Update\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        # Evaluate on full training set\n",
        "        z1_eval = X_train @ W1 + b1\n",
        "        a1_eval = relu(z1_eval)\n",
        "        z2_eval = a1_eval @ W2 + b2\n",
        "        p_eval = sigmoid(z2_eval)\n",
        "        loss = compute_loss(p_eval, y_train)\n",
        "        train_acc = accuracy(p_eval, y_train)\n",
        "        print(f\"Epoch {epoch} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What This Does**\n",
        "\n",
        "- Penalizes large weights\n",
        "- Shrinks model complexity\n",
        "- Improves generalization\n",
        "- Stabilizes training\n",
        "\n",
        "Now they see regularization isn't magic.\n",
        "\n",
        "It's just adding gradient pressure toward zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”¥ Part C â€” Overfitting Demonstration\n",
        "\n",
        "Now we make it hurt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shrink Training Data Artificially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "  Train Loss: 0.6903 | Train Acc: 0.6400\n",
            "  Test Loss:  0.6908 | Test Acc:  0.6228\n",
            "  Gap: 0.0172\n",
            "\n",
            "Epoch 100\n",
            "  Train Loss: 0.0555 | Train Acc: 1.0000\n",
            "  Test Loss:  0.1143 | Test Acc:  0.9649\n",
            "  Gap: 0.0351\n",
            "\n",
            "Epoch 200\n",
            "  Train Loss: 0.0214 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0893 | Test Acc:  0.9474\n",
            "  Gap: 0.0526\n",
            "\n",
            "Epoch 300\n",
            "  Train Loss: 0.0125 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0856 | Test Acc:  0.9474\n",
            "  Gap: 0.0526\n",
            "\n",
            "Epoch 400\n",
            "  Train Loss: 0.0084 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0856 | Test Acc:  0.9474\n",
            "  Gap: 0.0526\n",
            "\n",
            "Epoch 500\n",
            "  Train Loss: 0.0061 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0864 | Test Acc:  0.9474\n",
            "  Gap: 0.0526\n",
            "\n",
            "Epoch 600\n",
            "  Train Loss: 0.0047 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0876 | Test Acc:  0.9474\n",
            "  Gap: 0.0526\n",
            "\n",
            "Epoch 700\n",
            "  Train Loss: 0.0038 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0889 | Test Acc:  0.9561\n",
            "  Gap: 0.0439\n",
            "\n",
            "Epoch 800\n",
            "  Train Loss: 0.0031 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0901 | Test Acc:  0.9561\n",
            "  Gap: 0.0439\n",
            "\n",
            "Epoch 900\n",
            "  Train Loss: 0.0026 | Train Acc: 1.0000\n",
            "  Test Loss:  0.0913 | Test Acc:  0.9561\n",
            "  Gap: 0.0439\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Use only 50 training samples\n",
        "small_X = X_train[:50]\n",
        "small_y = y_train[:50]\n",
        "\n",
        "# Increase model capacity\n",
        "hidden_dim = 128\n",
        "\n",
        "# Re-initialize network\n",
        "W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
        "b1 = np.zeros((1, hidden_dim))\n",
        "\n",
        "W2 = np.random.randn(hidden_dim, 1) * 0.01\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "lr = 0.05\n",
        "lambda_reg = 0.0  # No regularization to encourage overfitting\n",
        "batch_size = 32\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Shuffle data each epoch\n",
        "    indices = np.random.permutation(len(small_X))\n",
        "    X_train_shuffled = small_X[indices]\n",
        "    y_train_shuffled = small_y[indices]\n",
        "\n",
        "    for start in range(0, len(X_train_shuffled), batch_size):\n",
        "        end = start + batch_size\n",
        "        X_batch = X_train_shuffled[start:end]\n",
        "        y_batch = y_train_shuffled[start:end]\n",
        "\n",
        "        # Forward\n",
        "        z1 = X_batch @ W1 + b1\n",
        "        a1 = relu(z1)\n",
        "\n",
        "        z2 = a1 @ W2 + b2\n",
        "        p = sigmoid(z2)\n",
        "\n",
        "        # Backward\n",
        "        dz2 = p - y_batch\n",
        "        dW2 = a1.T @ dz2 / len(X_batch)\n",
        "        db2 = np.mean(dz2, axis=0, keepdims=True)\n",
        "\n",
        "        da1 = dz2 @ W2.T\n",
        "        dz1 = da1 * relu_derivative(z1)\n",
        "\n",
        "        dW1 = X_batch.T @ dz1 / len(X_batch)\n",
        "        db1 = np.mean(dz1, axis=0, keepdims=True)\n",
        "\n",
        "        # Add L2 regularization (if enabled)\n",
        "        if lambda_reg > 0:\n",
        "            dW2 += lambda_reg * W2\n",
        "            dW1 += lambda_reg * W1\n",
        "\n",
        "        # Update\n",
        "        W2 -= lr * dW2\n",
        "        b2 -= lr * db2\n",
        "        W1 -= lr * dW1\n",
        "        b1 -= lr * db1\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        # Evaluate on small training set\n",
        "        z1_train = small_X @ W1 + b1\n",
        "        a1_train = relu(z1_train)\n",
        "        z2_train = a1_train @ W2 + b2\n",
        "        p_train = sigmoid(z2_train)\n",
        "        train_loss = compute_loss(p_train, small_y)\n",
        "        train_acc = accuracy(p_train, small_y)\n",
        "        \n",
        "        # Evaluate on full test set\n",
        "        z1_test = X_test @ W1 + b1\n",
        "        a1_test = relu(z1_test)\n",
        "        z2_test = a1_test @ W2 + b2\n",
        "        p_test = sigmoid(z2_test)\n",
        "        test_loss = compute_loss(p_test, y_test)\n",
        "        test_acc = accuracy(p_test, y_test)\n",
        "        \n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Test Loss:  {test_loss:.4f} | Test Acc:  {test_acc:.4f}\")\n",
        "        print(f\"  Gap: {train_acc - test_acc:.4f}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What happens?**\n",
        "\n",
        "- Training accuracy â†’ ~100%\n",
        "- Test accuracy â†’ drops\n",
        "- Loss gap widens\n",
        "\n",
        "That's overfitting.\n",
        "\n",
        "Real.\n",
        "Visible.\n",
        "Measurable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ”¥ Optional â€” Make It Worse\n",
        "\n",
        "Remove regularization.\n",
        "\n",
        "Increase learning rate slightly.\n",
        "\n",
        "Watch instability.\n",
        "\n",
        "**You now see:**\n",
        "\n",
        "- Capacity\n",
        "- Optimization\n",
        "- Generalization\n",
        "\n",
        "interacting in real time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ What This Exercise Teaches\n",
        "\n",
        "- Mini-batch introduces stochasticity.\n",
        "- L2 regularization adds gradient pressure.\n",
        "- Overfitting is capacity vs data imbalance.\n",
        "- Nothing mystical â€” all gradient manipulation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
