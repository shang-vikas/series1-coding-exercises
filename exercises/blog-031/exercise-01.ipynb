{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 01: Train a Tiny 2-Layer Network Manually\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-031/exercise-01.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ numpy is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"âœ“ {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}....\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ“ {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"numpy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Goal:**\n",
        "\n",
        "- Implement forward pass\n",
        "- Compute loss\n",
        "- Manually compute gradients\n",
        "- Update weights\n",
        "- Observe loss decreasek 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 0 - Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 - Tiny Dataset (Realistic-ish Binary Classification)\n",
        "\n",
        "We simulate something simple but meaningful:\n",
        "\n",
        "- If x1 + x2 > 3 â†’ class 1\n",
        "- Else â†’ class 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.array([\n",
        "    [2.0, 3.0],\n",
        "    [1.0, 1.0],\n",
        "    [3.0, 2.0],\n",
        "    [0.5, 0.5]\n",
        "])\n",
        "\n",
        "y = np.array([[1], [0], [1], [0]])\n",
        "\n",
        "# Batch size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2 - Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "W1 = np.random.randn(2, 2) * 0.1\n",
        "b1 = np.zeros((1, 2))\n",
        "\n",
        "W2 = np.random.randn(2, 1) * 0.1\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "lr = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3 - Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_derivative(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_loss(p, y): # just a fancy way of doing y_true - y_pred in maths using log loss with edge case handling over a batch of data.\n",
        "    eps = 1e-8\n",
        "    return -np.mean(y * np.log(p + eps) + (1 - y) * np.log(1 - p + eps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4 â€” One Training Step (Manual Backprop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Loss: 0.1380176200223486\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "z1 = X @ W1 + b1 # @ is matrix multiplication in numpy\n",
        "a1 = relu(z1)\n",
        "\n",
        "z2 = a1 @ W2 + b2\n",
        "p = sigmoid(z2)\n",
        "\n",
        "loss = compute_loss(p, y)\n",
        "print(\"Initial Loss:\", loss)\n",
        "\n",
        "# Backward pass\n",
        "\n",
        "# Output layer gradient\n",
        "dz2 = p - y                      # shape (4,1)\n",
        "\n",
        "dW2 = a1.T @ dz2 / len(X)\n",
        "db2 = np.mean(dz2, axis=0, keepdims=True)\n",
        "\n",
        "# Backprop into layer 1\n",
        "da1 = dz2 @ W2.T\n",
        "dz1 = da1 * relu_derivative(z1)\n",
        "\n",
        "dW1 = X.T @ dz1 / len(X)\n",
        "db1 = np.mean(dz1, axis=0, keepdims=True)\n",
        "\n",
        "# Update weights\n",
        "W2 -= lr * dW2\n",
        "b2 -= lr * db2\n",
        "W1 -= lr * dW1\n",
        "b1 -= lr * db1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5 â€” Run Multiple Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Loss: 0.6931\n",
            "Epoch 20 | Loss: 0.6931\n",
            "Epoch 40 | Loss: 0.6931\n",
            "Epoch 60 | Loss: 0.6931\n",
            "Epoch 80 | Loss: 0.6931\n",
            "Epoch 100 | Loss: 0.6931\n",
            "Epoch 120 | Loss: 0.6931\n",
            "Epoch 140 | Loss: 0.6931\n",
            "Epoch 160 | Loss: 0.6931\n",
            "Epoch 180 | Loss: 0.6931\n"
          ]
        }
      ],
      "source": [
        "# Re-initialize weights for full training\n",
        "W1 = np.random.randn(2, 2) * 0.1\n",
        "b1 = np.zeros((1, 2))\n",
        "\n",
        "W2 = np.random.randn(2, 1) * 0.1\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "for epoch in range(200):\n",
        "    # Forward\n",
        "    z1 = X @ W1 + b1\n",
        "    a1 = relu(z1)\n",
        "\n",
        "    z2 = a1 @ W2 + b2\n",
        "    p = sigmoid(z2)\n",
        "\n",
        "    loss = compute_loss(p, y)\n",
        "\n",
        "    # Backward\n",
        "    dz2 = p - y\n",
        "    dW2 = a1.T @ dz2 / len(X)\n",
        "    db2 = np.mean(dz2, axis=0, keepdims=True)\n",
        "\n",
        "    da1 = dz2 @ W2.T\n",
        "    dz1 = da1 * relu_derivative(z1)\n",
        "\n",
        "    dW1 = X.T @ dz1 / len(X)\n",
        "    db1 = np.mean(dz1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should see loss decrease.\n",
        "\n",
        "**No magic.**\n",
        "\n",
        "Just:\n",
        "\n",
        "- input Ã— gradient\n",
        "- weight Ã— gradient\n",
        "- repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”Ž What You Should Observe\n",
        "\n",
        "- Loss decreases steadily.\n",
        "- Removing ReLU changes behavior.\n",
        "- Increasing learning rate can cause divergence.\n",
        "- Zeroing W2 kills gradient flow to layer 1.\n",
        "- Changing initialization changes training stability.\n",
        "\n",
        "**We Encourage you to:**\n",
        "\n",
        "- Print intermediate gradients.\n",
        "- Set learning rate too high.\n",
        "- Set W2 very small.\n",
        "- Replace ReLU with sigmoid.\n",
        "\n",
        "**Let it break.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
