{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 02: Backpropagation Made Visible\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-03/exercise-02.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì numpy is already installed\n"
          ]
        }
      ],
      "source": [
        "# Install required packages using the kernel's Python interpreter\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def install_if_missing(package, import_name=None):\n",
        "    \"\"\"Install package if it's not already installed.\"\"\"\n",
        "    if import_name is None:\n",
        "        import_name = package\n",
        "\n",
        "    try:\n",
        "        importlib.import_module(import_name)\n",
        "        print(f\"‚úì {package} is already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}....\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"‚úì {package} installed successfully\")\n",
        "\n",
        "# Install required packages\n",
        "install_if_missing(\"numpy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tiny Network\n",
        "\n",
        "We'll build the smallest possible 2-layer network and print the \"blame\" numbers flowing backward.\n",
        "\n",
        "No frameworks. Just NumPy. No symbolic calculus. Just numbers.\n",
        "\n",
        "**Structure:**\n",
        "\n",
        "```\n",
        "x ‚Üí Linear1 ‚Üí ReLU ‚Üí Linear2 ‚Üí output ‚Üí loss\n",
        "```\n",
        "\n",
        "Single training example. One step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forward + Backward (With Printed Blame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Forward:\n",
            " z1: [2.20000000e+00 2.77555756e-17]\n",
            " a1: [2.20000000e+00 2.77555756e-17]\n",
            " y_pred: [2.25]\n",
            " loss: 1.5625\n",
            "\n",
            "Backward (Blame Signals):\n",
            " d_loss_y (blame at output): [2.5]\n",
            " dW2 (blame for W2):\n",
            " [[5.5000000e+00]\n",
            " [6.9388939e-17]]\n",
            " d_a1 (blame flowing to layer1 output): [ 2.5  -3.75]\n",
            " d_z1 (after ReLU gate): [ 2.5  -3.75]\n",
            " dW1 (blame for W1):\n",
            " [[ 2.5  -3.75]\n",
            " [ 5.   -7.5 ]]\n",
            "\n",
            "Updated W1:\n",
            " [[0.25  0.075]\n",
            " [0.3   0.95 ]]\n",
            "Updated W2:\n",
            " [[ 0.45]\n",
            " [-1.5 ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ----- Data -----\n",
        "x = np.array([1.0, 2.0])       # 2 input features\n",
        "y_true = np.array([1.0])       # target\n",
        "\n",
        "# ----- Parameters -----\n",
        "W1 = np.array([[0.5, -0.3],\n",
        "               [0.8,  0.2]])   # shape (2,2)\n",
        "b1 = np.array([0.1, -0.1])\n",
        "\n",
        "W2 = np.array([[1.0],\n",
        "               [-1.5]])        # shape (2,1)\n",
        "b2 = np.array([0.05])\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "# ----- Forward Pass -----\n",
        "z1 = x @ W1 + b1\n",
        "a1 = np.maximum(0, z1)   # ReLU\n",
        "z2 = a1 @ W2 + b2\n",
        "y_pred = z2              # no activation for simplicity\n",
        "\n",
        "loss = np.mean((y_pred - y_true)**2)\n",
        "\n",
        "print(\"Forward:\")\n",
        "print(\" z1:\", z1)\n",
        "print(\" a1:\", a1)\n",
        "print(\" y_pred:\", y_pred)\n",
        "print(\" loss:\", loss)\n",
        "\n",
        "# ----- Backward Pass -----\n",
        "\n",
        "# dLoss/dy_pred\n",
        "d_loss_y = 2 * (y_pred - y_true)\n",
        "\n",
        "# Gradients for W2 and b2\n",
        "dW2 = np.outer(a1, d_loss_y)\n",
        "db2 = d_loss_y\n",
        "\n",
        "# Propagate blame to a1\n",
        "d_a1 = d_loss_y @ W2.T\n",
        "\n",
        "# ReLU local gradient\n",
        "d_z1 = d_a1 * (z1 > 0)\n",
        "\n",
        "# Gradients for W1 and b1\n",
        "dW1 = np.outer(x, d_z1)\n",
        "db1 = d_z1\n",
        "\n",
        "print(\"\\nBackward (Blame Signals):\")\n",
        "print(\" d_loss_y (blame at output):\", d_loss_y)\n",
        "print(\" dW2 (blame for W2):\\n\", dW2)\n",
        "print(\" d_a1 (blame flowing to layer1 output):\", d_a1)\n",
        "print(\" d_z1 (after ReLU gate):\", d_z1)\n",
        "print(\" dW1 (blame for W1):\\n\", dW1)\n",
        "\n",
        "# ----- Update -----\n",
        "W1 -= lr * dW1\n",
        "b1 -= lr * db1\n",
        "W2 -= lr * dW2\n",
        "b2 -= lr * db2\n",
        "\n",
        "print(\"\\nUpdated W1:\\n\", W1)\n",
        "print(\"Updated W2:\\n\", W2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What Readers Should Notice\n",
        "\n",
        "### `d_loss_y`\n",
        "The first blame signal.\n",
        "Just: \"How wrong was the output?\"\n",
        "\n",
        "### `dW2`\n",
        "Blame assigned to last layer weights.\n",
        "Larger `a1` ‚Üí larger blame on corresponding weight.\n",
        "\n",
        "### `d_a1`\n",
        "Blame flows backward through `W2`.\n",
        "If `W2` is large, earlier layers inherit larger responsibility.\n",
        "\n",
        "### `d_z1`\n",
        "ReLU acts like a gate.\n",
        "If a neuron was inactive, its blame is zero.\n",
        "Dead neurons don't get blamed.\n",
        "\n",
        "### `dW1`\n",
        "Now earlier weights receive proportional blame.\n",
        "\n",
        "**No neuron \"realizes\" anything.**\n",
        "\n",
        "Numbers just flow backward based on local sensitivities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Engineer Interpretation\n",
        "\n",
        "Think of each `dSomething` as:\n",
        "\n",
        "**\"If this value had been slightly different, how much would the final loss change?\"**\n",
        "\n",
        "That's it.\n",
        "\n",
        "Backprop is just:\n",
        "\n",
        "1. compute output error\n",
        "2. propagate responsibility backward\n",
        "3. update knobs proportionally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insight\n",
        "\n",
        "**Backpropagation is not intelligence.**\n",
        "\n",
        "It is a systematic way of distributing blame across chained computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Blame Flow Visual Demo\n",
        "\n",
        "This prints gradient magnitudes as bars so readers see intensity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FORWARD PASS\n",
            "Layer1 activations: [2.20000000e+00 2.77555756e-17]\n",
            "Output: [2.25]\n",
            "Loss: 1.5625\n",
            "\n",
            "BACKWARD PASS (Blame Intensity)\n",
            "\n",
            "Output blame: [2.5] |||||||||||||||||||||||||\n",
            "\n",
            "Layer2 weight blame:\n",
            "W2[0] ‚Üí 5.5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "W2[1] ‚Üí 6.938893903907228e-17 \n",
            "\n",
            "Layer1 activation blame:\n",
            "a1[0] ‚Üí 2.5 |||||||||||||||||||||||||\n",
            "a1[1] ‚Üí -3.75 |||||||||||||||||||||||||||||||||||||\n",
            "\n",
            "Layer1 weight blame:\n",
            "W1[0] ‚Üí 2.5 |||||||||||||||||||||||||\n",
            "W1[1] ‚Üí -3.75 |||||||||||||||||||||||||||||||||||||\n",
            "W1[2] ‚Üí 5.0 ||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "W1[3] ‚Üí -7.5 |||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def bar(x):\n",
        "    return \"|\" * int(abs(x) * 10)\n",
        "\n",
        "# ----- Data -----\n",
        "x = np.array([1.0, 2.0])\n",
        "y_true = np.array([1.0])\n",
        "\n",
        "# ----- Parameters -----\n",
        "W1 = np.array([[0.5, -0.3],\n",
        "               [0.8,  0.2]])\n",
        "b1 = np.array([0.1, -0.1])\n",
        "\n",
        "W2 = np.array([[1.0],\n",
        "               [-1.5]])\n",
        "b2 = np.array([0.05])\n",
        "\n",
        "# ----- Forward -----\n",
        "z1 = x @ W1 + b1\n",
        "a1 = np.maximum(0, z1)\n",
        "z2 = a1 @ W2 + b2\n",
        "y_pred = z2\n",
        "\n",
        "loss = np.mean((y_pred - y_true)**2)\n",
        "\n",
        "print(\"\\nFORWARD PASS\")\n",
        "print(\"Layer1 activations:\", a1)\n",
        "print(\"Output:\", y_pred)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "# ----- Backward -----\n",
        "d_loss_y = 2 * (y_pred - y_true)\n",
        "dW2 = np.outer(a1, d_loss_y)\n",
        "d_a1 = d_loss_y @ W2.T\n",
        "d_z1 = d_a1 * (z1 > 0)\n",
        "dW1 = np.outer(x, d_z1)\n",
        "\n",
        "print(\"\\nBACKWARD PASS (Blame Intensity)\")\n",
        "print(\"\\nOutput blame:\", d_loss_y, bar(d_loss_y[0]))\n",
        "\n",
        "print(\"\\nLayer2 weight blame:\")\n",
        "for i, val in enumerate(dW2.flatten()):\n",
        "    print(f\"W2[{i}] ‚Üí\", val, bar(val))\n",
        "\n",
        "print(\"\\nLayer1 activation blame:\")\n",
        "for i, val in enumerate(d_a1.flatten()):\n",
        "    print(f\"a1[{i}] ‚Üí\", val, bar(val))\n",
        "\n",
        "print(\"\\nLayer1 weight blame:\")\n",
        "for i, val in enumerate(dW1.flatten()):\n",
        "    print(f\"W1[{i}] ‚Üí\", val, bar(val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç What This Demonstrates Visually\n",
        "\n",
        "When loss is high:\n",
        "\n",
        "- Output layer shows strong blame.\n",
        "- That blame distributes backward.\n",
        "- Earlier layers get weaker signals.\n",
        "- If a ReLU neuron was inactive ‚Üí zero bars.\n",
        "\n",
        "It becomes obvious:\n",
        "\n",
        "**Backprop is just blame propagation through multiplications.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Visual Diagram\n",
        "\n",
        "**FORWARD:**\n",
        "```\n",
        "x ‚Üí [Layer1] ‚Üí [Layer2] ‚Üí y_pred ‚Üí loss\n",
        "```\n",
        "\n",
        "**BACKWARD:**\n",
        "```\n",
        "loss ‚Üí blame ‚Üí Layer2 ‚Üí blame ‚Üí Layer1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
