{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d1584ea5",
      "metadata": {
        "id": "d1584ea5"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-05/exercise-00.ipynb)\n",
        "\n",
        "# ðŸ§ª Exercise 1 â€” Why Bag of Words Fails\n",
        "\n",
        "\n",
        "**Goal:** Destroy order mechanically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fbb60ebc",
      "metadata": {
        "id": "fbb60ebc",
        "outputId": "f68a2433-fb53-4b1d-a59a-e46ccd51669d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'dog': 1, 'bites': 0, 'man': 2}\n",
            "Vectors:\n",
            " [[1 1 1]\n",
            " [1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sentences = [\"dog bites man\", \"man bites dog\"]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"Vectors:\\n\", X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d07ff52",
      "metadata": {
        "id": "7d07ff52"
      },
      "source": [
        "**Expected output:**\n",
        "\n",
        "```\n",
        "Vocabulary: {'dog': 0, 'bites': 1, 'man': 2}\n",
        "Vectors:\n",
        "[[1 1 1]\n",
        " [1 1 1]]\n",
        "```\n",
        "\n",
        "Same vector.\n",
        "\n",
        "Different meaning.\n",
        "\n",
        "This is the mechanical reason RNNs existed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7eaf701",
      "metadata": {
        "id": "d7eaf701"
      },
      "source": [
        "# ðŸ§ª Exercise 2 â€” Manual Tiny RNN Forward Pass\n",
        "\n",
        "**Goal:** See recurrence happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "019bf90d",
      "metadata": {
        "id": "019bf90d",
        "outputId": "2904249f-987e-44a3-f76c-e32a2ede639e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 hidden: [[ 0.45952909 -0.13738992]]\n",
            "Step 2 hidden: [[0.89327092 0.94692458]]\n",
            "Step 3 hidden: [[0.62425975 0.74656684]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Input size = 3, hidden size = 2\n",
        "Wx = np.random.randn(3, 2)\n",
        "Wh = np.random.randn(2, 2)\n",
        "b = np.zeros((1, 2))\n",
        "\n",
        "def rnn_step(x, h_prev):\n",
        "    return np.tanh(x @ Wx + h_prev @ Wh + b)\n",
        "\n",
        "# Sequence of 3 one-hot inputs\n",
        "x_seq = [\n",
        "    np.array([[1,0,0]]),\n",
        "    np.array([[0,1,0]]),\n",
        "    np.array([[0,0,1]])\n",
        "]\n",
        "\n",
        "h = np.zeros((1,2))\n",
        "\n",
        "for t, x in enumerate(x_seq):\n",
        "    h = rnn_step(x, h)\n",
        "    print(f\"Step {t+1} hidden:\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7635f48",
      "metadata": {
        "id": "a7635f48"
      },
      "source": [
        "**Observe:**\n",
        "\n",
        "- Same Wx, Wh reused\n",
        "- Hidden state evolves\n",
        "- State depends on previous state\n",
        "\n",
        "This is recurrence in its purest form."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b36a4f6",
      "metadata": {
        "id": "2b36a4f6"
      },
      "source": [
        "# ðŸ§ª Exercise 3 â€” Show Hidden State Overwrite\n",
        "\n",
        "Now prepend noise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3498543b",
      "metadata": {
        "id": "3498543b",
        "outputId": "7f3236c8-ad09-40d7-be58-dd7f4e7a578e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final hidden state after noise: [[-0.14714332  0.23473421]]\n"
          ]
        }
      ],
      "source": [
        "noise = [np.random.randn(1,3) for _ in range(5)]\n",
        "new_seq = noise + x_seq\n",
        "\n",
        "h = np.zeros((1,2))\n",
        "\n",
        "for t, x in enumerate(new_seq):\n",
        "    h = rnn_step(x, h)\n",
        "\n",
        "print(\"Final hidden state after noise:\", h)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2860bd",
      "metadata": {
        "id": "ab2860bd"
      },
      "source": [
        "Now compare:\n",
        "\n",
        "- Without noise\n",
        "- With noise\n",
        "\n",
        "Early signal gets overwritten.\n",
        "\n",
        "Memory fragility becomes obvious."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb04959d",
      "metadata": {
        "id": "eb04959d"
      },
      "source": [
        "# ðŸ§ª Exercise 4 â€” Train a Real RNN (Tiny Shakespeare)\n",
        "\n",
        "We'll do character-level modeling.\n",
        "\n",
        "## Step 1 â€” Load tiny dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4afa9157",
      "metadata": {
        "id": "4afa9157"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "text = requests.get(url).text[:50000]  # small subset\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for ch,i in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6eeef0b",
      "metadata": {
        "id": "f6eeef0b"
      },
      "source": [
        "## Step 2 â€” Create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a131586a",
      "metadata": {
        "id": "a131586a"
      },
      "outputs": [],
      "source": [
        "sequence_length = 50\n",
        "\n",
        "data = []\n",
        "for i in range(len(text) - sequence_length):\n",
        "    seq = text[i:i+sequence_length]\n",
        "    target = text[i+1:i+sequence_length+1]\n",
        "    data.append((seq, target))\n",
        "\n",
        "def encode(seq):\n",
        "    return torch.tensor([char_to_idx[c] for c in seq])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04061917",
      "metadata": {
        "id": "04061917"
      },
      "source": [
        "## Step 3 â€” Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c5de2ac1",
      "metadata": {
        "id": "c5de2ac1"
      },
      "outputs": [],
      "source": [
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = SimpleRNN(vocab_size, hidden_size=128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "390d1208",
      "metadata": {
        "id": "390d1208"
      },
      "source": [
        "## Step 4 â€” Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca42e8aa",
      "metadata": {
        "id": "ca42e8aa",
        "outputId": "ad14057b-2f1a-4dca-e9a9-0252a1f691e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.6322\n",
            "Epoch 2 | Loss: 0.5484\n",
            "Epoch 3 | Loss: 0.5348\n",
            "Epoch 4 | Loss: 0.5000\n",
            "Epoch 5 | Loss: 0.4635\n"
          ]
        }
      ],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(0, 1000):  # small subset for speed\n",
        "        seq, target = data[i]\n",
        "        x = encode(seq).unsqueeze(0)\n",
        "        y = encode(target)\n",
        "\n",
        "        out = model(x)\n",
        "        loss = criterion(out.view(-1, vocab_size), y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / 1000\n",
        "\n",
        "for epoch in range(5):\n",
        "    loss = train_epoch()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d12b66",
      "metadata": {
        "id": "a1d12b66"
      },
      "source": [
        "Now Observe that:\n",
        "\n",
        "- Same weights reused per step\n",
        "- Sequential processing\n",
        "- Loss decreasing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dcfa7f9",
      "metadata": {
        "id": "8dcfa7f9"
      },
      "source": [
        "# ðŸ§ª Exercise 5 â€” Demonstrate Vanishing Gradient\n",
        "\n",
        "Increase sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b5742637",
      "metadata": {
        "id": "b5742637"
      },
      "outputs": [],
      "source": [
        "sequence_length = 200  # try 20, 50, 200\n",
        "\n",
        "# Recreate data with new sequence length\n",
        "data = []\n",
        "for i in range(len(text) - sequence_length):\n",
        "    seq = text[i:i+sequence_length]\n",
        "    target = text[i+1:i+sequence_length+1]\n",
        "    data.append((seq, target))\n",
        "\n",
        "# Reinitialize model\n",
        "model = SimpleRNN(vocab_size, hidden_size=128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df1cc28",
      "metadata": {
        "id": "6df1cc28"
      },
      "source": [
        "Then inspect gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "72cda8c0",
      "metadata": {
        "id": "72cda8c0",
        "outputId": "b78cfc53-1e68-476a-87a1-f71ed6e1ea45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedding.weight 0.023717757314443588\n",
            "rnn.weight_ih_l0 0.4803493320941925\n",
            "rnn.weight_hh_l0 0.2789147198200226\n",
            "rnn.bias_ih_l0 0.10563002526760101\n",
            "rnn.bias_hh_l0 0.10563002526760101\n",
            "fc.weight 0.5119559168815613\n",
            "fc.bias 0.20093689858913422\n"
          ]
        }
      ],
      "source": [
        "# Run one training step to get gradients\n",
        "seq, target = data[0]\n",
        "x = encode(seq).unsqueeze(0)\n",
        "y = encode(target)\n",
        "\n",
        "out = model(x)\n",
        "loss = criterion(out.view(-1, vocab_size), y)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(name, param.grad.norm().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aedd7a1f",
      "metadata": {
        "id": "aedd7a1f"
      },
      "source": [
        "As sequence length increases:\n",
        "\n",
        "- Early layers' gradients shrink\n",
        "- Training slows\n",
        "\n",
        "This makes vanishing gradient concrete."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f361f3",
      "metadata": {
        "id": "d7f361f3"
      },
      "source": [
        "# ðŸ§ª Exercise 6 -> Serial Bottleneck Demonstration\n",
        "\n",
        "Measure time per epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "157da83e",
      "metadata": {
        "id": "157da83e",
        "outputId": "19b7c693-b4fa-4fb3-f5fc-009f8929e6f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time per epoch: 26.345571517944336\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "train_epoch()\n",
        "end = time.time()\n",
        "\n",
        "print(\"Time per epoch:\", end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6180e4ff",
      "metadata": {
        "id": "6180e4ff"
      },
      "source": [
        "Now test:\n",
        "\n",
        "- `sequence_length = 20`\n",
        "- `sequence_length = 200`\n",
        "\n",
        "Longer sequence â†’ slower epoch.\n",
        "\n",
        "Why?\n",
        "\n",
        "Because RNN must compute:\n",
        "\n",
        "t1 â†’ t2 â†’ t3 â†’ ... â†’ t200\n",
        "\n",
        "No parallel shortcut.\n",
        "\n",
        "This is the structural bottleneck."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d829cf",
      "metadata": {
        "id": "85d829cf"
      },
      "source": [
        "## What You Just Saw\n",
        "\n",
        "- Bag of words destroys order.\n",
        "- RNN carries evolving hidden state.\n",
        "- Memory gets overwritten.\n",
        "- Gradients weaken over long sequences.\n",
        "- Computation is serial.\n",
        "- Training time scales with sequence length.\n",
        "\n",
        "No mythology.\n",
        "\n",
        "No metaphors.\n",
        "\n",
        "Just mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e896a942",
      "metadata": {
        "id": "e896a942"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}