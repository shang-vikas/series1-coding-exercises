{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d0b204d2",
      "metadata": {
        "id": "d0b204d2"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-05/exercise-01.ipynb)\n",
        "\n",
        "# üß™ IMDB Sentiment Classification ‚Äî Vanilla RNN\n",
        "\n",
        "**Goal:** Build and train a vanilla RNN for sentiment classification to experience how RNNs process sequences, compress information into hidden states, and reveal their fundamental limitations.\n",
        "\n",
        "## 1Ô∏è‚É£ Install + Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f8f87e38",
      "metadata": {
        "id": "f8f87e38"
      },
      "outputs": [],
      "source": [
        "%pip install datasets -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from collections import Counter\n",
        "import time\n",
        "import re\n",
        "\n",
        "def basic_english_tokenizer(text):\n",
        "    \"\"\"Simple tokenizer that splits on whitespace and converts to lowercase.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f97043",
      "metadata": {
        "id": "81f97043"
      },
      "source": [
        "## 2Ô∏è‚É£ Load IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "70ba88ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70ba88ba",
        "outputId": "e5913d61-9b30-45ee-bd3e-178b38b6984a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 25000\n",
            "Test samples: 25000\n",
            "\n",
            "Sample review:\n",
            "Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it w...\n",
            "Label: 0 (0=neg, 1=pos)\n"
          ]
        }
      ],
      "source": [
        "# Load IMDB dataset from Hugging Face datasets\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "print(\"Train samples:\", len(train_data))\n",
        "print(\"Test samples:\", len(test_data))\n",
        "print(\"\\nSample review:\")\n",
        "print(\"Text:\", train_data[0]['text'][:100] + \"...\")\n",
        "print(\"Label:\", train_data[0]['label'], \"(0=neg, 1=pos)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa11309",
      "metadata": {
        "id": "8fa11309"
      },
      "source": [
        "IMDB contains:\n",
        "\n",
        "- 25,000 training reviews\n",
        "- 25,000 test reviews\n",
        "- Binary sentiment: pos / neg\n",
        "\n",
        "## 3Ô∏è‚É£ Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "86863e24",
      "metadata": {
        "id": "86863e24"
      },
      "outputs": [],
      "source": [
        "# Tokenizer is defined above in imports\n",
        "tokenizer = basic_english_tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dbe5e3d",
      "metadata": {
        "id": "6dbe5e3d"
      },
      "source": [
        "## 4Ô∏è‚É£ Build Vocabulary\n",
        "\n",
        "We restrict vocab size to keep training manageable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cb903078",
      "metadata": {
        "id": "cb903078"
      },
      "outputs": [],
      "source": [
        "counter = Counter()\n",
        "\n",
        "for example in train_data:\n",
        "    text = example['text']\n",
        "    tokens = tokenizer(text)\n",
        "    counter.update(tokens)\n",
        "\n",
        "vocab_size = 20000\n",
        "most_common = counter.most_common(vocab_size - 2)\n",
        "\n",
        "vocab = {word: idx+2 for idx, (word, _) in enumerate(most_common)}\n",
        "vocab[\"<pad>\"] = 0\n",
        "vocab[\"<unk>\"] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42c2b26",
      "metadata": {
        "id": "e42c2b26"
      },
      "source": [
        "## 5Ô∏è‚É£ Numericalize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d9c07cdb",
      "metadata": {
        "id": "d9c07cdb"
      },
      "outputs": [],
      "source": [
        "def encode(text):\n",
        "    tokens = tokenizer(text)\n",
        "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51539856",
      "metadata": {
        "id": "51539856"
      },
      "source": [
        "## 6Ô∏è‚É£ Collate Function (Padding)\n",
        "\n",
        "RNNs need fixed batch lengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fbf27e7d",
      "metadata": {
        "id": "fbf27e7d"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    texts, labels = [], []\n",
        "\n",
        "    for example in batch:\n",
        "        text = example['text']\n",
        "        label = example['label']  # Already 0 or 1\n",
        "        encoded = torch.tensor(encode(text))\n",
        "        texts.append(encoded)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Get lengths before padding\n",
        "    lengths = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
        "\n",
        "    texts = pad_sequence(texts, batch_first=True)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return texts, lengths, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8563b888",
      "metadata": {
        "id": "8563b888"
      },
      "source": [
        "## 7Ô∏è‚É£ DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cebeca39",
      "metadata": {
        "id": "cebeca39"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_data, batch_size=64, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a734ce0",
      "metadata": {
        "id": "5a734ce0"
      },
      "source": [
        "## 8Ô∏è‚É£ Define Vanilla RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9ac4a0cb",
      "metadata": {
        "id": "9ac4a0cb"
      },
      "outputs": [],
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        out, hidden = self.rnn(packed)\n",
        "\n",
        "        # Use last hidden state\n",
        "        final_hidden = hidden.squeeze(0)\n",
        "        return self.fc(final_hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba99bb1",
      "metadata": {
        "id": "2ba99bb1"
      },
      "source": [
        "## 9Ô∏è‚É£ Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b2008b86",
      "metadata": {
        "id": "b2008b86"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = VanillaRNN(vocab_size, embed_dim=100, hidden_dim=128).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b538ca1d",
      "metadata": {
        "id": "b538ca1d"
      },
      "source": [
        "## üîü Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9f697b77",
      "metadata": {
        "id": "9f697b77"
      },
      "outputs": [],
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for texts, lengths, labels in train_loader:\n",
        "        texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device).float()\n",
        "\n",
        "        outputs = model(texts, lengths).view(-1)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d87fae",
      "metadata": {
        "id": "14d87fae"
      },
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6c9640b1",
      "metadata": {
        "id": "6c9640b1"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for texts, lengths, labels in test_loader:\n",
        "            texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device)\n",
        "            outputs = torch.sigmoid(model(texts, lengths).view(-1))\n",
        "            preds = (outputs > 0.5).long()\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbad5c0",
      "metadata": {
        "id": "8cbad5c0"
      },
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e7581b24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7581b24",
        "outputId": "60408514-188f-4e18-b201-9811f56fd975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.6697 | Test Acc: 0.6164 | Time: 30.18s\n",
            "Epoch 2 | Loss: 0.6171 | Test Acc: 0.6070 | Time: 28.35s\n",
            "Epoch 3 | Loss: 0.5827 | Test Acc: 0.5927 | Time: 28.15s\n",
            "Epoch 4 | Loss: 0.5783 | Test Acc: 0.7030 | Time: 29.10s\n",
            "Epoch 5 | Loss: 0.5113 | Test Acc: 0.7282 | Time: 28.50s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(5):\n",
        "    start = time.time()\n",
        "\n",
        "    loss = train_epoch()\n",
        "    acc = evaluate()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {acc:.4f} | Time: {time.time()-start:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "24jHhpA8qIFH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24jHhpA8qIFH",
        "outputId": "4a26c26b-fcfe-4d87-fc33-7573f2f194d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.4793 | Test Acc: 0.7396 | Time: 26.75s\n",
            "Epoch 2 | Loss: 0.4260 | Test Acc: 0.7644 | Time: 27.72s\n",
            "Epoch 3 | Loss: 0.4200 | Test Acc: 0.6746 | Time: 27.02s\n",
            "Epoch 4 | Loss: 0.3954 | Test Acc: 0.7620 | Time: 26.99s\n",
            "Epoch 5 | Loss: 0.3341 | Test Acc: 0.7595 | Time: 27.59s\n",
            "Epoch 6 | Loss: 0.3125 | Test Acc: 0.7578 | Time: 28.00s\n",
            "Epoch 7 | Loss: 0.2738 | Test Acc: 0.7716 | Time: 27.21s\n",
            "Epoch 8 | Loss: 0.2448 | Test Acc: 0.7557 | Time: 26.70s\n",
            "Epoch 9 | Loss: 0.2184 | Test Acc: 0.7758 | Time: 27.32s\n",
            "Epoch 10 | Loss: 0.1984 | Test Acc: 0.7727 | Time: 27.05s\n",
            "Epoch 11 | Loss: 0.1676 | Test Acc: 0.7815 | Time: 26.80s\n",
            "Epoch 12 | Loss: 0.3751 | Test Acc: 0.6184 | Time: 27.11s\n",
            "Epoch 13 | Loss: 0.4855 | Test Acc: 0.6952 | Time: 27.90s\n",
            "Epoch 14 | Loss: 0.4532 | Test Acc: 0.6524 | Time: 27.27s\n",
            "Epoch 15 | Loss: 0.3630 | Test Acc: 0.7384 | Time: 26.56s\n",
            "Epoch 16 | Loss: 0.3009 | Test Acc: 0.7330 | Time: 26.77s\n",
            "Epoch 17 | Loss: 0.2622 | Test Acc: 0.7276 | Time: 27.44s\n",
            "Epoch 18 | Loss: 0.2682 | Test Acc: 0.7611 | Time: 26.95s\n",
            "Epoch 19 | Loss: 0.2427 | Test Acc: 0.7555 | Time: 27.15s\n",
            "Epoch 20 | Loss: 0.3402 | Test Acc: 0.6799 | Time: 27.33s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(20):\n",
        "    start = time.time()\n",
        "\n",
        "    loss = train_epoch()\n",
        "    acc = evaluate()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test Acc: {acc:.4f} | Time: {time.time()-start:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c86bb3",
      "metadata": {
        "id": "c4c86bb3"
      },
      "source": [
        "You should see:\n",
        "\n",
        "- Accuracy around 80‚Äì85%.\n",
        "- Good.\n",
        "- But not state-of-the-art.\n",
        "\n",
        "And that's intentional.\n",
        "\n",
        "## ‚úÖ What This Exercise Teaches\n",
        "\n",
        "1Ô∏è‚É£ **Order matters**\n",
        "\n",
        "Unlike bag-of-words, performance is significantly higher.\n",
        "\n",
        "2Ô∏è‚É£ **Hidden state compresses entire review**\n",
        "\n",
        "Final decision comes from a single vector.\n",
        "\n",
        "3Ô∏è‚É£ **Same weights reused per timestep**\n",
        "\n",
        "True recurrence.\n",
        "\n",
        "## ‚ö†Ô∏è Shortfalls (Make These Visible)\n",
        "\n",
        "### ‚ùå 1. Long Reviews Hurt\n",
        "\n",
        "**Goal:** Show that longer sequences slow training and hurt accuracy because information must pass through every timestep sequentially.\n",
        "\n",
        "Increase max review length.\n",
        "\n",
        "You'll see:\n",
        "\n",
        "- Training slows\n",
        "- Accuracy plateaus\n",
        "\n",
        "Because:\n",
        "\n",
        "Information must travel through every timestep.\n",
        "\n",
        "### ‚ùå 2. Vanishing Gradients\n",
        "\n",
        "**Goal:** Measure gradient norms to show they shrink as sequences get longer, making it hard to learn from early tokens.\n",
        "\n",
        "Check gradient norm of embedding layer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8c5870",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "eb8c5870",
        "outputId": "03424d55-379d-40fe-c7ef-4cd9732b3e9a"
      },
      "source": [
        "# Run one training step to get gradients\n",
        "texts, lengths, labels = next(iter(train_loader))\n",
        "texts, lengths, labels = texts.to(device), lengths.to(device), labels.to(device).float()\n",
        "\n",
        "outputs = model(texts, lengths).view(-1)\n",
        "loss = criterion(outputs, labels)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"embedding\" in name and param.grad is not None:\n",
        "        print(\"Embedding grad norm:\", param.grad.norm().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1674dca9",
      "metadata": {
        "id": "1674dca9"
      },
      "source": [
        "Increase sequence length ‚Üí gradient shrinks.\n",
        "\n",
        "### ‚ùå 3. Serial Computation\n",
        "\n",
        "**Goal:** Demonstrate that training time scales linearly with sequence length because RNNs cannot parallelize across time steps, leaving GPUs underutilized.\n",
        "\n",
        "Time per epoch scales roughly linearly with sequence length.\n",
        "\n",
        "You cannot parallelize across time.\n",
        "\n",
        "GPU underutilized.\n",
        "\n",
        "### ‚ùå 4. Fixed Memory Bottleneck\n",
        "\n",
        "**Goal:** Show that all sequence information must be compressed into a fixed-size hidden vector, creating a memory bottleneck for long sequences.\n",
        "\n",
        "All review meaning compressed into:\n",
        "\n",
        "```python\n",
        "hidden_dim = 128\n",
        "```\n",
        "\n",
        "Long review.\n",
        "Single 128-dim vector.\n",
        "\n",
        "Compression pressure.\n",
        "\n",
        "## üß† Why This Is Perfect Before LSTM\n",
        "\n",
        "Students now feel:\n",
        "\n",
        "- Memory compression\n",
        "- Gradient fragility\n",
        "- Sequential bottleneck\n",
        "\n",
        "So when you introduce LSTM gates later, it solves a problem they already experienced.\n",
        "\n",
        "Not abstractly.\n",
        "\n",
        "Mechanically."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
