{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd33336",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-05/exercise-02.ipynb)\n",
    "\n",
    "# Visualizing Vanishing Gradients\n",
    "\n",
    "Good. Now we make vanishing gradients visible, not theoretical.\n",
    "\n",
    "We'll instrument your IMDB RNN so you can see how gradient strength decays through time.\n",
    "\n",
    "This turns \"vanishing gradient\" from a sentence into a measurement.\n",
    "\n",
    "## ðŸ”¬ Goal\n",
    "\n",
    "We want to answer:\n",
    "\n",
    "**When training on a long review, does the gradient reaching early words become smaller than the gradient for recent words?**\n",
    "\n",
    "If yes â†’ RNN forgets early information.\n",
    "\n",
    "## ðŸ§  Strategy\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Take one long batch\n",
    "2. Run forward pass\n",
    "3. Backprop once\n",
    "4. Measure gradient magnitude of hidden states at each timestep\n",
    "\n",
    "Because gradients must flow backward through time.\n",
    "\n",
    "If they shrink â†’ vanishing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a5ff5",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 1 â€” Modify Forward to Return All Hidden States\n",
    "\n",
    "Change your model slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee803a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload data (assuming you've run exercise-01)\n",
    "train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "train_data = list(train_iter)\n",
    "test_data = list(test_iter)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "counter = Counter()\n",
    "for label, text in train_data:\n",
    "    tokens = tokenizer(text)\n",
    "    counter.update(tokens)\n",
    "\n",
    "vocab_size = 20000\n",
    "most_common = counter.most_common(vocab_size - 2)\n",
    "vocab = {word: idx+2 for idx, (word, _) in enumerate(most_common)}\n",
    "vocab[\"<pad>\"] = 0\n",
    "vocab[\"<unk>\"] = 1\n",
    "\n",
    "def encode(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x)\n",
    "        \n",
    "        # Enable gradient tracking on intermediate outputs\n",
    "        out.retain_grad()\n",
    "        self.saved_outputs = out  # save all timestep outputs\n",
    "        \n",
    "        final_hidden = hidden.squeeze(0)\n",
    "        return self.fc(final_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef09bab5",
   "metadata": {},
   "source": [
    "Now `model.saved_outputs` contains:\n",
    "\n",
    "- Shape: `(batch_size, sequence_length, hidden_dim)`\n",
    "- All hidden states at every timestep\n",
    "\n",
    "## ðŸ§ª Step 2 â€” Gradient Inspection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d19cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for label, text in batch:\n",
    "        encoded = torch.tensor(encode(text))\n",
    "        texts.append(encoded)\n",
    "        labels.append(1 if label == \"pos\" else 0)\n",
    "    \n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_decay(model, loader):\n",
    "    model.train()\n",
    "    \n",
    "    texts, labels = next(iter(loader))\n",
    "    texts, labels = texts.to(device), labels.to(device).float()\n",
    "    \n",
    "    outputs = model(texts).squeeze()\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradients w.r.t. hidden outputs\n",
    "    grads = model.saved_outputs.grad  # shape: batch, seq_len, hidden\n",
    "    \n",
    "    if grads is None:\n",
    "        print(\"Enable requires_grad for saved_outputs\")\n",
    "        return\n",
    "    \n",
    "    # Average gradient magnitude per timestep\n",
    "    grad_magnitudes = grads.abs().mean(dim=(0,2)).detach().cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(grad_magnitudes)\n",
    "    plt.title(\"Gradient Magnitude Across Time Steps\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Average Gradient Magnitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return grad_magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bada415",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 3 â€” Run Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4bf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaRNN(vocab_size, embed_dim=100, hidden_dim=128).to(device)\n",
    "grad_magnitudes = visualize_gradient_decay(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efcc03c",
   "metadata": {},
   "source": [
    "## ðŸ“‰ What You'll See\n",
    "\n",
    "A plot like:\n",
    "\n",
    "```\n",
    "|\\\n",
    "| \\\n",
    "|  \\\n",
    "|   \\\n",
    "|    \\\n",
    "|     \\______\n",
    "|\n",
    "+------------------>\n",
    "```\n",
    "\n",
    "- Large gradients near the end.\n",
    "- Tiny gradients at early timesteps.\n",
    "\n",
    "That's vanishing gradient.\n",
    "\n",
    "Backprop must multiply through many Jacobians:\n",
    "\n",
    "```\n",
    "grad_t = grad_t+1 Ã— local_derivative\n",
    "```\n",
    "\n",
    "If `local_derivative < 1` on average â†’ exponential decay.\n",
    "\n",
    "## ðŸ§ª Make It Worse (Increase Sequence Length)\n",
    "\n",
    "Add truncation to enforce long sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215562e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch_long(batch):\n",
    "    texts, labels = [], []\n",
    "    \n",
    "    for label, text in batch:\n",
    "        encoded = torch.tensor(encode(text)[:400])  # force longer sequences\n",
    "        texts.append(encoded)\n",
    "        labels.append(1 if label == \"pos\" else 0)\n",
    "    \n",
    "    texts = pad_sequence(texts, batch_first=True)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "train_loader_long = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_batch_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0b80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize model\n",
    "model_long = VanillaRNN(vocab_size, embed_dim=100, hidden_dim=128).to(device)\n",
    "\n",
    "# Visualize with longer sequences\n",
    "grad_magnitudes_long = visualize_gradient_decay(model_long, train_loader_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7c873",
   "metadata": {},
   "source": [
    "You'll see:\n",
    "\n",
    "- Steeper decay.\n",
    "- Even smaller gradients at early timesteps.\n",
    "\n",
    "## ðŸ§  What This Proves\n",
    "\n",
    "- Early tokens barely receive gradient updates.\n",
    "- Model struggles to learn dependencies from far past.\n",
    "- Memory fragility is structural.\n",
    "\n",
    "This is not tuning.\n",
    "\n",
    "This is architecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
