{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "870ea7bc",
      "metadata": {
        "id": "870ea7bc"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-07/exercise-01.ipynb)\n",
        "\n",
        "# ðŸ§ª Exercise 2 â€” Visualizing Attention as Routing\n",
        "\n",
        "## Goal\n",
        "\n",
        "Show that attention is selective access, not memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e31e792",
      "metadata": {
        "id": "1e31e792"
      },
      "source": [
        "## ðŸ”¹ Setup\n",
        "\n",
        "We create a tiny sentence and manually inspect attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18cb68a",
      "metadata": {
        "id": "b18cb68a"
      },
      "source": [
        "## ðŸ§  Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e30d675e",
      "metadata": {
        "id": "e30d675e",
        "outputId": "a134a5c4-dd6b-4705-bd00-cb6fe56dedeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:\n",
            " tensor([[1.1576e-02, 1.2065e-03, 3.7276e-02, 1.0041e-01, 8.4953e-01],\n",
            "        [2.6881e-05, 1.7662e-04, 7.3634e-07, 9.9980e-01, 1.3803e-08],\n",
            "        [7.6419e-02, 4.0950e-02, 2.6440e-01, 1.5552e-03, 6.1668e-01],\n",
            "        [4.1226e-02, 1.2197e-01, 5.8175e-01, 1.9244e-02, 2.3581e-01],\n",
            "        [5.2087e-05, 8.4970e-05, 1.0952e-01, 5.3115e-04, 8.8981e-01]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Tiny example\n",
        "tokens = torch.randn(1, 5, 8)  # batch=1, seq=5, dim=8\n",
        "\n",
        "Wq = torch.randn(8, 8)\n",
        "Wk = torch.randn(8, 8)\n",
        "Wv = torch.randn(8, 8)\n",
        "\n",
        "Q = tokens @ Wq\n",
        "K = tokens @ Wk\n",
        "V = tokens @ Wv\n",
        "\n",
        "scores = Q @ K.transpose(-2, -1) / (8 ** 0.5)\n",
        "weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "output = weights @ V\n",
        "\n",
        "print(\"Attention weights:\\n\", weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbaa12bf",
      "metadata": {
        "id": "dbaa12bf"
      },
      "source": [
        "## ðŸ”Ž What They See\n",
        "\n",
        "A 5Ã—5 matrix.\n",
        "\n",
        "Each row = \"How much does this token care about every other token?\"\n",
        "\n",
        "This makes routing visible.\n",
        "\n",
        "Not hidden state compression.\n",
        "\n",
        "Direct lookup."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc3cc170",
      "metadata": {
        "id": "fc3cc170"
      },
      "source": [
        "## ðŸ’¡ Conceptual Link\n",
        "\n",
        "When processing token 4:\n",
        "\n",
        "Instead of remembering token 1 through a chain,\n",
        "it assigns direct weight to token 1.\n",
        "\n",
        "Memory replaced by access."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhS0hatd-n99"
      },
      "id": "IhS0hatd-n99",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}