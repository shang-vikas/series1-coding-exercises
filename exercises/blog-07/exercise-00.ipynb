{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fde29f9a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shang-vikas/series1-coding-exercises/blob/main/exercises/blog-07/exercise-00.ipynb)\n",
    "\n",
    "# üß™ Exercise 1 ‚Äî Sequential Bottleneck vs Parallel Access\n",
    "\n",
    "## Goal\n",
    "\n",
    "Make time dependency visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e6ddac",
   "metadata": {},
   "source": [
    "## üîπ What We Demonstrate\n",
    "\n",
    "- RNN processes tokens one-by-one.\n",
    "- Self-attention processes all tokens together.\n",
    "- Hardware utilization differs.\n",
    "- Architecture dictates compute pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea89b06",
   "metadata": {},
   "source": [
    "## üß† Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29360ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 200\n",
    "dim = 128\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, dim).to(device)\n",
    "\n",
    "# --- RNN ---\n",
    "rnn = nn.RNN(dim, dim, batch_first=True).to(device)\n",
    "\n",
    "start = time.time()\n",
    "_ = rnn(x)\n",
    "print(\"RNN forward time:\", time.time() - start)\n",
    "\n",
    "# --- Self-Attention ---\n",
    "attn = nn.MultiheadAttention(embed_dim=dim, num_heads=4, batch_first=True).to(device)\n",
    "\n",
    "start = time.time()\n",
    "_ = attn(x, x, x)\n",
    "print(\"Attention forward time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decae92",
   "metadata": {},
   "source": [
    "## üîé What They Observe\n",
    "\n",
    "- RNN must walk through sequence.\n",
    "- Attention processes whole sequence at once.\n",
    "- On GPU, attention is often faster for moderate sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60e646",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4e06558",
   "metadata": {},
   "source": [
    "## üí° Conceptual Link\n",
    "\n",
    "**RNN:**\n",
    "\n",
    "```\n",
    "t1 ‚Üí t2 ‚Üí t3 ‚Üí t4 ‚Üí ...\n",
    "```\n",
    "\n",
    "**Transformer:**\n",
    "\n",
    "```\n",
    "t1\n",
    "t2\n",
    "t3   ‚Üê all computed together\n",
    "t4\n",
    "```\n",
    "\n",
    "This proves:\n",
    "\n",
    "**Parallelism wasn't optimization ‚Äî it was architectural freedom.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a033f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebc318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d62a5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de27a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5cd76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44886e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca47726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dca925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be8f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba5925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d23b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852999ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e41473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a0b5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2144cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daefbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f48085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
